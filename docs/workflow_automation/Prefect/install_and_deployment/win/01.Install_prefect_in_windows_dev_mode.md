# Install prefect on windows

In this tutorial, we will install prefect on windows. This is for dev purpose, not for multiple user production deployment.

## 1. Create a virtual env

## 2. Install the prefect 

The `Prefect` framework can be deployed via python package.

You need to activate your virtual env before, and run the below commands in your virtual env.

```shell
# install the prefect package
python -m pip install --upgrade pip

# install prefect
pip install prefect

# check the installed prefect version
prefect version

# Minimum configuration to run prefect
# Set Prefect home and API
# You should isolate configurations per user:
# here I put my server home in my project folder. It's for dev only
setx PREFECT_HOME "C:\Users\PLIU\Documents\git\WorkflowPlayGround\prefect\server_conf"
setx PREFECT_API_URL "http://localhost:4200/api"



# start prefect server
prefect server start
```

After the above command, you will have two services:
- **API**: runs on http://127.0.0.1:4200/api
- **Web UI**: runs on http://127.0.0.1:4200

> Leave this PowerShell window open or run it as a background service (see §8). (Need to check)

## 3. Run a workflow with Prefect

You can save the below script in a directory. In my case, I put it under `prefect/server_conf/flows/pengfei/simple_flow.py`

```python
from prefect import flow, task, get_run_logger

@task(log_prints=True)
def extract():
    data = [1, 2, 3]
    msg = f"Task1: Extracting data: {data}"
    logger = get_run_logger()
    logger.info(msg)
    print(msg)
    return data

@task(log_prints=True)
def transform(data):
    print(f"Task2: Transforming data: {data}")
    return [x * 10 for x in data]

@task(log_prints=True)
def load(data):
    print(f"Task3: Loading result {data}")

@flow
def etl_flow():
    # task 1
    data = extract()
    # task 2
    clean = transform(data)
    # task 3
    load(clean)


if __name__ == "__main__":
    etl_flow()
```
This script uses both the print and logger to print info in the task logs

### 3.1 Tracing the output of your workflow

> By default, Prefect's logging system does not capture `print()` statements in your tasks. We can use `log_prints=True` to enable it
> 
Before you run the workflow, 
- make sure the `prefect server` is up.
- the virtual env is activated

```shell
# run the workflow as any python script.
python ./prefect/server_conf/flows/pengfei/simple_flow.py
```

### 3.2 Use Prefect Logger

Below is an example on how to use the Prefect default logger to log information.

```python

from prefect import flow, task, get_run_logger

@task
def say_hello(name: str):
    logger = get_run_logger()
    logger.info(f"Hello, {name}!")

@flow
def simple_flow():
    say_hello("Prefect User")

if __name__ == "__main__":
    simple_flow()

```


### 4. Use Prefect orchestration

**There is a breaking change between Prefect 2.x and 3.x.**

To avoid confusion, in this tutorial we will only show the configuration for `Prefect 3.x`.

#### 4.1 Build the deployment

`Prefect 3.x` introduces the **projects** concept(instead of the older `build/apply model` in Prefect 2.x).
You now define deployments declaratively in a `prefect.yaml` file.

Example file structure of a prefect project:

```text
C:\projects\spark_flow\
│
├── spark_wc_flow.py
└── prefect.yaml
```

Below is an example of the `prefect.yaml`

```yaml
# prefect.yaml
name: spark_project
prefect-version: 3.4.24

# it can have multiple deployments
deployments:
  # deployment 1
  - name: spark_wordcount
    entrypoint: spark_wc_flow.py:main_flow
    work_pool:
      name: local-pool # for local execution
      type: process
      tags: [spark, local]
      
  # Deployment 2: ETL flow
  - name: simple_etl
    entrypoint: simple_flow.py:etl_flow
    work_pool:
      name: local-pool
      type: process
      tags: [spark, local]

```

> If the work pool doesn’t exist, create it manually: `prefect work-pool create local-pool --type process`
> 

###

```shell
# create a work-pool
prefect work-pool create local-pool --type process

# start a worker which takes flow from a pool
prefect worker start --pool local-pool

# we can also create personal pool
prefect work-pool create "%USERNAME%-pool" --type process
prefect worker start --pool "%USERNAME%-pool"

# run the deployment 
prefect deployment run spark_project/spark_wordcount
```

