# 2. Configuration of kubespary (Ansible role)

In step 1, we have provisioned three server:

```text
[k8s]
k8s-01 ansible_host=10.0.2.7
k8s-02 ansible_host=10.0.2.8
k8s-03 ansible_host=10.0.2.9
```

In [ansible_installation](sys_admin/05.Install_ansible_debian.md) tutorial, we have installed ansible. So we have all the requirements to run a kubespary installation.

Now we just need to configure kubespray correctly to install k8s on these server correctly.

The configuration of kubespary contains two parts:

- ansible roles (a set of task ansible will run on these servers)
- inventory (a list of server which ansible will run the roles)

## 2.1 Deploy a k8s cluster

Kubespray provide a default configuration which is located at `kubespray/inventory/sample`

Let's copy it to a new folder

```shell
cd /path/to/kubespray
cp -rfp inventory/sample inventory/mycluster

ls inventory/mycluster
# you should see below contents
group_vars  inventory.ini
```

The inventory.ini contains a list of server with fake IP. You need to replace them with your real server IP.

The group_vars contains a list of roles(tasks) that kubespary will install on your cluster k8s.

For now, we only modify the inventory. Ansible accepts many different file format as inventory. I prefer the .yaml. In our case the file name will be **inventory.yaml** and has below content

```yaml
all:
  hosts:
    controlplane1:
      ansible_host: 10.0.2.7
      ip: 10.0.2.7
      access_ip: 10.0.2.7
    worker1:
      ansible_host: 10.0.2.8
      ip: 10.0.2.8
      access_ip: 10.0.2.8
    worker2:
      ansible_host: 10.0.2.9
      ip: 10.0.2.9
      access_ip: 10.0.2.9
  children:
    kube_control_plane:
      hosts:
        controlplane1:
    kube_node:
      hosts:
        worker1:
        worker2:
    etcd:
      hosts:
        controlplane1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
```

The **controlplane1**is the `master node` which will host `etcd`, `api server`, etc. The `worker1` and `worker2` are the worker node, which will host the pods deployed by the users.

Now we can run below command to start the installation

```shell
ansible-playbook -i inventory/pengfei_cluster/inventory.yaml --become --become-user=root cluster.yml
```

The option "--become, --become-user" allow us to run ansible with a non root user which has sudo rights.

If everything works well, this will take 20 mins to install all the necessary on the three servers to run a k8s cluster.

## 2.2 Add new nodes to the cluster

The k8s cluster can be easily scalable horizontally. With below command, we can add a new worker node to the cluster

### 2.2.1 Add new node to the inventory
For example, now we have a new node `worker3:10.0.2.10` which we want to add to our cluster

```yaml
all:
  hosts:
    controlplane1:
      ansible_host: 10.0.2.7
      ip: 10.0.2.7
      access_ip: 10.0.2.7
    worker1:
      ansible_host: 10.0.2.8
      ip: 10.0.2.8
      access_ip: 10.0.2.8
    worker2:
      ansible_host: 10.0.2.9
      ip: 10.0.2.9
      access_ip: 10.0.2.9
    worker3:
      ansible_host: 10.0.2.10
      ip: 10.0.2.10
      access_ip: 10.0.2.10
  children:
    kube_control_plane:
      hosts:
        controlplane1:
    kube_node:
      hosts:
        worker1:
        worker2:
        worker3:
    etcd:
      hosts:
        controlplane1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
```

### 2.2.2 Apply the change with kubespray

 

```shell
# Note you need to activate the kubespray virtual env to run below command
ansible-playbook -i inventory/pengfei_cluster/inventory.yaml --become --become-user=root cluster.yml --limit worker3

# enable kubespary virtual env
source /path/to/kubespary-venv/bin/activate
```

## Install kubectl and helm client

During the k8s installation, you can install the kubectl and helm client on your pc

### Install kubectl

Get the kubectl binary

```shell
# download the kubectl binary
wget https://storage.googleapis.com/kubernetes-release/release/v1.24.4/bin/linux/amd64/kubectl -O kubectl

# add execution rights
chmod a+x kubectl

# move it to /bin
mv kubectl ~/.local/bin/

# if you .local/bin is not in your path, you need to add following line to your .bashrc or .bash_profile

```

### Configure kubectl

Kubectl requires a config file to connect to the k8s api server. You can get the config file from the controlplane at `/etc/kubernetes/admin.conf`. This file should look like this:

```text
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3lOakV6TXpRMU5sb1hEVE15TURreU16RXpNelExTmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS2J2ClJraXoDdmRyeXFnTzdKNTJzeG9ONFgyTnJCYWdBQzBjcy83a3RjMzZBVjYKTVFxb2Y3M0JQQWRBNHAyVE9Eb0tsTGNkVWs4K2FiOFVYanhRbHZjeVJ4VjZOV2daamRlZHlYNTlYdWtUcWxEagp0RFdTWlJQcmRwcVhoWDMwVzFDN3haWms1SDJnRnF1MHRERmgwRUVpbWlST3p4S3h6NU1kUnE0V0NBYTg4d2dkCnlMWXFMQ2JZT2N5MTEvZEl6SXRaNUVPTlVBR2I4akdRTk9qcDZ6SHFRdlVaODRXUnp5Qkl1azB3MUJRcVFBVFMKY3I4PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://<ip_controlplane>:6443
  name: cluster.local
contexts:
- context:
    cluster: cluster.local
    user: kubernetes-admin
  name: kubernetes-admin@cluster.local
current-context: kubernetes-admin@cluster.local
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJU1JVMUxhTVJlT1l3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBNU1qWXhNek0wTlRaYUZ3MHlNekE1TWpZeE16TTBOVGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVJYXF6VG9FSlpsQTlMZEMvYnNTRjZRcXFyajhIago0WE5VdUY1ams5TGZRQ0JFWjVuU1pnNTJVcnJ3OHQ0bDVDZ0dSd3A1MVFCT095aEtWVWdwSktHZi85TEdRLzVpCjdxNkkrblROREFXRDZtYjYwc3dDZEpxVXkyZ0xUQ3ZGM1FRUEhSb1VacFFzeFRUU3ZoNkdJeDMzSlRxejhHQXMKalovTTk1ck5jcTRVbVJYVTB0WnQweDB2dmlaUXFYTmdoZUlad3NFa1U2UHlvVEUzSnc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBb1ErZXJyaE5rV25SaXVYazVJeU84UXJZK2lxOHdDM3A4aXR5MlFzL2kxNHNkaGE4CjZiTXczL1FQNTVKYlZmSjJxMnFOb2pwaDlCZ2ErRytyRlBoQWp4SDVPUGZxZStNYWlLWXpSWnlJSWlVR0twaFAKMmR3TXZmL0xNdkNBWCthZUJyMUVUOFF4RFkyWWxqMURqT1NLYThnZXdsSnJKN2NJNXd2Mm5PSGtFM01XaC9CSApEN3BkT3dobFJRSUdRMDdEbWJtcTFUOHN4L0lrNm1oK1liM2tvN3BJTmJ1T1EwYWRqNzJWU0lsMVZTMFJsRWdNCndjNWk1WG1IZmVaIvZzZQYVF3aC9RTHJvd1dkdlQ3d2pNU0cvbGQ0ejNrRQpVQVlKUlFRWmN1QjFrYzlRdGZHS0xKeHFZeCtteEJuam0vRjYyRVpMa0RTTFowMmxZMkt1VldSdkNlQ2VXRTk5CkRCdnJBb0dCQUpxMDBiaWQ1OS92d3VoUzBjN05YcFkvcENQVERTS20zdE0xcGhLbnFrSkFsYzI5VWNxYWxqVngKVjZlUGhoakR1RGgvODl6WTZWWmFLeW9laS9VbFJJVVZ3M1pieFVoZ0diZkF3YWhWUXRtMkFsY2UzcHAyempWVgpVWlZiVHdta1JnV3lEUlhQZFZnUkgzNCtEZllmaUZkdFc3TEYvUVd2YmxFRm5salBHVG15Ci0tLS0tRU5EIFJTQSBQUklWQVRFIEtFWS0tLS0tCg==

```

In the pc where you have installed kubectl, do the following command

```shell
mkdir -p ~/.kube/

vim ~/.kube/config

```
Then copy the above `admin.conf` file. Note you need to modify the `server url`. For example, if the IP of your controlplane is 10.50.5.58. You need to put this IP on the config file

### Install helm

See doc [post_k8s_installation_config.md](./k8s/post_k8s_installation_config.md)

## Test your k8s cluster

After the installation is finished, you can test your k8s cluster

```shell
kubectl get nodes

# You will get all available nodes in your k8s cluster (master and worker)
NAME            STATUS   ROLES                  AGE    VERSION

controlplane1   Ready    control-plane,master   142m   v1.23.7

worker1         Ready    <none>                 139m   v1.23.7

worker2         Ready    <none>                 139m   v1.23.7

```

## Certificate management in k8s

The components of the created cluster use certificates to communicate with each other. By default, Client 
certificates `generated by kubeadm expire after 1 year`. We need to renew them.
You can find the full doc here https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/

### Check certificate expiration

```shell
kubeadm certs check-expiration

# Output 
CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Nov 08, 2024 15:39 UTC   361d            ca                      no      
apiserver                  Nov 08, 2024 15:39 UTC   361d            ca                      no      
apiserver-kubelet-client   Nov 08, 2024 15:39 UTC   361d            ca                      no      
controller-manager.conf    Nov 08, 2024 15:39 UTC   361d            ca                      no      
front-proxy-client         Nov 08, 2024 15:39 UTC   361d            front-proxy-ca          no      
scheduler.conf             Nov 08, 2024 15:39 UTC   361d            ca                      no      

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Sep 23, 2032 13:34 UTC   8y              no      
front-proxy-ca          Sep 23, 2032 13:34 UTC   8y              no      

```

### Automatic certificate renewal

kubeadm renews all the certificates during `control plane upgrade`.

This feature is designed for addressing the simplest use cases; if you don't have specific requirements on certificate 
renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade), kubeadm will 
take care of keeping your cluster up to date and reasonably secure.

### Manual certificate renewal

You can renew your certificates manually at any time with the `kubeadm certs renew` command, with the appropriate 
command line options.

`kubeadm certs renew` can renew any specific certificate or, with the subcommand all, it can renew all of them, as shown below:

```shell
kubeadm certs renew all
```

### External CA mode

Todo
