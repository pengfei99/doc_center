# Kerberos integration in cluster hadoop spark

The objective of kerberos integration is to enforce user authentication and access control.


## General steps

In this tutorial, we use AD/krb as the authentication server. 

1. Design the cluster architecture, for each server `define a ip and hostname`
2. Configure `service principals/accounts`(e.g. hdfs, yarn, ssh, http) in AD which are required by the hadoop service to authenticate via kerberos 
3. Configure DNS for the linux server
4. Define different group for common-user, and admin user for different server access(namenode, datanode open to admin only)
5. Setup hadoop master(e.g. namenode, resourcemanager)
6. Setup hadoop worker(e.g. datanode, nodemanager) 
7. Setup hadoop client(hdfs client, yarn client, spark client)


## 1. Design the cluster architecture
In this tutorial, the cluster has 4 servers. 
1. hadoop-client: allow user to submit spark jobs, view job status, view data in hdfs.
2. hadoop-master: hdfs namenode, spark master node.
3. hadoop-worker1: hdfs datanode, spark worker node.
4. hadoop-worker2: hdfs datanode, spark worker node.

## 2. Configure AD for service account

AD provides different types of account, we choose to use `gMSA`. So the first step is to create gMSA account for the 4 servers.

For each server in the hadoop cluster(master and worker), we need a `gMSA` account. And the account must have the below attributes:
```shell
# servicePrincipalName
├── hdfs/nom_de_host.exemple.com@EXEMPLE.COM      # Service YARN & hdfs
├── host/nom_de_host.exemple.com@EXEMPLE.COM      # Service ssh 
└── HTTP/nom_de_host.exemple.com@EXEMPLE.COM      # Interface Web

# userPrincipalName
hdfs/nom_de_host.exemple.com@EXEMPLE.COM
```
For example, if we have 3 hosts(1 master, 2 workers) in the cluster, we need to have `three gMSA` accounts.

As we need to synchronize the `keytab file` with `gMSAd`, we need a static user account which allows us to do the refresh

### 2.1 Create a gMSA account in AD

```powershell
# create a gMSA account
New-ADServiceAccount -Name "gMSA-hclient" -Description "hadoop client linux" -DNSHostName "gMSA-hclient.casdds.casd" -ManagedPasswordIntervalInDays 1 -PrincipalsAllowedToRetrieveManagedPassword "Linux" ` -Enabled $True

# add spn to a gMSA account
setspn -A host/hclient.casd.fr@CASD.FR gMSA-hclient
setspn -A HTTP/hclient.casd.fr@CASD.FR gMSA-hclient
setspn -A HDFS/hclient.casd.fr@CASD.FR gMSA-hclient
```

### 2.2 Setup dns in AD

In the dns manager:
- create a host with hostname:`gMSA-hclient.casdds.casd` and it's associate IP.
- create a Pointer(PTR) with hostname:`gMSA-hclient.casdds.casd` and it's associate IP.


### 2.3 Set up the hostname in the linux server

The hostname in the linux server must match the AD and dns setup hostname

```shell
# change hostname
sudo hostnamectl set-hostname gMSA-hclient.casdds.casd

# change /etc/hosts with the below lines
sudo vim /etc/hosts 
127.0.0.1       localhost
10.50.5.224     gMSA-hclient.casdds.casd    gMSA-hclient

# edit resolv to setup AD as dns
sudo vim /etc/resolv.conf 

# add the below lines
search casdds.casd
nameserver <ad-ip>
```


## Setup hadoop master

Beware, on 29/01/2026, there are two major hadoop versions. 3.3.x and 3.4.x. The configuration of the two versions are quite different.
> When you write docs about the installation, you must specify explicitly which version you are using. 

Setup krb client and sssd, pam, ssh for user access of the linux server


## Setup hadoop worker

## Setup hadoop client
 