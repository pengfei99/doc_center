# Kerberos integration in cluster hadoop spark

The objective of kerberos integration is to enforce user authentication and access control.


## General steps

In this tutorial, we use AD/krb as the authentication server. 

1. Design the cluster architecture, for each server `define a hostname`
2. Configure AD for service account(e.g. hdfs, yarn, ssh, http) which runs inside linux server
3. Configure DNS for the linux server
4. Define different group for common-user, and admin user for different server access(namenode, datanode open to admin only)
5. Setup hadoop master 
6. Setup hadoop worker 
7. Setup hadoop client


## 1. Design the cluster architecture
In this tutorial, the cluster has 4 servers. 
1. hadoop-client: allow user to submit spark jobs, view job status, view data in hdfs
2. hadoop-master: hdfs namenode, spark master node
3. hadoop-worker1: hdfs datanode, spark worker node
4. hadoop-worker2: hdfs datanode, spark worker node

## 2. Configure AD for service account

AD provides different types of account, we choose to use `gMSA`. So the first step is to create gMSA account for the 3 servers


## Setup hadoop master

Setup krb client and sssd, pam, ssh for user access of the linux server


## Setup hadoop worker

## Setup hadoop client
 