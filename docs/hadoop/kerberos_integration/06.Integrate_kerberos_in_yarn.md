# Integrate kerberos into yarn

When integrating kerberos into yarn. We need to configure:
- container-executor for node manager
- yarn-site.xml for resource manager
- yarn-site.xml for node manager
- 

## 1. Configure container executor

Setting up the `secure container executor` is the most sensitive part of Kerberized YARN. 
It’s the bridge between YARN and Linux user isolation. If this configuration is wrong, NodeManager either 
refuses to start or runs insecurely.

When user submit a job to a YARN cluster. the resource manager divides the job into tasks, and decides which task should
run on which nodes:
- On each node, the `NodeManager` receives a task
- `NodeManager` asks `container-executor` to launch it
- `container-executor` switches to the actual Linux user account that maps to the user who submit the job
- the container runs with the real end user account

This mechanism prevents:
- users escaping containers
- privilege escalation
- cross-user access

> To be able to switch user safely, the `container-executor` must run as `root:hadoop(with setuid and setgid)`.

### 1.1 Set ownership and permissions of container-executor binary

The `NodeManager` requires two things to run a container:
- container-executor binary: `$HADOOP_HOME/bin/container-executor`
- container-executor.cfg: `$HADOOP_HOME/etc/hadoop/container-executor.cfg`

The ownership and permissions must be correct, otherwise the container will never run correctly.

```shell
# This is the minimum config
chown root:hadoop $HADOOP_HOME/bin/container-executor
chmod 6050 $HADOOP_HOME/bin/container-executor

# to avoid error message, I also did the below command before
chown root:hadoop -R $HADOOP_HOME
```
Now, let's understand 6050:
- 6 is the special bits: it means `4 (setuid) + 2 (setgid) → both special bits enabled.`
- 0 is the owner acl: 0 means `owner has zero rights`
- 5 is the group acl: 5 means `group gets 4 (read) + 1 (execute)`
- 0 is the other acl: 0 means `other users has zero rights`

> setuid and setgid means when anyone runs it, the process gets eUID = root and eGID = hadoop
Check the acl after the above command
```shell
sudo ls -lh $HADOOP_HOME/bin/container-executor
# expected output
---Sr-s--- 1 root hadoop 785K Jun 18  2023 /opt/hadoop/hadoop-3.3.6/bin/container-executor
```
More details of the acl
1. - : means a regular file
2. --S: -- means owner has no read and write right, S means`setuid is enabled` but `the owner does not have the execute bit`.
3. r-s: r- means group has read but no write right. s means `setgid is enabled` and `the group does have the execute bit`.
4. ---: `other users has zero rights`

### 1.2 Set ownership and permissions of container-executor.cfg

The `container-executor.cfg` file defines the `security policy of the container-executor binary`.
It is one of the most critical security files in a Hadoop/YARN cluster.

Below is the minimum config for `container-executor.cfg`

```ini
# this value must match the group that owning the container-executor binary.
yarn.nodemanager.linux-container-executor.group=hadoop

# specify user accounts that must never run containers.
banned.users=root,yarn,mapred

# specify user accounts allowed to interact with NodeManager.
allowed.system.users=hadoop

# Prevents system accounts from running containers.
min.user.id=1000

# Traffic control. Usually disabled unless doing bandwidth isolation.
feature.tc.enabled=false
```
Use the below command to set the ownership and permissions of container-executor.cfg

```shell
chown root:hadoop $HADOOP_HOME/etc/hadoop/container-executor.cfg
chmod 0400 $HADOOP_HOME/etc/hadoop/container-executor.cfg
```

### 1.3 Valid the container executor
To valid the container executor setup, you can use the below command
```shell
sudo $HADOOP_HOME/bin/container-executor --checksetup
```
> If it returns nothing, it means your setup is correct. If it returns error message, then correct them.
>

## Configure yarn-site.xml for the name node

In the name node, we specify the resource manager of the yarn cluster. Below is an example the minimum config:

```xml
<configuration>
    <!-- basic config for yarn to run -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
        <description>Minimum container memory allocation.</description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
        <description>Maximum container memory (12GB per worker).</description>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
        <description>Total memory for NodeManager (if master also acts as worker; adjust if not).</description>
    </property>


  <!-- kerberos config: ResourceManager identity -->
  <property>
    <name>yarn.resourcemanager.principal</name>
    <value>hdfs/_HOST@CASDDS.CASD</value>
  </property>

  <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>/etc/krb5.keytab</value>
  </property>

    
  <!-- Authorization -->
  <property>
    <name>yarn.acl.enable</name>
    <value>true</value>
  </property>

  <!-- kerberos config: Web UI SPNEGO -->
  <property>
    <name>yarn.resourcemanager.webapp.spnego-principal</name>
    <value>HTTP/_HOST@CASDDS.CASD</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.spnego-keytab</name>
    <value>/etc/krb5.keytab</value>
  </property>

  <!-- HTTPS -->
  <property>
    <name>yarn.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>
    
    <!-- Enable log aggregation -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

  <!-- HDFS directory for YARN logs -->
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/var/log/hadoop-yarn/apps</value>
  </property>

</configuration>

```


```xml
<configuration>



  <!-- Required for MapReduce & Spark on YARN -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  <!-- ResourceManager -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>sparkm01.casd.fr</value>
  </property>

  <!-- NodeManager resources (per worker) -->
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>14336</value>
  </property>

  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>8</value>
  </property>

  <!-- Scheduler limits -->
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>8192</value>
  </property>

  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>4</value>
  </property>

  <!-- Kerberos: ResourceManager -->
  <property>
    <name>yarn.resourcemanager.principal</name>
    <value>hdfs/sparkm01@CASD.FR</value>
  </property>

  <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>/etc/krb5.keytab</value>
  </property>

  <!-- Kerberos: NodeManager -->
  <property>
    <name>yarn.nodemanager.principal</name>
    <value>hdfs/sparkw01@CASD.FR</value>
  </property>

  <property>
    <name>yarn.nodemanager.keytab</name>
    <value>/etc/krb5.keytab</value>
  </property>

</configuration>
```

> We don't need timeline service here, because spark does not use timeline service. Unless you use explicitly ATS v1 or v2,

### Configuration of hadoop-env.sh

```shell
# Java
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Run HDFS daemons as hadoop user
export HDFS_NAMENODE_USER=hadoop
export HDFS_DATANODE_USER=hadoop

# Hadoop home
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_SBIN_DIR=${HADOOP_HOME}/sbin

# Hadoop config
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}


# JVM options (base)
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.net.preferIPv4Stack=true"
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.security.krb5.conf=/etc/krb5.conf"

#
# Heap defaults (fallback values)
#
export HADOOP_HEAPSIZE_MIN=1024
export HADOOP_HEAPSIZE_MAX=2048

# Hadoop security logging, to be remove for prod
export HADOOP_SECURITY_LOGGER=INFO,RFAS,console


# NameNode JVM tuning (Java 11 optimized)
export HDFS_NAMENODE_OPTS="
  -Xms2g
  -Xmx2g
  -XX:+UseG1GC
  -XX:MaxGCPauseMillis=500
  -XX:+DisableExplicitGC
  -XX:+ParallelRefProcEnabled
  -XX:InitiatingHeapOccupancyPercent=45
  -XX:MaxMetaspaceSize=256m
  -Xlog:gc*,gc+heap=info:file=/var/log/hadoop/gc-namenode.log:time,uptime:filecount=5,filesize=100m
"

#
# DataNode JVM options (Java 11 safe)
#
export HDFS_DATANODE_OPTS="
  -XX:+UseG1GC
  -XX:MaxGCPauseMillis=500
  -XX:+DisableExplicitGC
  -Dhadoop.security.logger=ERROR,RFAS
"

```

## Validation

```shell
yarn node -list
yarn application -list
```

Yarn Web Ui: `https://deb13-spark1.casd.fr:8090
`