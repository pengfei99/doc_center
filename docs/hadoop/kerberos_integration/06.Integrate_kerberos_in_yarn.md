# Integrate kerberos into yarn

When integrating kerberos into yarn. We need to configure:
- container-executor for node manager
- yarn-site.xml for resource manager
- yarn-site.xml for node manager
- 

## 1. Configure container executor

Setting up the `secure container executor` is the most sensitive part of Kerberized YARN. 
It’s the bridge between YARN and Linux user isolation. If this configuration is wrong, NodeManager either 
refuses to start or runs insecurely.

When user submit a job to a YARN cluster. the resource manager divides the job into tasks, and decides which task should
run on which nodes:
- On each node, the `NodeManager` receives a task
- `NodeManager` asks `container-executor` to launch it
- `container-executor` switches to the actual Linux user account that maps to the user who submit the job
- the container runs with the real end user account

This mechanism prevents:
- users escaping containers
- privilege escalation
- cross-user access

> To be able to switch user safely, the `container-executor` must run as `root:hadoop(with setuid and setgid)`.

### 1.1 Set ownership and permissions of container-executor binary

The `NodeManager` requires two things to run a container:
- container-executor binary: `$HADOOP_HOME/bin/container-executor`
- container-executor.cfg: `$HADOOP_HOME/etc/hadoop/container-executor.cfg`

The ownership and permissions must be correct, otherwise the container will never run correctly.

```shell
# This is the minimum config
sudo chown root:hadoop $HADOOP_HOME/bin/container-executor
sudo chmod 6050 $HADOOP_HOME/bin/container-executor
```
Now, let's understand 6050:
- 6 is the special bits: it means `4 (setuid) + 2 (setgid) → both special bits enabled.`
- 0 is the owner acl: 0 means `owner has zero rights`
- 5 is the group acl: 5 means `group gets 4 (read) + 1 (execute)`
- 0 is the other acl: 0 means `other users has zero rights`

> setuid and setgid means when anyone runs it, the process gets eUID = root and eGID = hadoop
Check the acl after the above command
```shell
sudo ls -lh $HADOOP_HOME/bin/container-executor
# expected output
---Sr-s--- 1 root hadoop 785K Jun 18  2023 /opt/hadoop/hadoop-3.3.6/bin/container-executor
```
More details of the acl
1. - : means a regular file
2. --S: -- means owner has no read and write right, S means`setuid is enabled` but `the owner does not have the execute bit`.
3. r-s: r- means group has read but no write right. s means `setgid is enabled` and `the group does have the execute bit`.
4. ---: `other users has zero rights`

> To avoid error message, I also did the below command before. You will notice in the nodemanager logs
> container-executor also checks the onwership and permissions of the $HADOOP_HOME folder

```shell
sudo chown -R root:hadoop -R $HADOOP_HOME
sudo chmod -R 750 $HADOOP_HOME
# if you did not change default log path, and you run your hadoop daemons as hadoop
# you need to make sure the user hadoop has group hadoop, and $HADOOP_HOME/logs has group write right
sudo chmod -R 770 $HADOOP_HOME/logs

```
### 1.2 Set ownership and permissions of container-executor.cfg

The `container-executor.cfg` file defines the `security policy of the container-executor binary`.
It is one of the most critical security files in a Hadoop/YARN cluster.

Below is the minimum config for `container-executor.cfg`

```ini
# this value must match the group that owning the container-executor binary.
yarn.nodemanager.linux-container-executor.group=hadoop

# specify user accounts that must never run containers.
banned.users=root,yarn,mapred

# specify user accounts allowed to interact with NodeManager.
allowed.system.users=hadoop

# Prevents system accounts from running containers.
min.user.id=1000

# Traffic control. Usually disabled unless doing bandwidth isolation.
feature.tc.enabled=false
```
Use the below command to set the ownership and permissions of container-executor.cfg

```shell
sudo chown root:hadoop $HADOOP_HOME/etc/hadoop/container-executor.cfg
sudo chmod 0400 $HADOOP_HOME/etc/hadoop/container-executor.cfg
```

### 1.3 Valid the container executor
To valid the container executor setup, you can use the below command
```shell
sudo $HADOOP_HOME/bin/container-executor --checksetup
```
> If it returns nothing, it means your setup is correct. If it returns error message, then correct them.
>

## Configure yarn-site.xml for the name node

In the name node, we specify the resource manager of the yarn cluster. Below is an example the minimum config:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- basic config for yarn to run -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
        <description>Minimum container memory allocation.</description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
        <description>Maximum container memory (12GB per worker).</description>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
        <description>Total memory for NodeManager (if master also acts as worker; adjust if not).</description>
    </property>


  <!-- kerberos config: ResourceManager identity -->
  <property>
    <name>yarn.resourcemanager.principal</name>
    <value>hdfs/_HOST@CASDDS.CASD</value>
  </property>

  <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>/etc/krb5.keytab</value>
  </property>

    
  <!-- Authorization -->
  <property>
    <name>yarn.acl.enable</name>
    <value>true</value>
  </property>

  <!-- kerberos config: Web UI SPNEGO -->
  <property>
    <name>yarn.resourcemanager.webapp.spnego-principal</name>
    <value>HTTP/_HOST@CASDDS.CASD</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.spnego-keytab</name>
    <value>/etc/krb5.keytab</value>
  </property>

  <!-- HTTPS -->
  <property>
    <name>yarn.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>
    
    <!-- Enable log aggregation -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

  <!-- HDFS directory for YARN logs -->
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/var/log/hadoop-yarn/apps</value>
  </property>

</configuration>

```

## Configure yarn-site.xml for the data node

In the `data node`, we specify the node manager of the yarn cluster. Below is an example the minimum config:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- basic config for yarn to run -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
        <description>Minimum container memory allocation.</description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
        <description>Maximum container memory (12GB per worker).</description>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
        <description>Total memory available to NodeManager (12GB).</description>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
        <description>Virtual memory ratio.</description>
    </property>

    <!-- kerberos config: NodeManager identity -->
    <property>
        <name>yarn.nodemanager.principal</name>
        <value>hdfs/_HOST@CASDDS.CASD</value>
    </property>

    <property>
        <name>yarn.nodemanager.keytab</name>
        <value>/etc/krb5.keytab</value>
    </property>

    <!-- kerberos config: ResourceManager identity -->
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- kerberos config: Secure container executor -->
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <!-- this group must match the group in container-executor.cfg -->
    <property>
        <name>yarn.nodemanager.linux-container-executor.group</name>
        <value>hadoop</value>
    </property>

    <!-- Web UI SPNEGO -->
    <property>
        <name>yarn.nodemanager.webapp.spnego-principal</name>
        <value>HTTP/_HOST@CASDDS.CASD</value>
    </property>

    <property>
        <name>yarn.nodemanager.webapp.spnego-keytab</name>
        <value>/etc/krb5.keytab</value>
    </property>

    <!-- HTTPS -->
    <property>
        <name>yarn.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>

</configuration>

```
> The krb principal of the resource manager must be specified in the `yarn-site.xml of the data node`. 
> 


> We don't need timeline service here, because spark does not use timeline service. Unless you use explicitly ATS v1 or v2,

### Configuration of hadoop-env.sh

```shell
# Java
export JAVA_HOME=/opt/java/jdk-11.0.2
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}

# Run HDFS daemons as hadoop user
export HDFS_NAMENODE_USER=hadoop
export HDFS_DATANODE_USER=hadoop
export YARN_RESOURCEMANAGER_USER=hadoop
export YARN_NODEMANAGER_USER=hadoop

# Hadoop home
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_SBIN_DIR=${HADOOP_HOME}/sbin

# Hadoop config
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}


# JVM options (base)
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.net.preferIPv4Stack=true"
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.security.krb5.conf=/etc/krb5.conf"

#
# Heap defaults (fallback values)
#
export HADOOP_HEAPSIZE_MIN=1024
export HADOOP_HEAPSIZE_MAX=2048

# Hadoop security logging, to be remove for prod
export HADOOP_SECURITY_LOGGER=INFO,RFAS,console


# NameNode JVM tuning (Java 11 optimized)
export HDFS_NAMENODE_OPTS="
  -Xms2g
  -Xmx2g
  -XX:+UseG1GC
  -XX:MaxGCPauseMillis=500
  -XX:+DisableExplicitGC
  -XX:+ParallelRefProcEnabled
  -XX:InitiatingHeapOccupancyPercent=45
  -XX:MaxMetaspaceSize=256m
  -Xlog:gc*,gc+heap=info:file=/var/log/hadoop/gc-namenode.log:time,uptime:filecount=5,filesize=100m
"

#
# DataNode JVM options (Java 11 safe)
#
export HDFS_DATANODE_OPTS="
  -XX:+UseG1GC
  -XX:MaxGCPauseMillis=500
  -XX:+DisableExplicitGC
  -Dhadoop.security.logger=ERROR,RFAS
"

```

## Log management of the yarn cluster

We will give more details on how to set up log management in the hadoop cluster in [chapiter](). But here, I just want
to highlight that, we have two types of logs for yarn:
- system logs: yarn daemon lifecycle events log (e.g. startup, shutdown, etc.), container allocation events, heartbeats 
          and registration messages. End user does not need to see these logs, and we should not allow end users to access
          these logs. 
- container logs: user submitted jobs that are running inside containers also generate logs. These logs are useful for end
         users to debug their applications, and we should allow end users to view them.

> As the utilities and audience of these logs are quite different, we should 
> - separate them in different locations
> - setup different log level
> - setup different log rotation policies.

## Validation

```shell
yarn node -list
yarn application -list
```

Yarn Web Ui: `https://deb13-spark1.casd.fr:8090
`