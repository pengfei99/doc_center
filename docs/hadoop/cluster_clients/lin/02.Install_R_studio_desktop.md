# Install Rstudio desktop on ubuntu 20

In this tutorial, we will install Rstudio desktop on ubuntu 20.

## Install the Rstudio desktop deb

You can find all the available version in ths page: https://posit.co/download/rstudio-desktop/#download

```shell
# install r
sudo apt-get install r-base

# gdebi-core lets you install local deb packages resolving and installing its dependencies
sudo apt-get install gdebi-core

# some dependency packages
sudo apt-get -y install libcurl4-gnutls-dev
sudo apt-get -y install libssl-dev

# get the rstudio desktop deb file
# note the desktop version is built with electron. So the diff between server and desktop must be very small
wget https://download1.rstudio.org/electron/focal/amd64/rstudio-2023.09.1-494-amd64.deb

# install the deb file
sudo gdebi rstudio-2023.09.1-494-amd64.deb

# launch the desktop
rstudio
```
## Install R packages

You need to run below command in **R console**

```shell
# install devtools
install.packages("devtools")

# install sparklyr
install.packages("sparklyr")
```

### Trouble shoot

If you have encounter errors such as `No package 'libxml-2.0' found` while installing sparklyr. You need to run below
command, it's a system dependency not R.

```shell
sudo apt-get install libxml2-dev 
```

### Load packages to current R session

```shell
library(sparklyr)

library(dplyr)
```

## Connect to a remote spark cluster

```shell
install.packages("devtools")

install.packages("sparklyr")

# Define your paths clearly as strings
my_spark_path <- "C:/Users/pliu/Documents/tools/spark/spark-3.5.8"
my_hadoop_path <- "C:/Users/pliu/Documents/tools/hadoop/hadoop-3.3.6"

# Set Envs (Important for the underlying shell, even if sparklyr is picky)
Sys.setenv(SPARK_HOME = my_spark_path)
Sys.setenv(HADOOP_HOME = my_hadoop_path)
Sys.setenv(HADOOP_CONF_DIR = file.path(my_hadoop_path, "etc/hadoop"))

system("java -version")

library(sparklyr)

# custom spark session config
conf <- spark_config()

# if we add nothing into the conf, the spark default conf will be loaded

conf$spark.executor.memory <- "300M"
conf$spark.executor.cores <- 1
conf$spark.executor.memory <- "1G"
conf$spark.executor.instances <- 2
conf$spark.dynamicAllocation.enabled <- "false"

conf$spark.submit.deployMode <- "client"

options(sparklyr.log.console = TRUE)
# create a spark session
# Connect by EXPLICITLY passing the spark_home
sc <- spark_connect(
  master = "yarn",
  spark_home = my_spark_path, # <--- Direct override
  config = conf,
  version = "3.5.8"
)

```
> Sparklyr has troubles to overwrite spark conf values in spark-defaults.conf. As sparklyr can only run in client mode
> if you put deployMode:cluster in spark-defaults.conf. It will cause errors such as 

```text
26/02/24 13:56:54 INFO sparklyr: Session (22473) is starting under 127.0.0.1 port 8880
26/02/24 13:56:54 INFO sparklyr: Session (22473) found port 8880 is available
26/02/24 13:56:54 INFO sparklyr: Gateway (22473) is waiting for sparklyr client to connect to port 8880
26/02/24 13:57:54 INFO sparklyr: Gateway (22473) is terminating backend since no client has connected after 60 seconds to 10.50.5.204/8880.
```

> The sparklyr client should run on localhost, but the cluster launch the  driver on another host `10.50.5.204`
>
> 
Below is a `spark-defaults.conf` works for sparklyr

```text
spark.master yarn
spark.yarn.archive hdfs:///utils/spark-libs/spark-3.5.8.zip

spark.yarn.stagingDir hdfs:///users

spark.serializer org.apache.spark.serializer.KryoSerializer
spark.rdd.compress true

spark.authenticate true
spark.network.crypto.enabled true
spark.io.encryption.enabled true

spark.executor.instances 2
spark.executor.cores 2
spark.executor.memory 1g
spark.executor.memoryOverhead 384m

spark.driver.cores 1
spark.driver.memory 1g
spark.driver.memoryOverhead 384m

spark.sql.adaptive.enabled true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.autoBroadcastJoinThreshold 32m

spark.network.timeout 600s
spark.executor.heartbeatInterval 60s
```


## Do some query

```shell
library(dplyr)

# Create a small DataFrame
df <- tibble::tibble(
  name = c("Alice", "Bob", "Charlie"),
  age  = c(25, 30, 35)
)

# Copy to Spark
sdf <- copy_to(sc, df, overwrite = TRUE)

# Show data (similar to df.show())
sdf %>% 
  print(n = Inf)

# Row count (similar to df.count())
total_rows <- sdf %>% 
  tally() %>% 
  collect()

cat("Total rows:", total_rows$n, "\n")

# Stop Spark session
spark_disconnect(sc)

```