# Install hdfs and spark clients For linux

For linux, follow the below steps

- Download jdk (11 for hadoop 3.3.6), set up JAVA_HOME in your env var
- Download the hadoop bin, set up HADOOP_HOME in your env var
- add $HADOOP_HOME/bin, $JAVA_HOME/bin into your `path`
- configure the `hadoop-env.sh` with the right java_home
- copy the dfs url from the server `core-site.xml` to your local `core-site.xml`, below is an example

## Install java

Via package manager

```bash
sudo apt update
sudo apt upgrade

# install jre
sudo apt install default-jre

# install jdk
sudo apt install default-jdk

# test jre
java -version

# test jdk
javac -version
```

Via tarball, you need to download the tarball first.

```shell
sudo mkdir -p /opt/java

sudo mv jdk11.tar.gz /opt/java

sudo tar -xzvf /opt/java/jdk11.tar.gz

sudo chmod -R 755 /opt/java
```

If you have multiple java version, you need to set up the currently used java version as the hadoop version that
requires.

```shell
# choose the right jre version
sudo update-alternatives --config java

# choose the right jdk version
sudo update-alternatives --config javac
```

Set **JAVA_HOME** path to the currently used jdk path. There are many ways to set the JAVA_HOME:

- In your own ~/.bashrc : the java config only works for you
- In /etc/profile.d/java.sh: The java config works for all users

We choose the second solution

```shell
# create a file to the export
sudo vim /etc/profile.d/java.sh

# add below content, you may need to change the jdk path
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export JAVA_HOME=/opt/java/jdk-11.0.2

# this is optional, because the install or update-alternatives already add the java bin path to the PATH
export PATH=$PATH:$JAVA_HOME/bin

# add the env var
source /etc/profile.d/java.sh

# test the value
echo $JAVA_HOME
```

## Install hadoop

You can get the latest hadoop release from this page. https://hadoop.apache.org/releases.html

In our case, we choose version `3.3.6`.

```shell
# Unzip the tar ball to `/opt/hadoop`
sudo tar -xzvf hadoop-3.3.6.tar.gz -C /opt/hadoop/ --strip-components=1

# make sure the owner of the unzip files are hadoop:hadoop
sudo chown -R root:root /opt/hadoop/
sudo chmod -R 755 /opt/hadoop/
```

Configure the `HADOOP_HOME`:

Run the below command on the three servers

```shell
# create a file to the export
sudo vim /etc/profile.d/hadoop.sh

# add below lines
export HADOOP_HOME=/opt/hadoop/hadoop-3.3.6
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
```

Reload the environment variables with `source /etc/profile.d/hadoop.sh`

## Install spark

You can get the latest stable version of spark from this site : https://spark.apache.org/downloads.html

In this tutorial, we choose version 3.4.1.

```shell
wget http://repolin.casd.fr/extra/spark/spark-x.x.x-bin-hadoop3.tgz/

tar -xzvf spark-x.x.x-bin-hadoop3 spark-x.x.x

nano /etc/profile.d/spark.sh

cd spark-x.x.x/conf
cp spark-defaults.conf.template spark-defaults.conf

nano spark-defaults.conf

hdfs_optimisation.md dfs -mkdir /spark-logs
hdfs_optimisation.md dfs -chmod 777 /spark-logs

./sbin/start-history-server.sh


# Now unzip the spark bin
tar -xzvf spark-3.4.1-bin-hadoop3.tgz

# create a folder in opt to store the spark bin
sudo mkdir -p /opt/spark
sudo mv spark-3.4.1-bin-hadoop3 /opt/spark/
sudo chown -R hadoop:root /opt/spark/
sudo mv spark-3.4.1-bin-hadoop3 spark-3.4.1
```

## Configure hdfs client

The simplest way to configure hdfs client is to take the name node config file, and put it on the client server:

- core-site.xml
- hdfs-site.xml

> You can remove some lines which are useless
> Below is an example of the core-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://deb13-spark1.casdds.casd:9000</value>
    </property>
    <!-- Enable hadoop kerberos authentication -->
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>

    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
    </property>

    <!-- Group resolution via OS / SSSD -->
    <property>
        <name>hadoop.security.group.mapping</name>
        <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
    </property>
    <!-- AD principal → local user mapping -->
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[2:$1@$0](.*@CASDDS\.CASD)s/@CASDDS\.CASD// RULE:[1:$1] DEFAULT
        </value>
        <description>Mapping du principal Kerberos vers le nom d’utilisateur local.</description>
    </property>
</configuration>

```

Below is an example of the `hdfs-site.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <!-- HDFS permissions -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>

    <!-- NameNode Kerberos identity -->
    <property>
        <name>dfs.namenode.kerberos.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SPNEGO authentication for Web UI -->
    <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SASL encryption for block transfer -->
    <property>
        <name>dfs.data.transfer.protection</name>
        <value>authentication</value>
    </property>

    <!-- Non privileged ports -->
    <property>
        <name>dfs.https.port</name>
        <value>50470</value>
    </property>

    <property>
        <name>dfs.secondary.https.port</name>
        <value>50490</value>
        <description>Port HTTPS pour le secondary-namenode.</description>
    </property>
    <property>
        <name>dfs.https.address</name>
        <value>deb13-spark1.casdds.casd:50470</value>
        <description>Adresse HTTPS d’écoute du Namenode.</description>
    </property>
    <property>
        <name>dfs.encrypt.data.transfer</name>
        <value>true</value>
    </property>

    <!-- Explicitly force HTTP -->
    <property>
        <name>dfs.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>

```

## configure yarn and spark

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- kerberos config: ResourceManager identity -->
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- kerberos config: Web UI SPNEGO -->
    <property>
        <name>yarn.resourcemanager.webapp.spnego-principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- HTTPS -->
    <property>
        <name>yarn.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>

```

## Test your client

```shell
hdfs dfs -ls /
```

Below is a pyspark job file `job1.py`
```python
from pyspark.sql import SparkSession

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("pengfei_test") \
        .getOrCreate()

    # For a basic test, create a small DataFrame
    df = spark.createDataFrame([
        ("Alice", 25),
        ("Bob", 30),
        ("Charlie", 35)
    ], ["name", "age"])

    df.show()

    # Just for validation: print row count
    print(f"Total rows: {df.count()}")

    spark.stop()

if __name__ == "__main__":
    main()


```

```bash
# submit the job in client mode
spark-submit   --master yarn   --deploy-mode client  --conf spark.yarn.stagingDir=hdfs:///users/pliu-ad --name my_pyspark_job   --num-executors 2   --executor-memory 1G   --executor-cores 1   --driver-memory 1G job1.py

# submit the job in cluster mode
spark-submit   --master yarn   --deploy-mode cluster  --conf spark.yarn.stagingDir=hdfs:///users/pliu-ad --name my_pyspark_job   --num-executors 2   --executor-memory 1G   --executor-cores 1   --driver-memory 1G   job1.py
```

> --conf spark.yarn.stagingDir=hdfs:///users/pliu-ad is essential, by default spark try to write in hdfs:///, you normally
> will get permission deny

## Optimization of spark

### Setup spark default libs in HDFS
When you submit spark jobs, you should see the below warning message. This warning means every Spark submission is 
re-uploading the entire Spark runtime to HDFS. It works, but it’s slow, wastes bandwidth, and stresses the NameNode.

```shell
WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading 
libraries under SPARK_HOME when submiting spark job. how to correct this
```
You can follow the below steps 
```shell
# step1: create spark archive for the default libs
cd $SPARK_HOME/jars
zip -r ../spark-libs.zip *

# step2: upload to hdfs
hdfs dfs -mkdir -p /utils/spark-libs
hdfs dfs -put spark-3.5.8.zip /utils/spark-libs

# step3: change spark-defaults.conf
sudo vim $SPARK_HOME/conf/spark-defaults.conf
# add the below line into the conf file
spark.yarn.archive hdfs:///utils/spark-libs/spark-3.5.8.zip

```

### Setup 'resource-types.xml'.

When you submit spark jobs, you should see the below warning message. This warning is harmless. It means YARN 
looked for an optional resource definition file and didn’t find it, so it fell back to defaults. The `resource-types.xml`
is only needed when you define custom resource types beyond CPU + memory (for example GPUs, FPGAs, or special devices).
```text
ResourceUtils: Unable to find 'resource-types.xml'.
```

### Setup Spark parallelism

When users submitting a job in spark, the user can change the spark parallelism. Don't set up the parallelisms number(partitions)
more than the cluster can process concurrently. This will create `queueing overhead`(task startup overhead is greater than actual compute time), and slow down your calculations.

The golden baseline is: `parallelism = total executor cores *(2 or 3)`. For example in my tutorial, I have 2 data nodes,
each node has 8 core available for spark. So my parallelism should be 16 * (2 or 3)= 32 or 48.

In your spark-defaults.conf you can put the below line to set up the parallelism number.

```shell
spark.default.parallelism 32
spark.sql.shuffle.partitions 32
```

Shuffle operations are used during data redistribution across partitions (e.g., in joins, groupBy, or aggregations). It's
very impacting parallelism performance due to disk I/O. We can optimise the shuffle operations with:
- spark.reducer.maxSizeInFlight: default value `48m`
- spark.shuffle.file.buffer: default value `32k`
- spark.shuffle.compress: default value `true`
- spark.shuffle.spill.compress: default value `true`

**spark.reducer.maxSizeInFlight** controls the maximum total size of map outputs (intermediate shuffle data) that 
a single reduce task can fetch simultaneously from mapper tasks over the network. Spark divides this limit across 
multiple parallel fetch requests (typically up to 5) to speed up data retrieval. Each fetched output requires 
an in-memory buffer on the reducer side, creating a fixed memory overhead per reduce task. Increasing it (e.g., to 64m) 
allows reducers to fetch larger chunks of data at once, potentially reducing network latency and improving 
throughput in clusters with high-bandwidth networks or large shuffle datasets. But it also increases memory 
consumption on reducers, as more data is buffered in memory during fetches. This can lead to `out-of-memory (OOM) errors`, 
garbage collection pauses, or executor failures if memory is limited. It's a trade-off between speed and stability.
> Only increase if you have ample executor memory (e.g., >10-20 GB per executor) and profiling shows network fetch bottlenecks. 
> In practice, values above 48m are common in large clusters but risky in smaller ones.
> 
**spark.shuffle.file.buffer** sets the size of the in-memory buffer used by each shuffle output 
stream when writing intermediate shuffle files to disk on the mapper side. Larger buffers allow more data to 
accumulate in memory before flushing to disk. A larger buffer (e.g., 1m or 1 MiB) reduces the frequency of disk 
writes, minimizing system calls, disk seeks, and I/O overhead. This can significantly improve shuffle write 
performance on systems with slower storage (e.g., HDDs) or high shuffle volumes, as fewer small writes mean 
less fragmentation and better throughput. It consumes more memory per shuffle output stream (one buffer per reducer 
partition per task). In environments with many partitions or concurrent tasks, this could lead to higher 
overall memory usage, potentially causing spills or OOMs. 
> The default is conservative and works well for most cases. Increasing to 1m (as you've done) is a common 
> recommendation for better I/O efficiency if your cluster has sufficient memory and is I/O-bound (e.g., slow disks). 
> However, test it—excessive sizes (e.g., >1m) might waste memory without proportional gains.
> 
**spark.shuffle.compress** enables compression of shuffle output files (map outputs) before they are written 
to disk or sent over the network. Compression uses the codec specified by `spark.io.compression.codec` (default: snappy).

**spark.shuffle.spill.compress** enables compression of data spilled to disk during shuffle operations when memory is 
insufficient to hold all intermediate data

### Setup spark default for all submit

We have seen various configurations which we can set up when we submit a spark job. But we don't want to 
type them every time. We can put them inside `$SPARK_HOME/conf/spark-defaults.conf`

Cluster manager config
```shell
spark.master yarn
spark.submit.deployMode cluster
spark.yarn.archive hdfs:///utils/spark-libs/spark-3.5.8.zip
```

Kerberos passthrough to yarn
```shell
spark.authenticate true
spark.network.crypto.enabled true
spark.io.encryption.enabled true
```

Default cluster ressource setup, these are intentionally small. Users can scale up explicitly.

```shell
spark.executor.instances 2
spark.executor.cores 2
spark.executor.memory 2g
spark.executor.memoryOverhead 384m

spark.driver.cores 1
spark.driver.memory 1g
spark.driver.memoryOverhead 384m
```
