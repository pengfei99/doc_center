# Install hdfs and spark clients For linux

For linux, follow the below steps

- Download jdk (11 for hadoop 3.3.6), set up JAVA_HOME in your env var
- Download the hadoop bin, set up HADOOP_HOME in your env var
- add $HADOOP_HOME/bin, $JAVA_HOME/bin into your `path`
- configure the `hadoop-env.sh` with the right java_home
- copy the dfs url from the server `core-site.xml` to your local `core-site.xml`, below is an example

## Install java

Via package manager

```bash
sudo apt update
sudo apt upgrade

# install jre
sudo apt install default-jre

# install jdk
sudo apt install default-jdk

# test jre
java -version

# test jdk
javac -version
```

Via tarball, you need to download the tarball first.

```shell
sudo mkdir -p /opt/java

sudo mv jdk11.tar.gz /opt/java

sudo tar -xzvf /opt/java/jdk11.tar.gz

sudo chmod -R 755 /opt/java
```

If you have multiple java version, you need to set up the currently used java version as the hadoop version that
requires.

```shell
# choose the right jre version
sudo update-alternatives --config java

# choose the right jdk version
sudo update-alternatives --config javac
```

Set **JAVA_HOME** path to the currently used jdk path. There are many ways to set the JAVA_HOME:

- In your own ~/.bashrc : the java config only works for you
- In /etc/profile.d/java.sh: The java config works for all users

We choose the second solution

```shell
# create a file to the export
sudo vim /etc/profile.d/java.sh

# add below content, you may need to change the jdk path
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export JAVA_HOME=/opt/java/jdk-11.0.2

# this is optional, because the install or update-alternatives already add the java bin path to the PATH
export PATH=$PATH:$JAVA_HOME/bin

# add the env var
source /etc/profile.d/java.sh

# test the value
echo $JAVA_HOME
```

## Install hadoop

You can get the latest hadoop release from this page. https://hadoop.apache.org/releases.html

In our case, we choose version `3.3.6`.

```shell
# Unzip the tar ball to `/opt/hadoop`
sudo tar -xzvf hadoop-3.3.6.tar.gz -C /opt/hadoop/ --strip-components=1

# make sure the owner of the unzip files are hadoop:hadoop
sudo chown -R root:root /opt/hadoop/
sudo chmod -R 755 /opt/hadoop/
```

Configure the `HADOOP_HOME`:

Run the below command on the three servers

```shell
# create a file to the export
sudo vim /etc/profile.d/hadoop.sh

# add below lines
export HADOOP_HOME=/opt/hadoop/hadoop-3.3.6
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
```

Reload the environment variables with `source /etc/profile.d/hadoop.sh`

## Install spark

You can get the latest stable version of spark from this site : https://spark.apache.org/downloads.html

In this tutorial, we choose version 3.4.1.

```shell
wget http://repolin.casd.fr/extra/spark/spark-x.x.x-bin-hadoop3.tgz/

tar -xzvf spark-x.x.x-bin-hadoop3 spark-x.x.x

nano /etc/profile.d/spark.sh

cd spark-x.x.x/conf
cp spark-defaults.conf.template spark-defaults.conf

nano spark-defaults.conf

hdfs_optimisation.md dfs -mkdir /spark-logs
hdfs_optimisation.md dfs -chmod 777 /spark-logs

./sbin/start-history-server.sh


# Now unzip the spark bin
tar -xzvf spark-3.4.1-bin-hadoop3.tgz

# create a folder in opt to store the spark bin
sudo mkdir -p /opt/spark
sudo mv spark-3.4.1-bin-hadoop3 /opt/spark/
sudo chown -R hadoop:root /opt/spark/
sudo mv spark-3.4.1-bin-hadoop3 spark-3.4.1
```

## Configure hdfs client

The simplest way to configure hdfs client is to take the name node config file, and put it on the client server:

- core-site.xml
- hdfs-site.xml

> You can remove some lines which are useless
> Below is an example of the core-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://deb13-spark1.casdds.casd:9000</value>
    </property>
    <!-- Enable hadoop kerberos authentication -->
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>

    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
    </property>

    <!-- Group resolution via OS / SSSD -->
    <property>
        <name>hadoop.security.group.mapping</name>
        <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
    </property>
    <!-- AD principal → local user mapping -->
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[2:$1@$0](.*@CASDDS\.CASD)s/@CASDDS\.CASD// RULE:[1:$1] DEFAULT
        </value>
        <description>Mapping du principal Kerberos vers le nom d’utilisateur local.</description>
    </property>
</configuration>

```

Below is an example of the `hdfs-site.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <!-- HDFS permissions -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>

    <!-- NameNode Kerberos identity -->
    <property>
        <name>dfs.namenode.kerberos.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SPNEGO authentication for Web UI -->
    <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SASL encryption for block transfer -->
    <property>
        <name>dfs.data.transfer.protection</name>
        <value>authentication</value>
    </property>

    <!-- Non privileged ports -->
    <property>
        <name>dfs.https.port</name>
        <value>50470</value>
    </property>

    <property>
        <name>dfs.secondary.https.port</name>
        <value>50490</value>
        <description>Port HTTPS pour le secondary-namenode.</description>
    </property>
    <property>
        <name>dfs.https.address</name>
        <value>deb13-spark1.casdds.casd:50470</value>
        <description>Adresse HTTPS d’écoute du Namenode.</description>
    </property>
    <property>
        <name>dfs.encrypt.data.transfer</name>
        <value>true</value>
    </property>

    <!-- Explicitly force HTTP -->
    <property>
        <name>dfs.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>

```

## configure yarn

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- kerberos config: ResourceManager identity -->
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- kerberos config: Web UI SPNEGO -->
    <property>
        <name>yarn.resourcemanager.webapp.spnego-principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- HTTPS -->
    <property>
        <name>yarn.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>

```

## Configure spark

Spark can take kerberos tickets automatically and submit the ticket to the hadoop cluster via yarn.
We don't need to specific configuration. We only need to make sure the user
has obtained a valid TGT ticket.

But if the spark job runs longer than the validity of the ticket, the job will fail. So the best practice
is to use a keytab file which allows spark to ask new tickets if it needs. Below is an example
of the spark-submit commands

```shell
spark-submit --master yarn --deploy-mode cluster \
  --principal hadoop-user@EXAMPLE.COM \
  --keytab /etc/security/keytabs/hadoop-user.keytab \
  --class com.example.MyApp hdfs:///user/hadoop-user/myapp.jar
```

To avoid typing the configuration in the spark-submit command. You can also configure the `$SPARK_HOME/conf/spark-defaults.conf`:

Note, the `$SPARK_HOME/conf/spark-defaults.conf` will be used by all users, so don't put user specific conf in it.
Each user can define their own spark-defaults.conf to put their personal config.

For example, in general `$SPARK_HOME/conf/spark-defaults.conf`, we can put
```shell
spark.yarn.stagingDir hdfs:///users
```
> `spark.yarn.stagingDir`: defines the root folder where spark writes the temporary files for spark jobs
> We will give you more details on how to set up spark-defaults.conf in the next section.
In personal `~/.spark/spark-defaults.conf`, you can put personal conf

```shell
spark.yarn.principal hadoop-user@EXAMPLE.COM
spark.yarn.keytab /etc/security/keytabs/hadoop-user.keytab
spark.hadoop.fs.defaultFS hdfs://namenode1.example.com:9000
```

## Test your hdfs client

```shell
hdfs dfs -ls /
```

## Test your spark client

Below is a pyspark job file `job1.py`
```python
from pyspark.sql import SparkSession

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("pengfei_test") \
        .getOrCreate()

    # For a basic test, create a small DataFrame
    df = spark.createDataFrame([
        ("Alice", 25),
        ("Bob", 30),
        ("Charlie", 35)
    ], ["name", "age"])

    df.show()

    # Just for validation: print row count
    print(f"Total rows: {df.count()}")

    spark.stop()

if __name__ == "__main__":
    main()


```

```bash
# submit the job in client mode
spark-submit   --master yarn   --deploy-mode client  --conf spark.yarn.stagingDir=hdfs:///users/ --name my_pyspark_job   --num-executors 2   --executor-memory 1G   --executor-cores 1   --driver-memory 1G job1.py

# submit the job in cluster mode
spark-submit   --master yarn   --deploy-mode cluster  --conf spark.yarn.stagingDir=hdfs:///users/pliu-ad --name my_pyspark_job   --num-executors 2   --executor-memory 1G   --executor-cores 1   --driver-memory 1G   job1.py

# submit with a spark-defaults.conf
spark-submit --name my_pyspark_job   job1.py 
```

> --conf spark.yarn.stagingDir=hdfs:///users/pliu-ad is essential, by default spark try to write in hdfs:///, you normally
> will get permission deny

## Optimization of spark

### Setup spark default libs in HDFS
When you submit spark jobs, you should see the below warning message. This warning means every Spark submission is 
re-uploading the entire Spark runtime to HDFS. It works, but it’s slow, wastes bandwidth, and stresses the NameNode.

```shell
WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading 
libraries under SPARK_HOME when submiting spark job. how to correct this
```
You can follow the below steps 
```shell
# step1: create spark archive for the default libs
cd $SPARK_HOME/jars
zip -r ../spark-libs.zip *

# step2: upload to hdfs
hdfs dfs -mkdir -p /utils/spark-libs
hdfs dfs -put spark-3.5.8.zip /utils/spark-libs

# step3: change spark-defaults.conf
sudo vim $SPARK_HOME/conf/spark-defaults.conf
# add the below line into the conf file
spark.yarn.archive hdfs:///utils/spark-libs/spark-3.5.8.zip

```

### Setup 'resource-types.xml'.

When you submit spark jobs, you should see the below warning message. This warning is harmless. It means YARN 
looked for an optional resource definition file and didn’t find it, so it fell back to defaults. The `resource-types.xml`
is only needed when you define custom resource types beyond CPU + memory (for example GPUs, FPGAs, or special devices).
```text
ResourceUtils: Unable to find 'resource-types.xml'.
```

### Setup Spark parallelism

When users submitting a job in spark, the user can change the spark parallelism. Don't set up the parallelisms number(partitions)
more than the cluster can process concurrently. This will create `queueing overhead`(task startup overhead is greater than actual compute time), and slow down your calculations.

The golden baseline is: `parallelism = total executor cores *(2 or 3)`. For example in my tutorial, I have 2 data nodes,
each node has 8 core available for spark. So my parallelism should be 16 * (2 or 3)= 32 or 48.

In your spark-defaults.conf you can put the below line to set up the parallelism number.

```shell
spark.default.parallelism 32
spark.sql.shuffle.partitions 32
```

Shuffle operations are used during data redistribution across partitions (e.g., in joins, groupBy, or aggregations). It's
very impacting parallelism performance due to disk I/O. We can optimise the shuffle operations with:
- spark.reducer.maxSizeInFlight: default value `48m`
- spark.shuffle.file.buffer: default value `32k`
- spark.shuffle.compress: default value `true`
- spark.shuffle.spill.compress: default value `true`

**spark.reducer.maxSizeInFlight** controls the maximum total size of map outputs (intermediate shuffle data) that 
a single reduce task can fetch simultaneously from mapper tasks over the network. Spark divides this limit across 
multiple parallel fetch requests (typically up to 5) to speed up data retrieval. Each fetched output requires 
an in-memory buffer on the reducer side, creating a fixed memory overhead per reduce task. Increasing it (e.g., to 64m) 
allows reducers to fetch larger chunks of data at once, potentially reducing network latency and improving 
throughput in clusters with high-bandwidth networks or large shuffle datasets. But it also increases memory 
consumption on reducers, as more data is buffered in memory during fetches. This can lead to `out-of-memory (OOM) errors`, 
garbage collection pauses, or executor failures if memory is limited. It's a trade-off between speed and stability.
> Only increase if you have ample executor memory (e.g., >10-20 GB per executor) and profiling shows network fetch bottlenecks. 
> In practice, values above 48m are common in large clusters but risky in smaller ones.
> 
**spark.shuffle.file.buffer** sets the size of the in-memory buffer used by each shuffle output 
stream when writing intermediate shuffle files to disk on the mapper side. Larger buffers allow more data to 
accumulate in memory before flushing to disk. A larger buffer (e.g., 1m or 1 MiB) reduces the frequency of disk 
writes, minimizing system calls, disk seeks, and I/O overhead. This can significantly improve shuffle write 
performance on systems with slower storage (e.g., HDDs) or high shuffle volumes, as fewer small writes mean 
less fragmentation and better throughput. It consumes more memory per shuffle output stream (one buffer per reducer 
partition per task). In environments with many partitions or concurrent tasks, this could lead to higher 
overall memory usage, potentially causing spills or OOMs. 
> The default is conservative and works well for most cases. Increasing to 1m (as you've done) is a common 
> recommendation for better I/O efficiency if your cluster has sufficient memory and is I/O-bound (e.g., slow disks). 
> However, test it—excessive sizes (e.g., >1m) might waste memory without proportional gains.
> 
**spark.shuffle.compress** enables compression of shuffle output files (map outputs) before they are written 
to disk or sent over the network. Compression uses the codec specified by `spark.io.compression.codec` (default: snappy).

**spark.shuffle.spill.compress** enables compression of data spilled to disk during shuffle operations when memory is 
insufficient to hold all intermediate data

### Spark SQL query optimization

We can ask spark to improve performance by `dynamically adjusting execution plans based on runtime statistics`. They 
are particularly relevant for `SQL queries, DataFrames, and Datasets in Spark`, where joins, aggregations, and 
partitioning can significantly impact efficiency. We will use:
- spark.sql.adaptive.enabled: default value `false`
- spark.sql.adaptive.coalescePartitions.enabled: default is `true` when `spark.sql.adaptive.enabled` is true; otherwise irrelevant
- spark.sql.autoBroadcastJoinThreshold: default value `10m`

**spark.sql.adaptive.enabled**: enables `Adaptive Query Execution (AQE)`, a runtime optimizer that re-optimizes the 
query plan after each stage based on actual data statistics collected during execution. Without `AQE`, Spark uses a 
static plan based on initial estimates, which can be suboptimal if data skew, cardinality misestimates, or varying 
data sizes occur. With AQE enabled, Spark can dynamically:
- Adjust join strategies (e.g., switch from sort-merge to broadcast hash join if one side is small).
- Handle data skew by splitting skewed partitions.
- Coalesce small partitions post-shuffle (if the sub-property is enabled).
> It can reduce execution time by 20-50% in complex queries by avoiding bad plans. Adds slight overhead for plan 
> re-optimization and stats collection (e.g., extra CPU/memory during stage boundaries)
> 
**spark.sql.adaptive.coalescePartitions.enabled** is a sub-feature of AQE that automatically coalesces (merges) 
small shuffle partitions after a shuffle stage to reduce the number of tasks in subsequent stages. Spark calculates 
the target partition size based on `spark.sql.adaptive.advisoryPartitionSizeInBytes` (default: 64MB) and merges 
partitions if they are smaller than a threshold (controlled by `spark.sql.adaptive.coalescePartitions.minPartitionNum` 
and others). This prevents **over-parallelism** where too many tiny tasks lead to scheduling overhead.

**spark.sql.autoBroadcastJoinThreshold** sets the `maximum size (in bytes) for a table/dataset` to be automatically 
broadcast (replicated) to all executors during a join. If one side of the join is smaller than this threshold, 
Spark uses a `broadcast hash join instead of a sort-merge join`, sending the small side to every node. This avoids 
shuffling the large side, reducing network I/O.

### Setup spark default for all submit

We have seen various configurations which we can set up when we submit a spark job. But we don't want to 
type them every time. We can put them inside `$SPARK_HOME/conf/spark-defaults.conf`


**Spark staging area config**, as spark need to write temporary files on hdfs and the default path is / (users normally has no write right
to here). So we need to set up a directory where users has the write right. We propose the below conf in `spark-defaults.conf`

```shell
spark.yarn.stagingDir hdfs:///users
```
Spark will expand the path to `/users/<username>/.sparkStaging`. Each user gets isolated workspace automatically.
> Make sure hdfs:///users exist

**Cluster manager config**
```shell
spark.master yarn
spark.submit.deployMode cluster
spark.yarn.archive hdfs:///utils/spark-libs/spark-3.5.8.zip
```
Serialization allow spark to use `KryoSerializer` instead of Java default Serializer.

```shell
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.rdd.compress true
```

Kerberos passthrough to yarn

```shell
spark.authenticate true
spark.network.crypto.enabled true
spark.io.encryption.enabled true
```

Default cluster ressource setup, these are intentionally small. Users can scale up explicitly.

```shell
spark.executor.instances 2
spark.executor.cores 2
spark.executor.memory 2g
spark.executor.memoryOverhead 384m

spark.driver.cores 1
spark.driver.memory 1g
spark.driver.memoryOverhead 384m
```

SQL Adaptive Execution

```shell
spark.sql.adaptive.enabled true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.autoBroadcastJoinThreshold 32m
```

Stability / Timeout Protection

```shell
spark.network.timeout 600s
spark.executor.heartbeatInterval 60s
```

