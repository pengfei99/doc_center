# Install hdfs spark cluster clients For windows

## 1. Download and install jdk

Spark requires jdk to run, so we need to install jdk first. Check your current system situation.

Open the command line by clicking **Start** > type cmd > click **Command Prompt**.

```shell
java --version

# you should see something like
```

If java is installed, you should see the version, otherwise you should see `commnad not found`.

Choose the version that your spark requires: `Spark 3.*` requires JDK 8 or 11. In this tutorial, we choose `JDK 11 (LTS)`.

You can use this [link](https://learn.microsoft.com/en-us/java/openjdk/download) to download it from windows official website.
**JDK is not cross-platform, the openjdk for Linux does not work on Windows server. You need to download the 
version complied for Windows.**

After installation, rerun 

```shell
java --version
```

> We don't recommend you to use .exe installer. We recommend you to use .zip
> By default, the installer will set java home folder at `C:\Program Files\Java\jdk-11.0.17` 

```powershell
@echo off
echo JAVA_HOME is: %JAVA_HOME%
echo Path contains Java: 
echo %PATH% | findstr /i "java"
java -version
echo If above shows version → ready for Hadoop client setup
pause
```

## 2. Download and install hadoop

You can find all hadoop releases on this [website](https://hadoop.apache.org/releases.html). But there is no official
release for Windows. But someone has done a release for windows. You need to download the `winutils for hadoop`. 
You can go to this [github page](https://github.com/cdarlint/winutils) to download the binary

Extract the `winuitls` zip, then copy all the files in bin/ (**winutils.exe**, **hadoop.dll**, **hdfs.dll**, etc) binaries 
to your original `hadoop/bin`.

Then you need to setup `HADOOP_HOME` and add `%%\bin` to your **path**.

If your path is set up correctly, you should be able to run below command from a `cmd` windows

```shell
hdfs dfs -ls /
```

### 2.1 Configure hadoop

For hadoop client to work, you need to configure three config files:
- core-site.xml
- hdfs-site.xml
- yarn-site.xml

Below is an example of `core-site.xml`
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://deb13-spark1.casdds.casd:9000</value>
    </property>
    <!-- Enable hadoop kerberos authentication -->
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>

    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
    </property>
    
    <!-- AD principal → local user mapping -->
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[2:$1@$0](.*@CASDDS\.CASD)s/@CASDDS\.CASD// RULE:[1:$1] DEFAULT
        </value>
        <description>Mapping du principal Kerberos vers le nom d’utilisateur local.</description>
    </property>
</configuration>
```
> You don't need group mapping in core-site.xml for windows

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

    <!-- HDFS permissions -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>

    <!-- NameNode Kerberos identity -->
    <property>
        <name>dfs.namenode.kerberos.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SPNEGO authentication for Web UI -->
    <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SASL encryption for block transfer -->
    <property>
        <name>dfs.data.transfer.protection</name>
        <value>authentication</value>
    </property>

    <!-- Non privileged ports -->
    <property>
        <name>dfs.https.port</name>
        <value>50470</value>
    </property>

    <property>
        <name>dfs.secondary.https.port</name>
        <value>50490</value>
        <description>Port HTTPS pour le secondary-namenode.</description>
    </property>
    <property>
        <name>dfs.https.address</name>
        <value>deb13-spark1.casdds.casd:50470</value>
        <description>Adresse HTTPS d’écoute du Namenode.</description>
    </property>
    <property>
        <name>dfs.encrypt.data.transfer</name>
        <value>true</value>
    </property>

    <!-- Explicitly force HTTP -->
    <property>
        <name>dfs.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>

```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- kerberos config: ResourceManager identity -->
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- kerberos config: Web UI SPNEGO -->
    <property>
        <name>yarn.resourcemanager.webapp.spnego-principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- HTTPS -->
    <property>
        <name>yarn.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>
```
### 2.2 Enable kerberos for hd

By default, Windows server uses native `LSA(Local Security Authority)` to manage kerberos tickets and the `native Windows ticket cache`
But the hdfs client does not use the `LSA` by default. As a result, you need to configure the `HDFS client` to look at 
the `Windows logon session` instead of a standard Kerberos credential `cache file (ccache)`.

You need to do:
1. enable `allowtgtsessionkey` in the Windows Registry
2. configure kerberos client(`C:\ProgramData\MIT\Kerberos5\krb5.ini`)
3. configure the `JAAS Login File`.
4. Edit hadoop-env.cmd to ensure hdfs client to use the right config

#### 2.2.1 Enable `allowtgtsessionkey` in the Windows Registry

To enable `allowtgtsessionkey` in the Windows Registry, you need to do the following steps:
1. Open `regedit`.
2. Navigate to: `HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Lsa\Kerberos\Parameters`.
3. Check/Create a new entry(DWORD value): `allowtgtsessionkey`. 
4. Set the value to `1`. 
5. Restart the server for this change to take effect.


#### 2.2.2 configure kerberos client

By default, you can find the kerberos client config file at `C:\ProgramData\MIT\Kerberos5\krb5.ini`. Normally you don't
need to modify it. 

Below is a template which you can follow
```ini
[libdefaults]
    default_realm = YOUR_AD_REALM.COM
    # This is crucial for Windows LSA compatibility
    default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96 rc4-hmac
    default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96 rc4-hmac
    forwardable = true

[realms]
    YOUR_AD_REALM.COM = {
        kdc = your_domain_controller.your_ad_realm.com
        admin_server = your_domain_controller.your_ad_realm.com
    }

[domain_realm]
    .your_ad_realm.com = YOUR_AD_REALM.COM
    your_ad_realm.com = YOUR_AD_REALM.COM
```

#### 2.2.3 configure the `JAAS Login File`.

You need a `JAAS (Java Authentication and Authorization Service)` configuration file to tell the HDFS client to use 
the `kerberos ticket cache generated by LSA`. You need to create a file named `gss-jaas.conf`:

For now, I stored the conf file in `C:\Users\pliu\Documents\java_krb_conf\gss-jaas.conf`
Below is a minimum version of the `gss-jaas.conf`
```conf
Client {
    com.sun.security.auth.module.Krb5LoginModule required
    useTicketCache=true
    doNotPrompt=true
    /* useTicketCache with no ticketCache path defaults to LSA on Windows */
    ticketCache="";
};
```

#### 2.2.4 Edit hadoop-env.cmd 

To ensure the `hdfs client` uses the right config, we need to add the below lines at the end of the file
`%HADOOP_HOME%\etc\hadoop\hadoop-env.cmd`

```text
@echo off

... ...

@rem A string representing this instance of hadoop. %USERNAME% by default.
set HADOOP_IDENT_STRING=%USERNAME%

set HADOOP_OPTS=%HADOOP_OPTS% -Djavax.security.auth.useSubjectCredsOnly=false
set HADOOP_OPTS=%HADOOP_OPTS% -Djava.security.krb5.conf="C:\ProgramData\MIT\Kerberos5\krb5.ini"
set HADOOP_OPTS=%HADOOP_OPTS% -Djava.security.auth.login.config="C:\Users\pliu\Documents\java_krb_conf\gss-jaas.conf"

@rem Optional: Enable Kerberos debugging if you encounter GSS errors
set HADOOP_OPTS=%HADOOP_OPTS% -Dsun.security.krb5.debug=true
```

> The last line is to enable debug mode for krb. When enabled, you should see all the krb communication details between
> the hdfs client and server


### 2.3 Run the terminal as Administrator issue.

When you open a `power-shell` as standard users, and run `hdfs dfs -ls \`. You should see the below error message

```text
... 
LSA: Error calling function Protocol status: 1312
LSA: A specified logon session does not exist. It may already have been terminated.
...
```

> The above two lines is a classic `Windows Kerberos "security wall."`. The Java process is being denied access to 
> the TGT because of how the Windows session is scoped. If you `run a terminal as Administrator`, it will work.

It is a common frustration: `UAC (User Account Control)` in Windows effectively "strips" the Kerberos 
delegation tokens from a standard user's session to prevent unauthorized processes from acting as that user.

To allow a standard (non-admin) user HDFS client to `access the LSA without elevation`, you need to address the 
Logon Session visibility.

> We need to find out how?

### 2.4 Debug

```powershell
# get hadoop conf value general form
hdfs getconf -confKey <conf-key-name>
# for example, get namenode principal
hdfs getconf -confKey dfs.namenode.kerberos.principal

```

### 2.5 T
## 3. Download and install spark