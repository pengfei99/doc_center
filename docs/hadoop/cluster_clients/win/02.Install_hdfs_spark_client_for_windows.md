# Install hdfs spark cluster clients For windows

## 1. Download and install jdk

Spark requires jdk to run, so we need to install jdk first. Check your current system situation.

Open the command line by clicking **Start** > type cmd > click **Command Prompt**.

```shell
java --version

# you should see something like
```

If java is installed, you should see the version, otherwise you should see `commnad not found`.

Choose the version that your spark requires: `Spark 3.*` requires JDK 8 or 11. In this tutorial, we choose `JDK 11 (LTS)`.

You can use this [link](https://learn.microsoft.com/en-us/java/openjdk/download) to download it from windows official website.
**JDK is not cross-platform, the openjdk for Linux does not work on Windows server. You need to download the 
version complied for Windows.**

After installation, rerun 

```shell
java --version
```

> We don't recommend you to use .exe installer. We recommend you to use .zip
> By default, the installer will set java home folder at `C:\Program Files\Java\jdk-11.0.17` 

```powershell
@echo off
echo JAVA_HOME is: %JAVA_HOME%
echo Path contains Java: 
echo %PATH% | findstr /i "java"
java -version
echo If above shows version → ready for Hadoop client setup
pause
```

## 2. Download and install hadoop

You can find all hadoop releases on this [website](https://hadoop.apache.org/releases.html). But there is no official
release for Windows. But someone has done a release for windows. You need to download the `winutils for hadoop`. 
You can go to this [github page](https://github.com/cdarlint/winutils) to download the binary

Extract the `winuitls` zip, then copy all the files in bin/ (**winutils.exe**, **hadoop.dll**, **hdfs.dll**, etc) binaries 
to your original `hadoop/bin`.

Then you need to setup `HADOOP_HOME` and add `%%\bin` to your **path**.

If your path is set up correctly, you should be able to run below command from a `cmd` windows

```shell
hdfs dfs -ls /
```

### 2.1 Configure hadoop

For hadoop client to work, you need to configure three config files:
- core-site.xml
- hdfs-site.xml
- yarn-site.xml

Below is an example of `core-site.xml`
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://deb13-spark1.casdds.casd:9000</value>
    </property>
    <!-- Enable hadoop kerberos authentication -->
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>

    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
    </property>

    <!-- Group resolution via OS / SSSD -->
    <property>
        <name>hadoop.security.group.mapping</name>
        <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
    </property>
    <!-- AD principal → local user mapping -->
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[2:$1@$0](.*@CASDDS\.CASD)s/@CASDDS\.CASD// RULE:[1:$1] DEFAULT
        </value>
        <description>Mapping du principal Kerberos vers le nom d’utilisateur local.</description>
    </property>
</configuration>
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

    <!-- HDFS permissions -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>

    <!-- NameNode Kerberos identity -->
    <property>
        <name>dfs.namenode.kerberos.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SPNEGO authentication for Web UI -->
    <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>

    <!-- SASL encryption for block transfer -->
    <property>
        <name>dfs.data.transfer.protection</name>
        <value>authentication</value>
    </property>

    <!-- Non privileged ports -->
    <property>
        <name>dfs.https.port</name>
        <value>50470</value>
    </property>

    <property>
        <name>dfs.secondary.https.port</name>
        <value>50490</value>
        <description>Port HTTPS pour le secondary-namenode.</description>
    </property>
    <property>
        <name>dfs.https.address</name>
        <value>deb13-spark1.casdds.casd:50470</value>
        <description>Adresse HTTPS d’écoute du Namenode.</description>
    </property>
    <property>
        <name>dfs.encrypt.data.transfer</name>
        <value>true</value>
    </property>

    <!-- Explicitly force HTTP -->
    <property>
        <name>dfs.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>

```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>deb13-spark1.casdds.casd</value>
        <description>Hostname of the ResourceManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>Auxiliary services for NodeManager.</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- kerberos config: ResourceManager identity -->
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>hdfs/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- kerberos config: Web UI SPNEGO -->
    <property>
        <name>yarn.resourcemanager.webapp.spnego-principal</name>
        <value>HTTP/deb13-spark1.casdds.casd@CASDDS.CASD</value>
    </property>


    <!-- HTTPS -->
    <property>
        <name>yarn.http.policy</name>
        <value>HTTPS_ONLY</value>
    </property>
</configuration>
```
### 2.2 Enable kerberos

```powershell

set HADOOP_OPTS=-Djava.security.krb5.conf=C:\ProgramData\MIT\Kerberos5\krb5.ini
```

```powershell
# get hadoop conf value general form
hdfs getconf -confKey <conf-key-name>
# for example, get namenode principal
hdfs getconf -confKey dfs.namenode.kerberos.principal

# enable debug mode for kerberos
set HADOOP_OPTS=-Dsun.security.krb5.debug=true
```
## 3. Download and install spark