{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Pengfei doc center","text":"<p>This website is build by using <code>mkdocs</code>. For more information about <code>mkdocs</code>, please visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"adminsys/os_setup/01.Lang_support/","title":"Debian server language support management","text":""},{"location":"adminsys/os_setup/01.Lang_support/#1-add-new-language-support","title":"1. Add new language support","text":"<pre><code># check installed language\nsudo locale -a\n\n# install a new language support\nsudo locale-gen en_US.UTF-8\n</code></pre>"},{"location":"adminsys/os_setup/01.Lang_support/#2-change-the-default-language-support","title":"2. Change the default language support","text":"<pre><code>sudo vim /etc/default/locale\n\n# add the below lines\nLANG=en_US.UTF-8\nLANGUAGE=\"en_US:en\"\nLC_ADDRESS=en_US.UTF-8\nLC_NAME=en_US.UTF-8\nLC_MONETARY=en_US.UTF-8\nLC_PAPER=en_US.UTF-8\nLC_IDENTIFICATION=en_US.UTF-8\nLC_TELEPHONE=en_US.UTF-8\nLC_MEASUREMENT=en_US.UTF-8\nLC_TIME=en_US.UTF-8\nLC_NUMERIC=en_US.UTF-8\n\n\n# load the new config\nsource /etc/default/locale\n\n# update the language support by using the default config\nsudo update-locale LANG=en_US.UTF-8\n</code></pre>"},{"location":"adminsys/os_setup/02.Add_debian_backports_repo/","title":"Add debian backports repo","text":"<p>The </p> <pre><code># add the backports repo to the source.list.d dir\necho \"deb http://deb.debian.org/debian bullseye-backports main\" | sudo tee /etc/apt/sources.list.d/bullseye-backports.list\n\n# update the repo index\nsudo apt update\n\n# install a package by using the backports repo\nsudo apt -t bullseye-backports install openssl\n</code></pre>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/","title":"Remote desktop access","text":"<p>If your linux runs on cloud, and you wish access this server's desktop interface, the below tutorial can help you. I have tested three solutions: - X2go: https://wiki.x2go.org/doku.php - Nomachine: https://www.nomachine.com/ - xrdp:</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#x2go-and-nomachine","title":"X2go and nomachine","text":"<p>If you have no other choice, these two solutions can be your last hope. They both have many bugs. The screen resolution for example is a pain in the ass to setup correctly. Nomachine also requires admin rights on both client and server.</p> <p>You can follow their official doc to install them.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#xrdp","title":"Xrdp","text":"<p>xrdp is a free and open-source implementation of <code>Microsoft RDP (Remote Desktop Protocol) server</code> that enables operating  systems other than Microsoft Windows (such as Linux and BSD-style operating systems) to provide a fully functional  <code>RDP-compatible remote desktop</code> experience. It works by bridging graphics from the X Window System to the  client and relaying controls from the client back to X Window Server.</p> <p>The initial versions of the XRDP project relied on a <code>local VNC server installation</code>. Due to the <code>slow performance</code> of  forwarding to a VNC server, the developers introduced the X11rdp mode,  resulting in improved draw times and  an overall better user experience. In 2019, the XRDP developers announced the xorgxrdp project as the replacement  to the <code>X11rdp mode</code>, which is the default mode that XRDP uses in new installations.</p> <p>You can visit their github page, if you are interested in how it works.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#xrdp-server-side-installation","title":"Xrdp server side installation","text":"<p>This tutorial is tested under ubuntu 24.</p> <pre><code># Step 1 \u2013 Update Ubuntu\nsudo apt-get update -y\n\nsudo apt-get upgrade #optional\n\n# Step 2 \u2013 Install XRDP\nsudo apt install xrdp -y\nsudo systemctl status \n\n# Step 3 \u2013 Configure SSL\n# xrdp daemon is lanuched by the service account xrdp, it requires certain privilege to access SSL/TLS certificates stored on the system.\n# The below command add user xrdp to the group ssl-cert\nsudo adduser xrdp ssl-cert\n\n# restart the deamon\nsudo systemctl restart xrdp\n\n# Add a Firewall Rule to allow inbound and outbound traffic on port 3389\nsudo ufw allow from 192.168.0.0/24 to any port 3389\nsudo ufw allow 3389\nsudo ufw reload \n\n# Step 4 \u2013 Test the XRDP connection\n</code></pre>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#xrdp-client-side-installation-and-configuration","title":"Xrdp client side installation and configuration","text":"<p>The client side installation happens on the pc which you want to use to connect to the server.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#for-windows","title":"For Windows:","text":"<ol> <li>Search for \u201cRemote Desktop Connection\u201d: You can do this by typing \u201cRemote Desktop Connection\u201d in the Windows search bar.</li> <li>Open the Remote Desktop Connection application: Click on the application in the search results. You should see below GUI. </li> </ol> <p> 3. Enter the Computer\u2019s IP Address or Hostname: In the Remote Desktop Connection window, you\u2019ll need to enter the  IP address or hostname of the computer you want to connect to. 4. Click \u201cConnect\u201d: Once you\u2019ve entered the required information, click the \u201cConnect\u201d button. When you get the certificate warning, click YES.</p> <p> 5. Enter Credentials: You\u2019ll be prompted to enter the username and password for the computer you are connecting to.  Ensure you have the correct credentials. The session value should be Xorg. If you choose <code>xvnc</code>, you need to install the required packages on the server side. 6. Connect: After entering the credentials, click \u201cOK\u201d or \u201cConnect\u201d to establish the RDP connection.</p> <p></p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#for-mac","title":"For Mac","text":"<ol> <li>Download Microsoft Remote Desktop from the App Store: If you don\u2019t have it already, you can download the Microsoft  Remote Desktop application from the Mac App Store.</li> <li>Open Microsoft Remote Desktop: Once installed, open the application.</li> <li>Click on \u201cNew\u201d: To create a new connection, click on the \u201c+\u201d icon or select \u201cNew\u201d from the File menu.</li> <li>Enter Connection Details: Enter the PC name or IP address, and configure other settings as needed.</li> <li>Save the Connection: Click \u201cAdd\u201d to save the connection.</li> <li>Connect: Select the newly created connection and click \u201cStart\u201d to initiate the RDP session.</li> </ol>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#for-linux","title":"For linux","text":"<p>You can download various rdp clients: - FreeRDP - rdesktop - KRDC - NeutrinoRDP</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#troubleshoot","title":"Troubleshoot","text":"<p>To debug <code>xrdp</code>, you can get the log from <code>/var/log/xrdp.log</code>.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#wrong-credential","title":"Wrong credential","text":"<p>If you get this message <code>login failed</code>, you have mistyped your details or logged in to an active session  (someone else is logged on with the same login). Linux allows multiple console connections, but only one GUI desktop connection. You need to close other GUI desktop connection</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#black-screen","title":"Black screen","text":"<p>For recent linux version(ubuntu 22, 24), you may get a BLACK screen. Because xrdp uses the Xorg(implementation of the X11 protocol of the X window System) to do the remote display. But ubuntu 22, 24 use Wayland, so it misses some important packages. </p> <p>Below command will install dbus-x11 package, which provides a D-Bus session bus for each X11 display.</p> <pre><code>sudo apt-get install dbus-x11\n\n# a script will be generated, normally you don't need to modify it. \nsudo nano /etc/xrdp/startwm.sh\n\n# just check if it matches with the below content\nif test -r /etc/profile; then\n        . /etc/profile\nfi\n\nif test -r ~/.profile; then\n        . ~/.profile\nfi\n\ntest -x /etc/X11/Xsession &amp;&amp; exec /etc/X11/Xsession\nexec /bin/sh /etc/X11/Xsession\n\n\n# if everything matches, just reboot the server\nsudo reboot\n</code></pre>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#custom-xrdp-port","title":"custom xrdp port","text":"<p>The xrdp server listens for incoming RDP connections on port number 3389 by default. </p> <p>To instruct xrdp to listen on a different port, you need to edit the <code>xrdp.ini</code></p> <pre><code>sudo vim /etc/xrdp/xrdp.ini\n\n# Locate the port directive in the [Globals] section and set the desired value. In this example, the RDP port is 49952:\nport=49952\n\n# save the file and restart the daemon\nsudo systemctl restart xrdp\n\n# don't forget change the firewall if you have one\nsudo ufw status\nsudo ufw allow 49952/tcp\nsudo ufw reload\n</code></pre> <p>For the client side, you need to specify the port number inside the <code>computer ip input</code> with the form IP_address:port_number.</p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/","title":"Debian security updates automation","text":""},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#1-apply-updates-manually","title":"1. Apply updates manually","text":"<pre><code># fetch repo updates\nsudo apt-get update\n\n#  list all available upgrades\nsudo apt list --upgradable\n\n# install updates\nsudo apt-get upgrade\n\n# clean outdated package cache:\nsudo apt-get autoclean\n\n# clean unnecessary dependencies:\nsudo apt autoremove -y\n\n# check the integrity of the apt-get, this the advance feature which is not implemented in apt. So you need to type apt-get\nsudo apt-get check\n\n# try to fix \nsudo apt --fix-broken install\n</code></pre> <p>If your linux kernel is updated, we recommend you to reboot your OS to check if everything is ok</p> <pre><code># restart \nsudo shutdown -r now\n\n# show the kernel version\nuname -mrs\n</code></pre> <p>A script which can automate the process via cron job</p> <pre><code>#!/bin/bash\nexport NEEDRESTART_MODE=a\nexport DEBIAN_FRONTEND=noninteractive\n## Questions that you really, really need to see (or else). ##\nexport DEBIAN_PRIORITY=critical\napt-get -qy clean\napt-get -qy update\napt-get -qy -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\" upgrade\n</code></pre> <p>You can notice, we set special shell variable named DEBIAN_FRONTEND, NEEDRESTART_MODE, and DEBIAN_PRIORITY to  avoid issues when running task in the backround via cron job.</p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#2-use-the-unattended-upgrades-package","title":"2. Use the unattended-upgrades package","text":"<p>There is a package called unattended-upgrades, which can install the security updates automatically in the background. We also recommend two more packages: - apt-listchanges: can compare a new package version with the one currently installed and show what has been                         changed by extracting the relevant entries from the Debian changelog and NEWS files. - bsd-mailx: traditional simple command-line-mode mail user agent</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n\n# install the packages\nsudo apt install unattended-upgrades apt-listchanges bsd-mailx\n\n# remove old conf and generate default conf\nsudo dpkg-reconfigure unattended-upgrades\n\n# Select \"Yes\" when prompted to enable automatic updates.\n</code></pre> <p>The objective of the three tools, <code>unattended-upgrades</code> install the updates, <code>apt-listchanges</code> log the changes  during the update, <code>bsd-mailx</code> send the log to user mail box.</p> <p>You can control the <code>unattended-upgrades</code> daemons with the below command.</p> <pre><code>systemctl start unattended-upgrades # start the service\nsystemctl stop unattended-upgrades # stop the service\nsystemctl restart unattended-upgrades # restart the service\nsystemctl enable unattended-upgrades # enable at boot time\nsystemctl disable unattended-upgrades # disable at boot time\nsystemctl status unattended-upgrades # get the status\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#21-configure-the-unattended-upgrades-daemon","title":"2.1 Configure the unattended-upgrades daemon","text":"<p>There are two important conf files for <code>unattended-upgrades</code> daemon: - /etc/apt/apt.conf.d/50unattended-upgrades: it's auto generated after the installation of <code>unattended-upgrades</code> - /etc/apt/apt.conf.d/20auto-upgrades: You need to add it manually or call <code>sudo dpkg-reconfigure -plow unattended-upgrades</code>                                       to generate this config file</p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#211-50unattended-upgrades","title":"2.1.1 50unattended-upgrades","text":"<p>This conf file set up the package repo origin. Below is an example</p> <pre><code># open the conf file\nsudo vim /etc/apt/apt.conf.d/50unattended-upgrades\n\n    \"origin=Debian,codename=${distro_codename},label=Debian\";\n    \"origin=Debian,codename=${distro_codename},label=Debian-Security\";\n    \"origin=Debian,codename=${distro_codename}-security,label=Debian-Security\";\n</code></pre> <p>You can skip packages by using blacklist</p> <pre><code>// Use python regular expression\n// \nUnattended-Upgrade::Package-Blacklist {\n    \"nginx\";\n        \"linux-image*\";\n};\n</code></pre> <p>You need to configure an email address to get email when there is a problem or package upgrades:</p> <pre><code>Unattended-Upgrade::Mail \"notify@server1.cyberciti.biz\";\n# Or at least send it to root user on the same system:\n# You can access root mail from /var/mails via root account\nUnattended-Upgrade::Mail \"root\";\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#212-enable-auto-cleanup-of-old-packages","title":"2.1.2 Enable Auto-Cleanup of Old Packages","text":"<p>After auto upgrades, we can also remove old unused packages</p> <pre><code>sudo vim /etc/apt/apt.conf.d/50unattended-upgrades\n\n# enable this line\nUnattended-Upgrade::Remove-Unused-Kernel-Packages \"true\";\nUnattended-Upgrade::Remove-Unused-Dependencies \"true\";\n\n# we don't recommend auto reboot at all\nUnattended-Upgrade::Automatic-Reboot \"false\";  # Reboots automatically if required\n# if you set auto reboot to true, you need also set the reboot time\nUnattended-Upgrade::Automatic-Reboot-Time \"03:00\";  # Set the reboot time (change as needed)\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#213-enable-periodic-updates","title":"2.1.3 Enable Periodic Updates","text":"<p>/etc/apt/apt.conf.d/20auto-upgrades</p> <p>This config file activates the <code>unattended-upgrades</code> daemon. It also sets how often the apt clean the unnecessary packages.</p> <p>We recommend you add at least the below three lines in this config file.</p> <pre><code># Update-Package-Lists is like apt update, you can choose 0, 1, 2, etc\n# \"0\" : Disable automatic updates.\n# \"1\" : Update package lists daily.\n# \"2\" : Update every 2 days, etc.\n# in our case, it runs every 7 days\nAPT::Periodic::Update-Package-Lists \"7\";\n\n# like apt upgrade\nAPT::Periodic::Unattended-Upgrade \"7\";\n\n# set how often the clean will be done\nAPT::Periodic::AutocleanInterval \"15\";\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#22-configure-the-apt-listchanges","title":"2.2. Configure the apt-listchanges","text":"<p>The main config file of this daemon is  <code>/etc/apt/listchanges.conf</code>. Below is an example</p> <pre><code>[apt]\nfrontend=pager\nwhich=news\nemail_address=root\nemail_format=text\nconfirm=false\nheaders=false\nreverse=false\nsave_seen=/var/lib/apt/listchanges.db\n</code></pre> <p>change the mail_address if you want to redirect the mail to another mail box. </p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#23-test-your-installation","title":"2.3 Test your installation","text":"<pre><code>sudo unattended-upgrades --dry-run --debug\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#3-view-and-config-the-upgrade-schedules","title":"3. View and config the upgrade schedules","text":"<p>In debian <code>Debian 11/10</code> Unattended Upgrades daemon uses <code>systemd timer</code> to schedules the updates.  To view schedule value, use the below command</p> <pre><code># schedules used for download packages\nsystemctl cat apt-daily.timer \n\n# output example\n# /lib/systemd/system/apt-daily.timer\n[Unit]\nDescription=Daily apt download activities\n\n[Timer]\nOnCalendar=*-*-* 6,18:00\nRandomizedDelaySec=12h\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n\n\n# schedules used for upgrade packages\nsystemctl cat apt-daily-upgrade.timer\n\n# output example\n# /lib/systemd/system/apt-daily-upgrade.timer\n[Unit]\nDescription=Daily apt upgrade and clean activities\nAfter=apt-daily.timer\n\n[Timer]\nOnCalendar=*-*-* 6:00\nRandomizedDelaySec=60m\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#31-modify-the-default-schedules","title":"3.1 Modify the default schedules","text":"<p>Edit the schedules used for download packages</p> <pre><code>systemctl edit apt-daily.timer \n# restart the service\nsudo systemctl restart apt-daily.timer \n# check the status\nsystemctl status apt-daily.timer \n</code></pre> <p>Edit the schedules used for upgrade packages</p> <pre><code>systemctl edit apt-daily-upgrade.timer\nsudo systemctl restart apt-daily-upgrade.timerr\nsystemctl status apt-daily-upgrade.timer\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#4-trouble-shoot","title":"4. Trouble shoot","text":"<p>If you encounter problems, you can check the log of the <code>unattended-upgrades</code> daemon. </p> <pre><code>tail -f /var/log/unattended-upgrades/unattended-upgrades-shutdown.log\n</code></pre>"},{"location":"adminsys/os_setup/06.Accept_private_ca_certificate/","title":"Accept private ca certificate","text":"<p>If you want your server to accept custom certificates, you can follow the below steps. There are two scenarios:  1. You have a self-signed certificate  2. You have a certificate which signed by a CA, but the CA is not recognized by the server by default.</p> <p>For <code>scenario 1</code>, you just copy the self-signed certificate. For <code>scenario 2</code>, you should copy the root CA certificate, so all the certificate signed by this CA will be accepted in the future.</p>"},{"location":"adminsys/os_setup/06.Accept_private_ca_certificate/#1-convert-certificate-to-accepted-format","title":"1. Convert certificate to accepted format","text":"<p>Debian only accepts certificate of format .crt or .pem. If your certificate is in other formats, you need to convert them into the accepted format</p> <pre><code># convert der to crt\nopenssl x509 \\\n       -inform der -in domain.der \\\n       -out domain.crt\n\n# convert pcks7 to crt\nopenssl pkcs7 \\\n       -in domain.p7b \\\n       -print_certs -out domain.crt\n\n# convert pkcs12 to crt\nopenssl pkcs12 \\\n       -in domain.pfx \\\n       -nodes -out domain.combined.crt\n</code></pre> <p>Certain certificate format contains also the private key, so pay attention on the output file, don't leak the private key.</p>"},{"location":"adminsys/os_setup/06.Accept_private_ca_certificate/#2-add-certificate-as-trusted","title":"2. Add certificate as trusted","text":"<pre><code># to keep track off the custom ca we create a sub-folder\nsudo mkdir /usr/local/share/ca-certificates/casd-ca\n\n# copy the certificate\nsudo cp your-ca.crt /usr/local/share/ca-certificates/casd-ca/.\n\n# ask debian to load the new certificate\nsudo update-ca-certificates\n\n# test it with a site which uses the certificate or signed by the certificate\ncurl https://target-url\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/","title":"Configure a shared folder in linux","text":"<p>The goal of this tutorial is to show how to set up a shared folder for all users. Users must be able to access data inside this folder (read write and execute by default) without the owner of the data changing the acl manually.</p> <p>The command such as <code>cp, mv</code> conserve the origin ACL of the data, so even the default ACL of the shared folder allows all users to access the data, but if the data is created in another folder and copied in the shared folder, by default the data conserves the origin ACL. As a result, the data may not be accessible </p> <p>The idea is : 1. create a shared folder called <code>/home/common</code> 2. set default ACL to o::rwx (give others read, write rights.) 3. set up a systemd to auto change ACL, when copy or move data to the shared folder</p>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#1-create-the-shared-folder","title":"1. Create the shared folder","text":"<pre><code># the owner and group will be root:root\nsudo mkdir /home/common\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#2-setup-default-acl-for-the-shared-folder","title":"2. Setup default ACL for the shared folder","text":"<p>Run the below command to install the required packages</p> <pre><code># install required packages\nsudo apt update\n\nsudo apt install inotify-tools acl -y\n</code></pre> <ul> <li>acl: offers more options than basic chmod</li> <li>inotify-tools: overwatch a folder, when a waiting event happens, it can trigger target actions</li> </ul> <p>Configure default ACL </p> <pre><code># by default we grant full access for others. For the owner and group, the origin ACL will be conserved.\nsudo setfacl -d -m o::rwx /home/common\n</code></pre> <p>After this step, all files and folders created in the shared folder will inherit the default ACL </p>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#3-configure-a-systemd-daemon-to-auto-update-acl","title":"3. Configure a systemd daemon to auto update ACL","text":""},{"location":"adminsys/os_setup/07.Setup_shared_folder/#31-create-the-daemon-script","title":"3.1 Create the daemon script","text":"<p>Create the daemon script in <code>/usr/local/bin</code></p> <pre><code># choose your favorite editor\nsudo vim /usr/local/bin/update_acl.sh\n</code></pre> <p>Copy the below script in the file</p> <pre><code>#!/bin/bash\n\n# the dir which the daemon will watch\nWATCH_DIR=\"/home/common\"\n# the ACL will be enforced by the daemon\nACL_PERMISSIONS=\"o::rwx\"\n\ninotifywait -m -r -e close_write,moved_to,create \"$WATCH_DIR\" --format \"%w%f\" |\nwhile read NEWITEM; do\n  # check if the new coming item is a directory or a file\n    if [ -d \"$NEWITEM\" ]; then\n        echo \"Fixing ACL for new directory: $NEWITEM\"\n        # -R means recursively update the ACL of the new directory.\n        setfacl -R -m \"$ACL_PERMISSIONS\" \"$NEWITEM\"\n        # -d sets default ACL so future files in the new directory inherit correct permissions.\n        setfacl -d -m \"$ACL_PERMISSIONS\" \"$NEWITEM\"\n    else\n        echo \"Fixing ACL for new file: $NEWITEM\"\n        setfacl -m \"$ACL_PERMISSIONS\" \"$NEWITEM\"\n    fi\ndone\n</code></pre> <p>make the script executable</p> <pre><code>sudo chmod +x /usr/local/bin/update_acl.sh\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#32-create-the-systemd-daemon-launcher-for-update_aclsh","title":"3.2 Create the systemd daemon launcher for update_acl.sh","text":"<p>The systemd daemon launcher must be located at <code>/etc/systemd/system/</code>. By convention, we name it as <code>update_acl.service</code></p> <p>Open the file with your favorite editor</p> <pre><code>sudo vim /etc/systemd/system/update_acl.service\n</code></pre> <p>Copy the below lines in the file</p> <pre><code>[Unit]\nDescription=Update ACLs for date copied to shared directory\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/update_acl.sh\nRestart=always\nUser=root\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#33-enable-the-systemd-daemon","title":"3.3 Enable the systemd daemon","text":"<pre><code># reload the daemon list from the repository\nsudo systemctl daemon-reload\n\n# enable the service for startup\nsudo systemctl enable update_acl.service\n\n# start the service \nsudo systemctl start update_acl.service\n\n# check the satus\nsudo systemctl status update_acl.service\n\n# stop the service\nsudo systemctl stop update_acl.service\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#4-test-the-solution","title":"4. Test the solution","text":"<p>After the above steps, you need to login to the server with two different users: - user1 - user2</p> <p>user1 actions</p> <pre><code>#  create a file in his home\ntouch ~/test1.txt\n\n# set the acl to owner only, \nchmod 0700 ~/test.txt\n\n# copy the file to the /home/common\ncp ~/test.txt /home/common\n\n# create a file directly in the shared folder\ncd /home/common\n\n# create a file\ntouch test2.txt\n</code></pre> <p>user2 actions</p> <pre><code># go to the share folder\ncd /home/common\n\n# list the existing files\nls -lah\n\n# show the content of test1 and test2\ncat test1.txt\ncat test2.txt\n</code></pre> <p>If user2 can show the content, it means the daemon works well. If user2 see <code>permission deny</code>, it means something went wrong. Call admin linux</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/","title":"Upgrade Debian from 11 to 13","text":"<p>If you are under debian 11, and you want to upgrade to 13. You can follow the below steps.</p> <p>Debian only supports upgrades one major version at a time, so you must pass through Bookworm (12) before reaching Trixie (13).</p> <p>So we need to upgrade debian 11 to 12, then to 13.</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#1-preparation-of-debian-11","title":"1. Preparation of debian 11.","text":"<p>To make sure your debian 11 is ready. Let's run the below commands</p> <pre><code># make sure you have the latest debian 11 packages\nsudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n\n# check your architecture\ndpkg --print-architecture\n\n# check your release version\nlsb_release -a\n\n# if you don't have lsb_release commands, you can install it\n# lsb-release is the package name. The command is lsb_release\nsudo apt install lsb-release\n\nsudo reboot\n</code></pre>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#2-upgrade-to-debian-12bookworm","title":"2. Upgrade to Debian 12(Bookworm)","text":"<p>Edit the sources list by replacing <code>bullseye</code> source with <code>bookworm</code> source.</p> <pre><code># open the source.list file and comment the old sources\nsudo vim /etc/apt/sources.list\n\n# add the new debian 12 sources\ndeb http://deb.debian.org/debian bookworm main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free non-free-firmware\n</code></pre> <p>Start the upgrade process</p> <pre><code>sudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n\n# check your release version\nlsb_release -a\n\n# you should see debian 12 as output\n\n# restart the server\nsudo reboot\n</code></pre> <p>During the upgrade, you may be asked to confirm if you want to use the new default conf or your old conf for <code>sudoer</code> or <code>sshd</code> I recommend you to keep your version. Because the new conf may remove your sudo rights or ssh access.</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#3-upgrade-to-debian-13trixie","title":"3. Upgrade to Debian 13(Trixie)","text":"<p>Edit the sources list by replacing <code>bookworm</code> source with <code>Trixie</code> source.</p> <pre><code># open the source.list file and comment the old sources\nsudo vim /etc/apt/sources.list\n\n# add the new debian 13 sources\ndeb http://deb.debian.org/debian trixie main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian-security trixie-security main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian trixie-updates main contrib non-free non-free-firmware\n</code></pre> <p>Start the upgrade process</p> <pre><code>sudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n\n# check your release version\nlsb_release -a\n\n# you should see debian 13 as output\n\n# restart the server\nsudo reboot\n</code></pre> <p>During the upgrade, you may be asked to confirm if you want to use the new default conf or your old conf for <code>sudoer</code> or <code>sshd</code> I recommend you to keep your version. Because the new conf may remove your sudo rights or ssh access.</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#3-post-upgrade-cleanup","title":"3. Post-upgrade cleanup","text":"<pre><code># Check services that were replaced or modified:\nsystemctl --failed\n\n# verify the kernel\nuname -a\n\n# check your release version\nlsb_release -a\n\n# verify the firmware\ndpkg -l | grep firmware\n</code></pre>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/","title":"Configure Postfix MTA as Send-Only on Debian 11","text":""},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#1-setup-server-hostname","title":"1 Setup server hostname","text":"<p>The hostname of the server will be used as the name of the sender of the emails. So you should keep it nice and clean</p> <pre><code># get the current hostname\nhostname\n\n# if the name does not fit you, you can set up a new hostname\nsudo hostnamectl set-hostname smtp.casd.local --static\n</code></pre>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#2install-the-packages","title":"2Install the packages","text":"<pre><code># Install mailutils package\nsudo apt install mailutils\n\n# install postfix\nsudo apt install postfix\n</code></pre> <p>As the <code>postfix</code> package installs, you\u2019ll be asked to select an option on screen for your mail server.  For General type of email configuration window, select Internet site and click OK button. Here we suppose your server has internet connexion.</p> <p>The next page will ask you to set your Mail server name, this can be domain or server hostname with an A record. In this tutorial, we choose the host name of the server <code>smtp.casd.local</code>.</p>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#3-configure-postfix-mta-server","title":"3. Configure Postfix MTA Server","text":"<p>Edit Postfix configuration file /etc/postfix/main.cf to ensure it is configured as send only ( Only relaying emails from the local server).</p> <p>Set Postfix to listen on the 127.0.0.1loopback interface. <code>The default setting is to listen on all interfaces</code></p> <pre><code># open the conf file\nsudo vim /etc/postfix/main.cf\n\n# edit the below line\ninet_interfaces=loopback-only\nmyhostname=smtp.casd.local\n\n# restart the postfix service\nsudo systemctl restart postfix\n</code></pre>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#4-test-the-postfix-service","title":"4. Test the postfix service","text":"<p>To test email delivery, use the mail command like below.</p> <pre><code># send a mail to userx@example.com with title `Postfix Testing` and content `Postfix Send-Only Server`\necho \"Postfix Send-Only Server\" | mail -s \"Postfix Testing\" userx@example.com\n</code></pre> <p>Check the junk mails, you may find it there.</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/","title":"Ansible roles","text":"<p>Read this doc  for more details on how to write an ansible role</p> <p>An Ansible Role is a <code>self-contained, portable unit</code> of Ansible automation that serves as the preferred method for  grouping related tasks and associated variables, files, handlers, and other assets in a known file structure.  While automation tasks can be written exclusively in an Ansible Playbook, Ansible Roles allow you to create bundles  of automation content that can be  - run in 1 or more plays,  - reused across playbooks,  - shared with other users in collections.</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#role-organization","title":"role organization","text":"<p><code>Ansible Roles</code> are expressed in YAML files. When a role is included in a task or a play, Ansible looks for  a <code>main.yml</code> file in at least 1 of 8 standard role directories such as : - tasks,  - handlers,  - modules,  - defaults,  - variables,  - files,  - templates, - meta.</p> <pre><code>roles/\n    my_role1/               # this hierarchy represents a \"role\"\n        tasks/            #\n            main.yml      #  &lt;-- tasks file can include smaller files if warranted\n        handlers/         #\n            main.yml      #  &lt;-- handlers file\n        templates/        #  &lt;-- files for use with the template resource\n            ntp.conf.j2   #  &lt;------- templates end in .j2\n        files/            #\n            bar.txt       #  &lt;-- files for use with the copy resource\n            foo.sh        #  &lt;-- script files for use with the script resource\n        vars/             #\n            main.yml      #  &lt;-- variables associated with this role\n        defaults/         #\n            main.yml      #  &lt;-- default lower priority variables for this role\n        meta/             #\n            main.yml      #  &lt;-- role dependencies\n        library/          # roles can also include custom modules\n        module_utils/     # roles can also include custom module_utils\n        lookup_plugins/   # or other types of plugins, like lookup in this case\n\n    my_role2/              # same kind of structure as \"my_role1\" was above, but for another purpose\n    my_role3/              # \"\"\n    my_role4/              # \"\"\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#role-vs-playbook","title":"Role vs Playbook","text":"<p>Why use an Ansible Role instead of an Ansible Playbook? Ansible Roles and Ansible Playbooks are both tools for organizing and executing automation tasks, but each serves a different purpose. Whether you choose to create Ansible Roles or write all of your tasks in an Ansible Playbook depends on your specific use case and your experience with Ansible.</p> <p>Most automation developers and system administrators begin creating automation content with individual playbooks. A playbook is a list of automation tasks that execute for a defined inventory. Tasks can be organized into a play\u2014a grouping of 1 or more tasks mapped to a specific host and executed in order. A playbook can contain 1 or more plays, offering a flexible mechanism for executing Ansible automation in a single file.</p> <p>While playbooks are a powerful method for automating with Ansible, writing all of your tasks in a playbook isn\u2019t always the best approach. In instances where scope and variables are complex and reusability is helpful, creating most of your automation content in Ansible Roles and calling them within a playbook may be the more appropriate choice.</p> <p>The following example illustrates the use of a role, linux-systemr-roles.timesync, within a playbook. In this instance, over 4 tasks would be required to achieve what the single role accomplishes. </p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#creating-a-role","title":"Creating a role","text":"<p>You can create a new role skeleton by using <code>ansible-galaxy</code></p> <pre><code> ansible-galaxy role init role_name\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#sharing-a-role","title":"Sharing a role","text":"<p>There are few ways to share your ansible roles:</p> <ul> <li>Ansible Galaxy: A free repository for sharing roles and other Ansible content with the larger Ansible community.             Roles can be uploaded to Ansible Galaxy via the command-line (CLI), whereas collections can be shared                 from the web interface. Since Ansible Galaxy is a community site, content is not vetted, certified.</li> <li>Ansible automation hub: repo for <code>Red Hat Ansible Automation Platform</code>, which is a central repository for                         finding, downloading, and sharing <code>Ansible Content Collections</code>.</li> <li>Private automation hub: An on-premise repository. You can share roles and other automation content within your                             enterprise, allowing teams to simplify workflows and speed up automation. </li> </ul>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-roles-in-an-ansible-playbook","title":"Use roles in an ansible playbook","text":"<p>There are three ways to integre an <code>ansible role</code> in an <code>ansible playbook</code>. - Use the <code>roles</code> option in playbook - Use the <code>include_role</code> in a task  - Use the <code>import_role</code> in a task</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-roles-option-in-playbook","title":"Use the <code>roles</code> option in playbook","text":"<p>Below is an example of a playbook which calls the role <code>configure_sshd_pam_sssd_openldap</code> and <code>intall_nginx</code> before tasks. </p> <p>If you have multiple roles, the order is not guarantied with this approach. The roles are executed before tasks. If you want to order the task and roles, use the <code>include_role</code> or <code>import_role</code></p> <pre><code>---\n- hosts: linux_servers\n  roles:\n    - configure_sshd_pam_sssd_openldap\n    - install_nginx\n  tasks:\n    - name: task1\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-include_role-in-a-task","title":"Use the <code>include_role</code> in a task","text":"<p>The content of the role is parsed during the execution of the task. </p> <pre><code>---\n- hosts: linux_servers\n  tasks:\n    - name: Print a message\n      ansible.builtin.debug:\n        msg: \"this task runs before the role1\"\n\n    - name: Include the role with name role1\n      ansible.builtin.include_role:\n        name: role1\n      vars:\n        dir: '/opt/a'\n        app_port: 5000\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-import_role-in-a-task","title":"Use the <code>import_role</code> in a task","text":"<p>The content of the role is parsed at the start of the playbook. </p>"},{"location":"adminsys/os_setup/security/","title":"Debian server security docs","text":"<p>In this folder, we store all documentation about debian server security.</p>"},{"location":"adminsys/os_setup/security/#1-sshd-authentication","title":"1. SSHD Authentication","text":"<p>The most common way to remote access a debian server is via ssh protocol. On the sever side, a sshd daemon runs as  ssh server that listens to port 22 (default port). It supports many authentication mechanisms such as:</p> <ul> <li>Password Authentication (Using /etc/shadow) : This method only works for local users (not LDAP, SSSD, or Kerberos users). In <code>/etc/ssh/sshd_config</code>, put <code>UsePAM no \\n PasswordAuthentication yes</code></li> <li>Public Key Authentication (No Password Required): SSHD checks if the user private key matches a valid public key in ~/.ssh/authorized_keys.</li> <li>GSSAPI/Kerberos Authentication: SSHD can authenticate users using GSSAPI (Kerberos-based authentication) without PAM</li> <li>PAM (Pluggable Authentication Modules) Recommended: It supports multiple authentication methods (LDAP, Kerberos, SSSD, MFA).</li> </ul>"},{"location":"adminsys/os_setup/security/#11-terms","title":"1.1 Terms","text":"<p>On linux server, to allow user remote access, we use many daemons:</p> <ul> <li>sshd</li> <li>pam(Pluggable Authentication Modules):</li> <li>sssd(System Security Services Daemon)Recommended: Provides access to remote identity and authentication providers, such as LDAP, Active Directory (AD), FreeIPA, or Kerberos.</li> <li>nslcd(Name Service LDAP Daemon) deprecated: Connects the Name Service Switch (NSS) and PAM to an LDAP directory for user authentication and identity lookup.</li> <li>nscd(Name Service Cache Daemon): Caches results from services like DNS and LDAP to reduce query load. sssd has its own caching mechanism, do not recommend when using sssd.</li> </ul>"},{"location":"adminsys/os_setup/security/#12-ssh-client-server-authentication-workflow","title":"1.2 SSH client server authentication workflow","text":"<p>In the below section, we describe the authentication  workflow of a ssh server configured with <code>sshd -&gt; pam -&gt; sssd -&gt; krb/openldap</code></p>"},{"location":"adminsys/os_setup/security/#step-1-ssh-client-initiates-connection","title":"Step 1. SSH Client Initiates Connection","text":"<p>A user runs:</p> <pre><code>ssh user@debian-server\n</code></pre> <p>The SSH daemon (sshd) on the Debian server receives the connection request and begins the authentication process.</p>"},{"location":"adminsys/os_setup/security/#step2-sshd-hands-authentication-to-pam","title":"Step2. SSHD Hands Authentication to PAM","text":"<p>SSHD is configured to use PAM (Pluggable Authentication Modules) for user authentication. It checks /etc/pam.d/sshd, which includes configurations for authentication backends. PAM invokes the relevant authentication module, in this case, SSSD.</p>"},{"location":"adminsys/os_setup/security/#step3-pam-calls-sssd-for-authentication","title":"Step3. PAM Calls SSSD for Authentication","text":"<p>PAM is configured to use SSSD via the module: /etc/pam.d/common-auth</p> <p>You should see the below line which tells pam to query sssd for authentication</p> <pre><code>auth    [success=1 default=ignore]    pam_sss.so\n</code></pre>"},{"location":"adminsys/os_setup/security/#step-4-sssd-queries-openldapkerberos-for-user-authentication","title":"Step 4. SSSD Queries OpenLDAP/kerberos for User Authentication","text":"<p>SSSD Configuration (/etc/sssd/sssd.conf) specifies OpenLDAP as the backend. SSSD checks if the credentials are cached: If cached, it allows authentication without querying OpenLDAP (useful for offline authentication). If not cached, SSSD sends the authentication request to OpenLDAP.</p>"},{"location":"adminsys/os_setup/security/#step-5-openldap-validates-user-credentials","title":"Step 5. OpenLDAP Validates User Credentials","text":"<p>OpenLDAP checks: If the user exists in the LDAP directory (uid=user). The password stored in LDAP. If using LDAP bind authentication, OpenLDAP attempts to bind as the user with the provided password. If using Kerberos (via LDAP), OpenLDAP defers authentication to a Kerberos KDC.</p>"},{"location":"adminsys/os_setup/security/#step-6-authentication-result-passed-back","title":"Step 6. Authentication Result Passed Back","text":"<p>If authentication is successful, OpenLDAP responds to SSSD. SSSD caches the credentials (if caching is enabled). SSSD informs PAM of the successful authentication. PAM notifies SSHD that the user is authenticated.</p>"},{"location":"adminsys/os_setup/security/#step-7-sshd-grants-access","title":"Step 7. SSHD Grants Access","text":"<p>If the user has the correct authorization (i.e., shell access, SSH keys, group policies), SSHD grants access. The user gets a shell on the Debian server. Authentication Flow Summary SSH Client \u2192 Requests login from SSHD. SSHD \u2192 Delegates authentication to PAM. PAM \u2192 Calls pam_sss.so to use SSSD. SSSD \u2192 Queries OpenLDAP (or checks cache). OpenLDAP \u2192 Verifies user credentials. SSSD \u2192 Returns authentication result to PAM. PAM \u2192 Informs SSHD. SSHD \u2192 Grants or denies access.</p>"},{"location":"adminsys/os_setup/security/#additional-notes","title":"Additional Notes","text":"<p>If 2FA (Two-Factor Authentication) is enabled, PAM may prompt for additional verification. If public key authentication is used, SSHD may bypass PAM and authenticate using the user's SSH key. If the LDAP server is down, authentication fails unless SSSD has cached credentials. Authorization (e.g., checking if a user belongs to a specific group) is usually done via sssd's access_provider settings.</p> <p><code>/etc/pam.d/sshd</code></p> <pre><code># PAM configuration for the Secure Shell service\n\n# Standard Un*x authentication.\n@include common-auth\n\n# Disallow non-root logins when /etc/nologin exists.\nauth       required     pam_env.so\nauth       sufficient   pam_unix.so nullok \nauth       sufficient   pam_sss.so use_first_pass\nauth       required     pam_deny.so\n\naccount    required     pam_unix.so\naccount    sufficient   pam_sss.so\n\npassword   required    pam_sss.so\nsession    required    pam_sss.so\n\n# Uncomment and edit /etc/security/access.conf if you need to set complex\n# access limits that are hard to express in sshd_config.\n# account  required     pam_access.so\n\n# Standard Un*x authorization.\n@include common-account\n\n# SELinux needs to be the first session rule.  This ensures that any\n# lingering context has been cleared.  Without this it is possible that a\n# module could execute code in the wrong domain.\nsession [success=ok ignore=ignore module_unknown=ignore default=bad]        pam_selinux.so close\n\n# Set the loginuid process attribute.\nsession    required     pam_loginuid.so\n\n# Create a new session keyring.\nsession    optional     pam_keyinit.so force revoke\n\n# Standard Un*x session setup and teardown.\n@include common-session\n\n# Print the message of the day upon successful login.\n# This includes a dynamically generated part from /run/motd.dynamic\n# and a static (admin-editable) part from /etc/motd.\nsession    optional     pam_motd.so  motd=/run/motd.dynamic\nsession    optional     pam_motd.so noupdate\n\n# Print the status of the user's mailbox upon successful login.\nsession    optional     pam_mail.so standard noenv # [1]\n\n# Set up user limits from /etc/security/limits.conf.\nsession    required     pam_limits.so\n\n# Read environment variables from /etc/environment and\n# /etc/security/pam_env.conf.\nsession    required     pam_env.so # [1]\n# In Debian 4.0 (etch), locale-related environment variables were moved to\n# /etc/default/locale, so read that as well.\nsession    required     pam_env.so user_readenv=1 envfile=/etc/default/locale\n\n# SELinux needs to intervene at login time to ensure that the process starts\n# in the proper default security context.  Only sessions which are intended\n# to run in the user's context should be run after this.\nsession [success=ok ignore=ignore module_unknown=ignore default=bad]        pam_selinux.so open\n\n# Standard Un*x password updating.\n@include common-password\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_local_account/","title":"Configure ssh server on debian 11","text":"<p>An <code>SSH Server</code> is a service that runs as a <code>daemon background process (systemd service)</code> on a computer (server) and  allows <code>secure remote access</code> using the <code>Secure Shell (SSH) protocol</code>. </p> <p>It listens for incoming connections on <code>port 22 (by default)</code>. It then authenticates users, creates a  secure encrypted session, and allows remote execution of commands.</p> <p>It enables users to:   - Log in remotely by using <code>passwords, SSH keys, or Kerberos authentication</code>.   - Execute commands on a remote system.   - Transfer files securely (using scp or sftp).   - Forward network connections (port forwarding, tunneling).</p>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_local_account/#general-workflow","title":"General workflow","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/","title":"Configure debian server ssh to use pam ldap","text":"<p>We will use libpam-ldapd as the ldap server client and server authenticator to check user login and password via ldap server. It is a newer alternative to the original <code>libpam-ldap</code>. libpam-ldapd uses the same backend <code>(nslcd)</code> as <code>libnss-ldapd</code>, and thus also shares the same configuration file <code>(/etc/nslcd.conf)</code> for LDAP connection parameters. If you're already using libnss-ldapd for NSS, it may be more convenient to use libpam-ldapd's pam_ldap implementation.</p> <p>The /etc/pam.d/common-* files are managed by pam-auth-update (from libpam-runtime).</p> <p>The libpam-ldapd package includes <code>/usr/share/pam-configs/ldap</code>, and running <code>dpkg-reconfigure libpam-runtime</code> will let you configure the <code>pam_unix/pam_ldap</code> module(s) to use in /etc/pam.d/common-*.</p> <p>The nslcd is the name service LDAP connection daemon.</p> <p>Installing the libpam-ldapd package will automatically select the pam_ldap module for use in /etc/pam.d/common-*.</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#61-install-the-required-packages","title":"6.1 Install the required packages","text":"<pre><code>sudo apt-get install libnss-ldapd libpam-ldapd\n</code></pre> <p>After the installation, a pop-up window will require you to enter the <code>ldap uri</code> and the <code>base dn</code> of the ldap server</p> <p>For example</p> <pre><code>ldap_uri: ldap://10.50.5.57/ or ldap://ldap.casd.local/\n\nldap_base_dn: dc=casd,dc=local\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#62-edit-the-config","title":"6.2 Edit the config","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#621-the-first-config-is-etcnslcdconf","title":"6.2.1 The first config is <code>/etc/nslcd.conf</code>","text":"<p>As you already enter some information during installation. This file is filled with some info.</p> <p>Below is a working example.</p> <pre><code># /etc/nslcd.conf\n# nslcd configuration file. See nslcd.conf(5)\n# for details.\n\n# The user and group nslcd should run as.\nuid nslcd\ngid nslcd\n\n# The location at which the LDAP server(s) should be reachable.\nuri ldap://10.50.5.57/\n\n# The search base that will be used for all queries.\nbase dc=casd,dc=local\n\n# The LDAP protocol version to use.\n#ldap_version 3\n\n# The DN to bind with for normal lookups.\n#binddn cn=annonymous,dc=example,dc=net\n#bindpw secret\n\n# The DN used for password modifications by root.\n#rootpwmoddn cn=admin,dc=example,dc=com\n\n# SSL options\n#ssl off\n#tls_reqcert never\n# tls_cacertfile /etc/ssl/certs/ca-certificates.crt\n\n# The search scope.\n#scope sub\n</code></pre> <p>The good practice is not write the <code>binddn</code> and <code>bindpw</code> with admin privilege. If you leave it empty, <code>pam-ldapd</code> will use the current user login and pwd to bind to the ldap. So it's safer.</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#622-etcnsswitchconf","title":"6.2.2 /etc/nsswitch.conf","text":"<p>Change the old version to below version</p> <pre><code># /etc/nsswitch.conf\n#\n# Example configuration of GNU Name Service Switch functionality.\n# If you have the `glibc-doc-reference' and `info' packages installed, try:\n# `info libc \"Name Service Switch\"' for information about this file.\n\npasswd:         files ldap\ngroup:          files ldap\nshadow:         files ldap\ngshadow:        files\n\nhosts:          files dns\nnetworks:       files\n\nprotocols:      db files\nservices:       db files\nethers:         db files\nrpc:            db files\n\nnetgroup:       nis\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#623-etcpamdcommon-","title":"6.2.3  /etc/pam.d/common-*","text":"<p>There are a list of config files for pam which are located at  /etc/pam.d/. In our case, we need to modify: - /etc/pam.d/common-auth - /etc/pam.d/common-account - /etc/pam.d/common-session - /etc/pam.d/common-password</p> <pre><code>sudo vim /etc/pam.d/common-auth\n\n# comment the old content, and add below line\nauth      sufficient  pam_unix.so\nauth      sufficient  pam_ldap.so minimum_uid=1000 use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-account\n# comment the old content, and add below line\naccount   required    pam_unix.so\naccount   sufficient  pam_ldap.so minimum_uid=1000\naccount   required    pam_permit.so\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-session\n# comment the old content, and add below line\nsession   required    pam_unix.so\nsession   optional    pam_ldap.so minimum_uid=1000\n# this line will create the user home for first login\nsession    required   pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-password\n# comment the old content, and add below line\npassword  sufficient  pam_unix.so nullok md5 shadow use_authtok\npassword  sufficient  pam_ldap.so minimum_uid=1000 try_first_pass\npassword  required    pam_deny.so\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#624-etcsshsshd_config","title":"6.2.4 /etc/ssh/sshd_config","text":"<p>Normally, you don't need to modify the  /etc/ssh/sshd_config. Because the <code>libpam-ldapd</code> will set UsePAM yes automatically for sshd to use PAM authentication.</p> <p>If you have troubles, don't forget to check </p> <p>The above conf is the minimun for the pam-ldapd works. You need to enrich it if you have special requirements</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#63-restart-the-service","title":"6.3 Restart the service","text":"<p>As we metioned before, the</p> <pre><code># check the status of the daemon\nsudo systemctl status nscd\nsudo systemctl status nslcd\n\n# restart the service\nsudo systemctl restart nscd\nsudo systemctl restart nslcd\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#64-test-and-troubleshoot","title":"6.4 Test and troubleshoot","text":"<p>To ensure that everything is working correctly you can run </p> <pre><code># this command prints all user account of the server which also includes the users from LDAP\ngetent passwd\n\n# below is an example of user passwd from ldap\ntrigaud:x:3000:3000:Titouan:/home/trigaud:/bin/bash\n\n# below can show the user shadow form ldap too\ngetent shadow \n</code></pre> <p>To test authentication log in with an LDAP user, you can run below command</p> <pre><code># general form to local login\nsu - &lt;UID&gt;\n\n# for example, run below command and enter the pwd. if it's correct, \nsu - trigaud\n</code></pre> <p>To troubleshoot problems you can run <code>nslcd in debug mode</code> (remember to stop nscd when debugging). Debug mode should return a lot of information about the LDAP queries that are performed and errors that may arise.</p> <pre><code>/etc/init.d/nscd stop\n/etc/init.d/nslcd stop\nnslcd -d\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#for-ad-compatibility","title":"For AD compatibility","text":"<p>To use AD as authentication server, we can't use <code>nslcd</code> anymore. We need to test the <code>sssd</code> and <code>AD</code> connexion.</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_sssd_ldap/","title":"Configure debian server ssh to use sssd","text":"<p>In </p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/","title":"Configure debian server to use AD/Krb for sshd authentication","text":"<p>In this tutorial, we show how to configure sshd, pam, sssd, to allow a <code>debian 11</code> server to use AD/Krb as  authentication server. We will follow the below steps:</p> <p>We suppose we have : - <code>AD/Krb</code> : The ip address is <code>10.50.5.64</code>, ad domain name <code>casdds.casd</code>, krb realm name <code>CASDDS.CASD</code>, hostname <code>auth</code>, fqdn is <code>auth.casdds.casd</code> - <code>debian 11</code>: ip address is <code>10.50.5.199</code>, hostname is <code>hadoop-client</code>, fqdn is <code>hadoop-client.casdds.casd</code></p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-1-prerequisite","title":"Step 1: Prerequisite","text":""},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#11-reset-hostname-of-hadoop-client","title":"1.1 Reset hostname of hadoop-client","text":"<p>The hostname is essential for the server to have a valid FQDN in the domain, so we need to make sure the hostname is set correctly. Follow the below steps: - set system hostname - update /etc/hosts</p> <pre><code># general form\nsudo hostnamectl set-hostname &lt;custom-hostname&gt;\n\n# for example \nsudo hostnamectl set-hostname hadoop-client\n\n# check the new hostname with below command\nhostname\n\n# expected output\nhadoop-client\n</code></pre> <p>you can also directly edit the hostname config file(not recommended) by using <code>sudo vim /etc/hostname</code></p> <p>Update <code>/etc/hosts</code>:</p> <pre><code>sudo vim /etc/hosts \n\n127.0.1.1 hadoop-client.casdds.casd hadoop-client\n10.50.5.199 hadoop-client.casdds.casd   hadoop-client\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#12-update-system-packages-in-hadoop-client","title":"1.2 Update system packages in hadoop-client","text":"<pre><code>sudo apt update \nsudo apt upgrade\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#13-change-dns-server-settings-in-hadoop-client","title":"1.3 Change dns server settings in hadoop-client","text":"<p>To join the server into an AD domain, you must use the AD as dns server.</p> <p>Edit the <code>/etc/resolv.conf</code> :</p> <pre><code>search casdds.casd\nnameserver 10.50.5.64\nnameserver 8.8.8.8\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#14-install-the-required-packages-in-hadoop-client","title":"1.4 Install the required packages in hadoop-client","text":"<pre><code>sudo apt install realmd sssd sssd-tools libnss-sss libpam-sss adcli samba-common-bin krb5-user oddjob oddjob-mkhomedir packagekit -y\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-2-join-the-debian-serverhadoop-client-to-the-ad-domain","title":"Step 2 : Join the debian server(hadoop-client) to the AD domain","text":""},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#21-check-if-the-domain-can-be-reached-or-not","title":"2.1. Check if the domain can be reached or not","text":"<pre><code>realm discover CASDDS.CASD\n</code></pre> <ul> <li>If the error message is realm command is unknown, open a new shell.</li> <li>If the error message is CASDDS.CASD is unknown, check the dns server ip is reachable, and dns server name setup is correct.</li> </ul>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#22-join-the-serverhadoop-client-to-the-ad-domain","title":"2.2. Join the server(hadoop-client) to the AD domain","text":"<p>To execute the below command, you must have an account with <code>domain administrator</code> privilege :</p> <pre><code>sudo realm join --user=Administrateur CASDDS.CASD\n</code></pre> <p>If there is no error message, it means your server has joined the domain.</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#23-configure-the-linux-serverhadoop-client-account-in-ad","title":"2.3 Configure the linux server(hadoop-client) account in AD","text":"<p>If the <code>hadoop-client</code> has success joined the AD domain, you should see the server appears in the <code>Computer</code> section in the AD manager GUI. Check the below figure</p> <p></p> <p>To check, you need to connect to the <code>Windows Server</code> -&gt; Open <code>AD manager</code> -&gt; In <code>Users and Computers</code> subfolder of  Active Directory. You should find a line of <code>HADOOP-CLIENT</code>. Right Click on it, and select <code>properties</code>, you should see the below pop-up window</p> <p></p> <p>Select the <code>Trust this computer for delegation to any service</code> option in <code>Delegation</code>. </p> <p>Click on the <code>Static IP address</code> option in <code>Dial-in</code>, then put the address ip of the <code>hadoop-client</code>.  </p> <p>You can add a new computer in AD manually, but we don't recommend that. Try to use the <code>realm join</code></p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-3-config-adkrb-dns-server-to-well-integrate-hadoop-client","title":"Step 3: Config AD/Krb, DNS server to well integrate hadoop-client","text":"<p>To make the debian server (hadoop-client) fqdn <code>recognizable</code> and <code>reachable</code> by the other servers in the domain, we need to configure the dns server </p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#31-check-the-dns-entries-in-windows-server","title":"3.1. Check the dns entries in windows server","text":"<p>Open the <code>dns manager</code> in the Windows server (<code>auth.casdds.casd</code>). Check the forward lookup and reverse lookup. You need to make sure the <code>hostname, fqdn and ip address</code> are correct. The two below figures are examples of the <code>hadoop-client</code> config.</p> <p></p> <p></p> <p>Normally, these entries are created automatically by the <code>realm join</code> command. If they are not created correcly, you  need to create them manually.</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#32-check-the-spn-service-principal-name-in-windows-server","title":"3.2. Check the SPN (Service Principal Name) in Windows server","text":"<p>Every registered computer in the domain should have a <code>valid SPN (Service Principal Name)</code>. You can check the name by  using the below command. You can open a <code>powershell prompt</code> in the <code>AD/krb</code> server.</p> <pre><code>setspn -L hadoop-client\n\n# expected output\nRegistered ServicePrincipalNames for CN=HADOOP-CLIENT,CN=Computers,DC=casdds,DC=casd:\n        RestrictedKrbHost/hadoop-client.casdds.casd\n        RestrictedKrbHost/HADOOP-CLIENT\n        host/hadoop-client.casdds.casd\n        host/HADOOP-CLIENT\n</code></pre> <p>If you don't see any outputs, you can create a <code>SPN (Service Principal Name)</code> manually. Below is the command to do so.</p> <pre><code># create a new spn and link it to the hadoop-client(AD account)\n# The -S option adds an SPN only if it does not already exist (avoids duplicates).\nsetspn -S host/hadoop-client.casdds.casd hadoop-client\n\n# generate a keytab for principal  host/hadoop-client.casdds.casd@CASDDS.CASD\nktpass -princ host/hadoop-client.casdds.casd@CASDDS.CASD -mapuser HADOOP-CLIENT$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 -out hadoop-client.keytab\n</code></pre> <p>Below lines are the explanation of the commands: - The <code>ktpass</code> command can generate a keytab file for Kerberos authentication.  - <code>-princ host/hadoop-client.casdds.casd@CASDDS.CASD</code> defines the Kerberos principal name. - <code>-mapuser HADOOP-CLIENT$</code> maps the Kerberos principal to the account (HADOOP-CLIENT) in Active Directory (AD). The <code>$</code> indicates it's a computer account (not a user). - <code>-crypto AES256-SHA1</code> specifies that only the <code>AES256-SHA1</code> crypto algo is supported. You can replace it with <code>ALL</code> to specify all available cryptographic algorithms should be supported for encryption. - <code>-ptype KRB5_NT_PRINCIPAL</code> specifies the principal type as KRB5_NT_PRINCIPAL, which is used for standard Kerberos authentication(for services, use KRB5_NT_SRV_HST). - <code>-pass *</code> prompts the user to enter the password manually. Typically, computer accounts in AD have auto-generated passwords. - <code>-out hadoop-client.keytab</code> saves the keytab file, which will be used by the linux server(hadoop-client) for authentication.</p> <p>The <code>hadoop-client.keytab</code> file is copied to the hadoop-client, so it can use this keytab for Kerberos authentication. <code>hadoop-client</code> can use the kerberos ticket to prove the identity of <code>hadoop-client</code>.</p> <p>After coping the <code>hadoop-client.keytab</code> file to <code>hadoop-client</code>, you can use the below command to check keytab contents:</p> <pre><code>klist -k /tmp/hadoop-client.keytab  \n\n# you should see outputs like\n\"host/hadoop-client.casdds.casd\"\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#33-rename-the-keytab-file-in-linuxhadoop-client","title":"3.3 Rename the keytab file in linux(hadoop-client)","text":"<p>In linux, many Kerberos-aware applications (e.g. kinit, Hadoop, etc.) expect the keytab file to be named <code>krb5.keytab</code>  and located in <code>/etc/</code> by default. We can use the below commands</p> <pre><code>sudo mv hadoop-client.keytab /etc/krb5.keytab\n\n# Ensures only root can read it (for security).\nsudo chmod 600 /etc/krb5.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#34-leave-and-rejoin-the-realm","title":"3.4 Leave and rejoin the realm","text":"<p>If there are errors that you can't resolve, you can always leave the realm and rejoin</p> <pre><code>sudo realm leave CASDDS.CASD\n\nsudo realm join --user=Administrateur CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#35-create-a-service-account-in-ad-for-sssd-daemon","title":"3.5. Create a service account in AD for sssd daemon.","text":"<p>As we explained before, the linux server relies on <code>sssd</code> daemon to get the <code>user id and groups</code> from the AD server. This requires sssd to have an account that allows him to access AD.</p> <p>You need to create a service account <code>sssd</code> in <code>Active Directory manager</code>. Then use the below command to create a <code>principal</code> and the <code>keytab</code> file.</p> <pre><code>ktpass -princ sssd@CASDDS.CASD -mapuser sssd -crypto AES256-SHA1 -ptype KRB5_NT_PRINCIPAL -pass * -out sssd.keytab\n</code></pre> <p>Copie the keytab file to the debian server(hadoop-client):</p> <pre><code>scp sssd.keytab sssd@debian.casdds.casd:/tmp/\n</code></pre> <p>Put the keytab file in /etc</p> <pre><code>sudo cp /tmp/sssd.keytab /etc/\n# need to check the acl of the file, 644 is too open for me\nsudo chmod 644 /etc/sssd.keytab\nsudo chown root:root /etc/sssd.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-4-configuration-of-sssd-pam-and-kerberos","title":"Step 4 : Configuration of SSSD, PAM and Kerberos","text":"<p>We will follow the below order to configure each component: - kerberos client: configure krb client to connect to the target krb Realm - sshd/pam: configure sshd server to use pam as authentication backend - pam/sssd: configure pam to use sssd as backend - sssd/krb: configure sssd to use krb plugin</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#41-configure-kerberos-client-in-debianhadoop-client-server","title":"4.1 Configure kerberos client in debian(hadoop-client) server","text":"<pre><code># install the required package\nsudo apt install krb5-user\n\n# edit the config file `/etc/krb5.conf`  \nsudo vim /etc/krb5.conf\n</code></pre> <p>Put the below content in the file <code>/etc/krb5.conf</code> </p> <pre><code> [libdefaults]\n        default_realm = CASDDS.CASD\n\n        default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        permitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        # Fadoua said this must be removed from, otherwise the ticket will not be forwad to the target host \n        # proxiable = true\n        ticket_lifetime = 24h\n        dns_lookup_realm = true\n        dns_lookup_kdc = true\n        dns_canonicalize_hostname = false\n        rdns = false\n         allow_weak_crypto = true\n\n\n[realms]\n        CASDDS.CASD = {\n                kdc = 10.50.5.64\n                admin_server = 10.50.5.64\n        }\n\u2026..\n[domain_realm]\n\u2026.\n        .casdds.casd = CASDDS.CASD\n        casdds.casd = CASDDS.CASD\n</code></pre> <p>To check the krb client, use the below command</p> <pre><code># ask a ticket kerberos\nkinit host/hadoop-client.casdds.casd\n\n# the short version should work if the keytab is in place, if not you can specify the path of keytab\nkinit -kt /etc/krb5.keytab host/hadoop-client.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#42-configure-sshd-to-use-pam","title":"4.2. Configure sshd to use pam","text":"<p>We need to edit two files: - <code>/etc/ssh/sshd_config</code> (configuration for the ssh server) - <code>/etc/ssh/ssh_config</code> (configuration for the ssh client)</p> <p>In <code>/etc/ssh/sshd_config</code>, enable the below lines</p> <pre><code># disable other authentication methods\nChallengeResponseAuthentication no\nPasswordAuthentication no\n\n# use pam as authentication backend\nUsePAM yes\n\n# GSSAPI options for sshd server to accept GSSAPI, it's required for the server to accept krb ticket as \n# credentials\n# \nGSSAPIAuthentication yes\n# Cleans up the Kerberos credentials after the session.\nGSSAPICleanupCredentials yes\n# Ensures that the SSH client does not strictly check for a valid acceptor name in the Kerberos tickets.\nGSSAPIStrictAcceptorCheck no\n# Allows the exchange of Kerberos keys for stronger encryption.\nGSSAPIKeyExchange yes\n\n\nX11Forwarding yes\n\nPrintMotd no\n\n\n# Allow client to pass locale environment variables\nAcceptEnv LANG LC_*\n\n# override default of no subsystems\nSubsystem       sftp    /usr/lib/openssh/sftp-server\n</code></pre> <p>You need to restart the sshd service to enable the new config </p> <pre><code>sudo systemctl restart sshd\n</code></pre> <p>In the <code>/etc/ssh/ssh_config</code>, you need to add the below line </p> <pre><code>   Host *\n       GSSAPIAuthentication yes\n       GSSAPIDelegateCredentials yes\n       PasswordAuthentication no\n</code></pre> <p>For hadoop-client, the <code>ssh_config</code> is not required, because it defines the behaviour of the ssh client. It needs to be configured in the ssh client which wants to connect to the hadoop-client ssh server. </p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#43-configure-pam","title":"4.3 Configure pam","text":"<p>All the configuration files for pam are located in <code>/etc/pam.d/</code>. The below is the minimum config for the pam to use sssd daemon as authentication backend.</p> <pre><code>### /etc/pam.d/common-auth\nsudo: unable to resolve host debian118: Name or service not known\nauth      sufficient  pam_unix.so try_first_pass\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-account\nsudo: unable to resolve host debian118: Name or service not known\naccount   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <pre><code>### /etc/pam.d/common-password\nsudo: unable to resolve host debian118: Name or service not known\npassword  sufficient  pam_unix.so\npassword  sufficient  pam_sss.so\npassword  required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-session\nsudo: unable to resolve host debian118: Name or service not known\nsession   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#44-configure-sssd","title":"4.4 Configure sssd","text":"<p>Now we need to configure the sssd daemon. The main config file is in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[sssd]\nservices = nss, pam\ndomains = casdds.casd\nconfig_file_version = 2\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n\n[domain/casdds.casd]\nldap_sasl_authid = sssd@CASDDS.CASD\nkrb5_keytab = /etc/sssd.keytab\ndefault_shell = /bin/bash\nkrb5_store_password_if_offline = True\ncache_credentials = True\nkrb5_realm = CASDDS.CASD\nrealmd_tags = manages-system joined-with-adcli\nid_provider = ad\nfallback_homedir = /home/%u@%d\nad_domain = casdds.casd\nuse_fully_qualified_names = False\nldap_id_mapping = True\naccess_provider = ad\nldap_group_nesting_level = 2\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#5configure-ssh-client-on-windows","title":"5.configure ssh client on Windows","text":"<p>In windows, there are many ssh clients: - MobaXterm: - tabby: https://tabby.sh/ - powershell+openssh - PuTTY</p> <p>Below is the instruction on how to install and configure openssh via PowerShell</p> <pre><code>Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\nStart-Service sshd\nSet-Service -Name sshd -StartupType 'Automatic'\n</code></pre> <p>In windows, all the configuration file for openssh is located in <code>C:\\ProgramData\\ssh</code> If you want to setup the config for ssh server, you can edit the file in <code>C:\\ProgramData\\ssh\\sshd_config</code>.</p> <p>To restart ssh service in windows</p> <pre><code># start sshd service\nStart-Service sshd\n\n# restart sshd service \nRestart-Service sshd\n</code></pre> <p>Configure ssh client </p> <pre><code># open a notepad\nnotepad $env:USERPROFILE\\.ssh\\config\n\n# add the below lines\n# * means for all hosts\nHost *\n    GSSAPIAuthentication yes\n    GSSAPIDelegateCredentials yes \n</code></pre> <p>You can also define the behaviors host by host, below is an example</p> <pre><code>Host hadoop-client\n    HostName hadoop-client.casdds.casd\n    User pengfei@casdds.casd\n    Port 22\n    GSSAPIAuthentication yes\n    GSSAPIDelegateCredentials yes \n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-6-test-the-solution","title":"Step 6 : Test the solution","text":"<p>In our scenario, the user follow the below steps: 1. first login to a Windows server, the first ticket kerberos is generated in the Windows server. 2. user ssh to hadoop-client with the ticket kerberos with option forward ticket 3. user try to access hdfs cluster with the forward kerberos ticket</p> <p>Suppose you have an account <code>user</code> in AD with the privilege to connect to <code>hadoop client</code>  \\</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#61-understand-the-ticket","title":"6.1. Understand the ticket","text":"<p>In linux, you can ask a ticket and check the ticket with the below command</p> <pre><code># ask a new ticket, you need to provide a password associated with the provided principal\nkinit user@CASDDS.CASD  \n\n# check the ticket contents\nklist -5fea   \n</code></pre> <p>The option: - 5: Show only Kerberos 5 tickets (modern Kerberos version). - f: Show ticket flags (like FORWARDABLE, RENEWABLE, etc.). - e: Display encryption type used for the ticket. - a Show addresses associated with the ticket (if address-restriction of the ticket is activated).</p> <p>You should see the below output as the ticket content</p> <pre><code>Ticket cache: FILE:/tmp/krb5cc_1000\nDefault principal: user@CASDDS.CASD\n\nValid starting       Expires              Service principal\n03/31/25 10:00:00  03/31/25 20:00:00  krbtgt/CASDDS.CASD@CASDDS.CASD\n        Flags: FRI\n        Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96\n        Addresses: 192.168.1.100\n</code></pre> <p>A kerberos ticket has the below properites:</p> <ul> <li>Ticket cache: Location of the ticket. </li> <li>Default principal: Your Kerberos identity (user@EXAMPLE.COM).</li> <li>Valid starting / Expires: Time range for which the ticket is valid.</li> <li>Service principal: The Kerberos service this ticket is for. (krbtgt/CASDDS.CASD@CASDDS.CASD is a tgt issued by CASDDS.CASD the kdc server)</li> <li>Flags (-f option): F = Forwardable (Can be forwarded to another machine). R = Renewable (Can be extended before expiration). I = Initial (Freshly obtained).</li> <li>Encryption type (-e option): aes256-cts-hmac-sha1-96, means AES-256 encryption with SHA-1 HMAC.</li> <li>Addresses (-a option): Shows the IP addresses associated with the ticket (if address-restricted).</li> </ul> <p>You can ask ticket with special options:</p> <pre><code># below command ask a Forwardable, Renewable for a 7 day validity\nkinit -f -r 7d\n</code></pre> <p>Based on the kdc configuration, it may or may not generate the ticket.</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#62-connexion-ssh","title":"6.2. Connexion SSH","text":"<p>From windows, if the server has joined the domain, windows will generate a kerberos ticket after user logon:</p> <pre><code># check the user ticket\nklist -5fea\n\n# for windows ssh client\n# -K active la d\u00e9l\u00e9gation Kerberos\nssh -K user@debian.casdds.casd  \n\n# for linux ssh client\nssh -o GSSAPIDelegateCredentials=yes user@debian.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#appendix","title":"Appendix :","text":""},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#acl-for-etcsssdsssdconf","title":"ACL for /etc/sssd/sssd.conf","text":"<p>The Permissions for <code>/etc/sssd/sssd.conf</code> must be <code>600</code> :</p> <pre><code>sudo chmod 600 /etc/sssd/sssd.conf\n</code></pre>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/","title":"Configure sssd to use static uid, gid","text":"<p>POSIX (Portable Operating System Interface) is <code>UNIX/Linux standards for identity and access control</code>. A <code>POSIX</code>  account use specific attributes such as - uidNumber \u2013 Unique User ID (UID) - gidNumber \u2013 Primary Group ID (GID) - homeDirectory \u2013 User's home directory path  - loginShell \u2013 The default shell (e.g., /bin/bash)</p> <p>These attributes allow UNIX/Linux systems to recognize, authenticate users, and create user workspace.</p> <p>By default, AD does not use POSIX attributes for user and group.  Instead, AD relies on:</p> <ul> <li>Security Identifiers (SIDs): Every user and group has a <code>SID</code>, which is a unique identifier in Windows.</li> <li><code>sAMAccountName</code>: pliu (This is legacy login, )</li> <li>UserPrincipalName (UPN): pliu@casd.eu (authentication in modern windows server)</li> </ul>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#1-default-behavior-when-sssd-uses-ad-as-authentication-backend","title":"1. Default behavior when sssd uses AD as authentication backend","text":"<p>The default behavior in <code>SSSD</code> and <code>Winbind</code> is to use <code>auto id mapping</code>. SSSD will dynamically generate UIDs and GIDs from the <code>AD objects's ObjectSID</code>. This will lead to inconsistent UIDs and GIDs across machines.</p> <p>In certain scenarios, it will create conflicting ACL in user home. For example, if a user with AD account with name <code>test</code> login to a linux server, a user home will be created <code>/home/test</code>. If the user account is deleted, and a new account <code>test</code> is created, when the new <code>test</code> user login to the linux server, it will user /home/test as home dir too. But the new and old <code>test</code> will have different uid. So the old files in /home/test will have old uid as owner, the new  <code>test</code> user can't access it. </p> <p>To avoid inconsistent UIDs and GIDs, we recommend you to use static uidNumber and gidNumber.</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#2-configure-sssd-to-user-static-uidnumber-and-gidnumber","title":"2. Configure sssd to user static uidNumber and gidNumber.","text":"<p>To configure sssd to user static uidNumber and gidNumber, follow the below steps 1. Add posix attributes in AD 2. Configure sssd to read posix attributes</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#21-adds-posix-attributes-in-ad","title":"2.1 Adds posix attributes in AD","text":"<p>If SSSD wants to use static uidNumber and gidNumber, the AD server must have those attributes. Before Windows server 2016. The AD server can use <code>rfc2307</code> schema, which allows us to create posix compatible user  accounts and groups. This feature has been removed since <code>Windows server 2016</code>. But you can still add attributes such as <code>uidNumber</code>, <code>gidNumber</code> to a user account or group. </p> <p>Open <code>AD users and groups gui</code>-&gt; on the toolbar, click on <code>view</code>-&gt; Select <code>advance features</code> -&gt; now when you double-click on  a user account, you will see a tab called </p> <p></p> <p></p> <p>The official doc can be found here.</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#22-configure-sssd-to-read-posix-attributes","title":"2.2 Configure sssd to read posix attributes","text":"<p>Before changing your sssd configuration, make sure <code>AD Objects have gidNumber and uidNumber Attributes</code>.</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#221-for-ad-windows-server-2016","title":"2.2.1 For AD &lt; Windows server 2016.","text":"<p>Configure the AD server to use <code>rfc2307</code> schema, and create posix compatible accounts and groups. Then add the below  conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False\nldap_schema = rfc2307\n</code></pre>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#221-for-ad-windows-server-2016_1","title":"2.2.1 For AD &gt;= Windows server 2016.","text":"<p>Add the below conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False  # Important: Forces usage of uidNumber/gidNumber\nldap_user_uid_number = uidNumber\nldap_user_gid_number = gidNumber\nldap_group_gid_number = gidNumber\nenumerate = True  # Optional: Lists all users and groups\n</code></pre>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#23-restart-sssd-and-check-uid-gid","title":"2.3 Restart sssd and check uid, gid","text":"<pre><code># restart sssd\nsystemctl restart sssd\n# clear sssd cache\nsss_cache -E\n\n# check user id and groups\nid &lt;uid&gt;\n\n# you should see the output id value matches the value which you deined in AD\n\n# check group id\ngetent group &lt;groupname&gt;\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/","title":"Configure debian server sshd auth with pam, sssd, kerberos","text":"<p>There is a complete tutorial on how to setup openldap, kerberos for unified authentication. You can visit this website.</p> <p>In this tutorial, we show a scenario light compares to the architecture which shows in the above tutorial.  We don't use the <code>SASL/GSSAPI</code> to delegate user password check to kerberos.</p> <p>It means, in the below tutorial, user has a password in openldap, and a password in kerberos, which are not synchronized automatically. The user id does the link of user between openldap and kerberos.</p> <p>Suppose we have a ldap and kerberos server running on <code>10.50.5.200</code> with url as <code>krb.casd.local</code>.</p> <p>suppose we have three servers: - krb.casd.local: server hosts openldap and kerberos (can be replaced by AD/kerberos) - ssh-server.casd.local: server runs sshd server which uses pam, sssd, sssd-krb5 to check user authentication - ssh-client.casd.local: a vm runs an ssh client and krb5-client, user can get a kerberos ticket, and use this ticket to ssh                            into the ssh-server.casd.local</p> <p>The full authentication process: 1. user get a kerberos ticket (kinit ) (in ssh-client.casd.local) 2. user init an ssh connection with the cached kerberos ticket(ssh uid@ssh-server.casd.local) (in ssh-client.casd.local) 3. ssh-server receives the ssh connection requests (sshd config checks all the possible authentication methods) (in ssh-server.casd.local) 4. <code>sshd</code> delegate the authentication to <code>pam</code>(<code>UsePAM yes in sshd_config</code>), <code>pam</code> delegate to <code>sssd</code>, <code>sssd</code> delegate to <code>sssd-krb5</code>. Because we set <code>auth_provider</code>       as <code>krb5</code> in <code>sssd</code>. (in ssh-server.casd.local) 5. <code>sssd-krb5</code> sends a request to <code>krb.casd.local</code> to verify the authenticity of the kerberos ticket. This steps requires <code>ssh-server.casd.local</code>     has a valid principal in <code>krb.casd.local</code> (in ssh-server.casd.local) 6. <code>krb.casd.local</code> verify the ticket and send the result back to <code>ssh-server.casd.local</code>. (in krb.casd.local) 7. <code>sssd-krb5</code> in <code>ssh-server.casd.local</code> receives the result, if ok, it will ask the <code>id_provider</code> of the <code>sssd</code>,        in our case it's <code>openldap</code> to get <code>uid</code> and <code>gid</code> of the user with the uid of the kerberos ticket. The user       <code>uid</code> and <code>gid</code> information will be transfer to <code>nss</code>. <code>sssd</code> tells pam it's ok, <code>pam</code> tells sshd it's ok.      Then pam will create a user session in the server after user login. (in ssh-server.casd.local) 8. user will get a terminal on <code>ssh-server.casd.local</code> with uid and gids from the <code>openldap</code> account."},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#1-configure-and-test-krb-client-on-ssh-clientcasdlocal-and-ssh-servercasdlocal","title":"1. Configure and test krb client on ssh-client.casd.local and ssh-server.casd.local","text":"<p>We need to install the kerberos client on both servers: - ssh-client.casd.local - ssh-server.casd.local</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#11-install-the-required-packages","title":"1.1 Install the required packages","text":"<pre><code># krb client package\nsudo apt install krb5-user\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#12-configure-the-krb-client","title":"1.2 Configure the krb client","text":"<pre><code>sudo vim /etc/krb5.conf\n\n# add the below content\n[libdefaults]\n    default_realm = CASD.LOCAL\n\n# The following krb5.conf variables are only for MIT Kerberos.\n    kdc_timesync = 1\n    ccache_type = 4\n    forwardable = true\n    proxiable = true\n        rdns = false\n\n\n# The following libdefaults parameters are only for Heimdal Kerberos.\n    fcc-mit-ticketflags = true\n\n[realms]\n    CASD.LOCAL = {\n        kdc = krb.casd.local\n        admin_server = krb.casd.local\n    }\n\n\n[domain_realm]\n        casd.local = CASD.LOCAL\n        .casd.local = CASD.LOCAL\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#13-test-the-client","title":"1.3 Test the client","text":"<pre><code># generate a ticket, the principal must exist in the krb server\nkinit pliu@CASD.LOCAL\n\n# normally, you can view the ticket\nklist\n\n# clean the ticket\nkdestory\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#2-config-sshd-pam-sssd-on-ssh-servercasdlocal","title":"2. Config sshd, pam, sssd on ssh-server.casd.local","text":""},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#21-install-required-packages","title":"2.1 Install required packages","text":"<pre><code>sudo apt install sssd sssd-tools libnss-sss libpam-sss libpam-mkhomedir\n</code></pre> <ul> <li>sssd: package for daemon sssd((System Security Services Daemon))</li> <li>sssd-tools: provides command-line utilities for managing and troubleshooting SSSD</li> <li>nss: NSS (Name Service Switch) is a subsystem in Linux and Unix-like systems that allows applications           to retrieve information about users, groups, hosts, networks, services, and more from various           sources (like /etc/passwd, LDAP, NIS, or SSSD). By default, this daemon is running on a debian server,           no need to install the package. The main config is in <code>/etc/nsswitch.conf</code></li> <li>libnss-sss: This daemon allows <code>nss</code> to retrieve user information from <code>sssd</code></li> <li>libpam-sss: This daemon allows <code>pam</code> to use <code>sssd</code> as an authentication mechanism. </li> <li>libpam-mkhomedir: This daemon allows <code>pam</code> to create home dir for newly connected users.</li> </ul>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#22-configure-sshd-to-use-pam-as-an-authentication-mechanism","title":"2.2 Configure sshd to use pam as an authentication mechanism","text":"<pre><code>Include /etc/ssh/sshd_config.d/*.conf\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n#HostKey /etc/ssh/ssh_host_rsa_key\n#HostKey /etc/ssh/ssh_host_ecdsa_key\n#HostKey /etc/ssh/ssh_host_ed25519_key\n\n# Ciphers and keying\n#RekeyLimit default none\n\n# Logging\n#SyslogFacility AUTH\n#LogLevel INFO\n\n# Authentication:\n\n#LoginGraceTime 2m\n#PermitRootLogin prohibit-password\n#StrictModes yes\n#MaxAuthTries 6\n#MaxSessions 10\n\n#PubkeyAuthentication yes\n\n# Expect .ssh/authorized_keys2 to be disregarded by default in future.\n#AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2\n\n#AuthorizedPrincipalsFile none\n\n#AuthorizedKeysCommand none\n#AuthorizedKeysCommandUser nobody\n\n# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts\n#HostbasedAuthentication no\n# Change to yes if you don't trust ~/.ssh/known_hosts for\n# HostbasedAuthentication\n#IgnoreUserKnownHosts no\n# Don't read the user's ~/.rhosts and ~/.shosts files\n#IgnoreRhosts yes\n\n# To disable tunneled clear text passwords, change to no here!\n#PasswordAuthentication yes\n#PermitEmptyPasswords no\n\n# Change to yes to enable challenge-response passwords (beware issues with\n# some PAM modules and threads)\nChallengeResponseAuthentication no\n#AuthenticationMethods gssapi-with-mic,password\n\n# Kerberos options\n# KerberosAuthentication yes\n# KerberosOrLocalPasswd yes\n# KerberosTicketCleanup yes\n# KerberosGetAFSToken yes\n#UseDNS yes\n# GSSAPI options\nGSSAPIAuthentication yes\nGSSAPICleanupCredentials yes\nGSSAPIStrictAcceptorCheck no\n#GSSAPIKeyExchange no\nAllowTcpForwarding yes\nAllowAgentForwarding yes\nGssapiKeyExchange yes\n# Set this to 'yes' to enable PAM authentication, account processing,\n# and session processing. If this is enabled, PAM authentication will\n# be allowed through the ChallengeResponseAuthentication and\n# PasswordAuthentication.  Depending on your PAM configuration,\n# PAM authentication via ChallengeResponseAuthentication may bypass\n# the setting of \"PermitRootLogin without-password\".\n# If you just want the PAM account and session checks to run without\n# PAM authentication, then enable this but set PasswordAuthentication\n# and ChallengeResponseAuthentication to 'no'.\nUsePAM yes\nUseDNS yes\n#AllowAgentForwarding yes\n#AllowTcpForwarding yes\n#GatewayPorts no\nX11Forwarding yes\n#X11DisplayOffset 10\n#X11UseLocalhost yes\n#PermitTTY yes\nPrintMotd no\n#PrintLastLog yes\n#TCPKeepAlive yes\n#PermitUserEnvironment no\n#Compression delayed\n#ClientAliveInterval 0\n#ClientAliveCountMax 3\n#UseDNS no\n#PidFile /var/run/sshd.pid\n#MaxStartups 10:30:100\n#PermitTunnel no\n#ChrootDirectory none\n#VersionAddendum none\nPermitRootLogin yes\n#PasswordAuthentication yes\n\n# no default banner path\n#Banner none\n\n# Allow client to pass locale environment variables\nAcceptEnv LANG LC_*\n\n# override default of no subsystems\nSubsystem   sftp    /usr/lib/openssh/sftp-server\n\n# Example of overriding settings on a per-user basis\n#Match User anoncvs\n#   X11Forwarding no\n#   AllowTcpForwarding no\n#   PermitTTY no\n#   ForceCommand cvs server\n</code></pre> <p>In the above conf, you can notice that I have two authentication methods: <code>GSSAPI</code> and <code>PAM</code>. Here <code>GSSAPI</code> is used to allow user to submit his ticket kerberos to sshd server. The sssd-krb5 can only support user login and password auth via kerberos. </p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#23-configure-pam-to-use-sssd","title":"2.3 configure pam to use sssd","text":"<p><code>pam</code> has a list of configuration files(located in <code>/etc/pam.d/</code>): - common-auth: user authentication  - common-account: User account management - common-password: Allow user to modify password. - common-session: user session settings</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#231-common-auth","title":"2.3.1 common-auth","text":"<p>The simplest config example :</p> <pre><code>auth      sufficient  pam_unix.so\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <p>pam_unix.so: Uses local account to authenticate users pam_sss.so use_first_pass: Uses SSSD as first method to authenticate users. pam_deny.so: Denies access if all the above authentication method fails. pam_permit.so: Allows authentication if all previous steps succeed.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#232-common-account","title":"2.3.2 common-account","text":"<p>This controls how the user account can interact with the system.  Below is a simple config example. </p> <pre><code>account   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <p>don't add <code>account requisite  pam_deny.so</code> in the config, otherwise you can no longer become root with sudoers right.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#233-common-password","title":"2.3.3 common-password:","text":"<p>Allow user to modify password.</p> <pre><code>password  sufficient  pam_unix.so nullok md5 shadow use_authtok\npassword  sufficient  pam_sss.so try_first_pass\npassword  required    pam_deny.so\n</code></pre> <p>This configuration is not enough for user to change password. You need to change sssd, ldap/kerberos config to  allow users to change their passwords through sssd, Kerberos/LDAP.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#234-common-session","title":"2.3.4 common-session","text":"<pre><code>session   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre> <p>pam_mkhomedir.so: Create a home directory on first login if it doesn\u2019t exist with umask=0022. pam_sss.so: Ensures SSSD session modules are applied.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#24-configure-nss","title":"2.4 Configure NSS","text":"<p>Ensure SSSD is used for user and group lookup.</p> <p>The NSS (Name Service Switch) main config is located at <code>/etc/nsswitch.conf</code>:</p> <p>The following config is a simple example tells Linux to check both local files (/etc/passwd) and SSSD for user information.</p> <pre><code>sudo vim /etc/nsswitch.conf\n\npasswd:         files sss\ngroup:          files sss\nshadow:         files sss\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#25-configure-sssd","title":"2.5 Configure sssd","text":"<p>The <code>sssd</code> can query ldap/kerberos, AD/kerberos to check user authenticity(auth_provider), query ldap, or AD to get user id, groups, etc(id_provider).</p> <p><code>sssd</code> also allows user to change password of the backend(e.g. ldap, krb)</p> <pre><code>[sssd]\nservices = nss, pam, ssh\ndomains = casd.local\nconfig_file_version = 2\n\n[domain/casd.local]\nid_provider = ldap\nldap_uri = ldap://krb.casd.local\nldap_search_base = dc=casd,dc=local\n\nauth_provider = krb5\nchpass_provider = krb5\nkrb5_realm = CASD.LOCAL\nkrb5_server = krb.casd.local\nkrb5_kpasswd = krb.casd.local\ndebug_level = 5\nkrb5_validate = true\nkrb5_ccachedir = /var/tmp # note that RHEL-7 default to KERNEL ccaches, which are preferred in most cases to FILE\nkrb5_keytab = /etc/krb5.keytab\ncache_credentials = true\n\noverride_homedir = /home/%u\ndefault_shell = /bin/bash\n\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n</code></pre> <p>You need to restart the daemon <code>sssd</code>, after modifying the <code>sssd.conf</code></p> <pre><code>sudo systemctl restart sssd\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#251-debug-sssd-by-using-sssd-tools","title":"2.5.1 debug sssd by using sssd-tools","text":"<p>You need the admin right to run this command, otherwise you will get <code>command not found</code> error message. You can view the documentation of the tool with <code>sudo sssctl</code>.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#check-the-validity-of-the-sssd-config","title":"Check the validity of the sssd config","text":"<pre><code>sudo sssctl config-check\n\n# output\nIssues identified by validators: 0\n\nMessages generated during configuration merging: 0\n\nUsed configuration snippet files: 0\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#list-all-available-domain-configured-in-sssd","title":"list all available domain configured in sssd","text":"<pre><code>sudo sssctl domain-list\n\n# output example\ncasd.local\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#check-the-status-of-a-domain","title":"check the status of a domain","text":"<pre><code>sudo sssctl domain-status casd.local\n\n# output example\nOnline status: Online\nActive servers:\nKPASSWD: krb.casd.local\nKERBEROS: krb.casd.local\nLDAP: krb.casd.local\nDiscovered KPASSWD servers:\n- krb.casd.local\nDiscovered KERBEROS servers:\n- krb.casd.local\nDiscovered LDAP servers:\n- krb.casd.local\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#check-the-status-of-a-user","title":"check the status of a user","text":"<pre><code>sudo sssctl user-checks pengfei\n\n# output example\nuser: pengfei\naction: acct\nservice: system-auth\n\nSSSD nss user lookup result:\n - user name: pengfei\n - user id: 3002\n - group id: 4000\n - gecos: pengfei\n - home directory: /home/pengfei\n - shell: /bin/bash\n\nSSSD InfoPipe user lookup result:\n - name: pengfei\n - uidNumber: 3002\n - gidNumber: 4000\n - gecos: pengfei\n - homeDirectory: /home/pengfei\n - loginShell: /bin/bash\n\ntesting pam_acct_mgmt\n\npam_acct_mgmt: Success\n\nPAM Environment:\n - no env -\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#252-clear-sssd-cache","title":"2.5.2 Clear sssd cache","text":"<p>Use the below command to clear SSSD cache, if SSSD is using outdated credentials.</p> <pre><code>sss_cache -E   # Clear all cached entries\nsss_cache -u username  # Clear cache for a specific user\nsss_cache -g groupname  # Clear cache for a specific group\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#26-create-service-principals-for-kerberos-authentication","title":"2.6 Create service principals for kerberos authentication","text":"<p>sssd-krb5 requires a service principal to be able to talk with the kerberos server. So we need to create a service account(principal) for <code>ssh-server.casd.local</code> to be able to access <code>krb.casd.local</code></p> <p>In <code>ssh-server.casd.local</code>, run the below command</p> <pre><code># connect to krb server via kadmin. The principal which you use to connect to the admin console \n# must has the admin rights in krb server\nsudo kadmin -p admin/admin@CASD.LOCAL\n\n# you should see the below terminal\nkadmin:\n\n# create a principal with a generated password for the ssh-server\nkadmin:  addprinc -randkey auth-agent/sssd-test.casd.local@CASD.LOCAL\n\n# export the principal with encrypted password to the default keytab\nkadmin:  ktadd auth-agent/sssd-test.casd.local@CASD.LOCAL\n\n# exit the kadmin shell\nquit\n\n# check the principal in the keytab\nsudo klist -k /etc/krb5.keytab\n\n# short version\nsudo klist -ke\n</code></pre> <p>if your principal does not have admin rights, you can edit the <code>/etc/krb5kdc/kadm5.acl</code> file to grant admin rights to certain principals</p> <p>The below file is an example. One common way to set up Kerberos administration is to <code>allow any principal   ending in /admin is given full administrative rights.</code></p> <pre><code># To enable this, uncomment the following line:\n*/admin *\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#27-test-ssh-connexions","title":"2.7 Test ssh connexions","text":"<pre><code># on the ssh-server, you can already check if the pam, sssd, openldap config\ngetent passwd username\n\n# on the ssh-client, \nssh uid@ssh-server.casd.local\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/","title":"Guide pour int\u00e9grer une machine Debian 11 \u00e0 un domaine Active Directory et configurer SSH avec GSSAPI/Kerberos","text":"<p>In this tutorial, we show how to join a <code>debian 11</code> server </p>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-1-preparation-de-la-machine-debian","title":"\u00c9tape 1 : Pr\u00e9paration de la machine Debian","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#11-renommer-la-machine","title":"1.1 Renommer la machine","text":"<p>Modifier le nom d'h\u00f4te selon la politique de l'entreprise :</p> <pre><code>sudo nano /etc/hostname  # Exemple : debian.casdds.casd\nsudo hostnamectl set-hostname \"nouveau_nom\"\n</code></pre> <p>Metter \u00e0 jour <code>/etc/hosts</code> pour inclure le nom et l'IP statique :</p> <pre><code>sudo nano /etc/hosts  # Ajouter : 10.50.5.X   debian.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#12-mise-a-jour-du-systeme","title":"1.2 Mise \u00e0 jour du syst\u00e8me","text":"<pre><code>sudo apt update \n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#13-configurer-le-dns-sur-debian","title":"1.3 Configurer le DNS sur Debian","text":"<p>D\u00e9finir le serveur DNS AD dans <code>/etc/resolv.conf</code> :</p> <pre><code>search casdds.casd\nnameserver 10.50.5.64\nnameserver 8.8.8.8\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#14-installer-les-paquets-necessaires","title":"1.4 Installer les paquets n\u00e9cessaires","text":"<pre><code>sudo apt install realmd sssd sssd-tools libnss-sss libpam-sss adcli samba-common-bin krb5-user oddjob oddjob-mkhomedir packagekit -y\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-2-joindre-le-domaine-active-directory","title":"\u00c9tape 2 : Joindre le domaine Active Directory","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#21-decouvrir-le-domaine","title":"2.1. D\u00e9couvrir le domaine","text":"<p>V\u00e9rifier la connectivit\u00e9 avec le contr\u00f4leur de domaine :</p> <pre><code>realm discover CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#22-rejoindre-le-domaine","title":"2.2. Rejoindre le domaine","text":"<p>Utiliser un compte administrateur AD :</p> <pre><code>sudo realm join --user=Administrateur CASDDS.CASD\n</code></pre> <p>A ce stade, mon client Debian a bien rejoint mon domaine et appara\u00eet dans la console Utilisateurs et Ordinateurs Active Directory de mon serveur Windows. S\u2019il n\u2019apparait pas, on peut l\u2019ajouter manuellement dans Ordinateurs en s\u00e9lectionne l\u2019@ip static et d\u00e9l\u00e9gation kerberos </p> <p>{{:datascience:admin_system:linux:capture1.png?400|}} {{:datascience:admin_system:linux:capture2.png?400|}}</p>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-3-configuration-dns-et-kerberos","title":"\u00c9tape 3 : Configuration DNS et Kerberos","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#31-ajouter-lenregistrement-dns-sur-windows","title":"3.1. Ajouter l'enregistrement DNS sur Windows","text":"<p>Dans le serveur DNS Windows :</p> <p>Ajouter un enregistrement A pour la machine Debian dans la zone Forward Lookup*. (S'il n'est pas pr\u00e9sent) </p> <ul> <li>Cr\u00e9er un enregistrement PTR dans la zone Reverse Lookup.(S'il n'est pas pr\u00e9sent)  {{:datascience:admin_system:linux:capture1.png?400|}} {{:datascience:admin_system:linux:capture4.png?400|}}</li> </ul>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#32-enregistrer-le-spn-service-principal-name","title":"3.2. Enregistrer le SPN (Service Principal Name)","text":"<p>Pour assurer que l'enregistrement existant, on tape: </p> <p>Sur le contr\u00f4leur de domaine (PowerShell administrateur) :   </p> <pre><code>setspn -L debian\n</code></pre> <p>Si host/debian.casdds.casd n'est pas pr\u00e9sent, on l'ajouter avec Powershell admin :</p> <pre><code>setspn -S host/debian.casdds.casd debian\nktpass -princ host/debian.casdds.casd@CASDDS.CASD -mapuser DEBIAN$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#33-verifier-le-keytab-kerberos","title":"3.3. V\u00e9rifier le keytab Kerberos","text":"<p>Sur Debian :</p> <pre><code>klist -k /etc/krb5.keytab  # V\u00e9rifier la pr\u00e9sence de \"host/debian.casdds.casd\"\n</code></pre> <p>Si absent, quitter et rejoigner le domaine :</p> <pre><code>sudo realm leave CASDDS.CASD\nsudo realm join --user=Administrateur CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#34-generation-et-deploiement-dun-fichier-keytab","title":"3.4. G\u00e9n\u00e9ration et d\u00e9ploiement d\u2019un fichier keytab","text":"<p>G\u00e9n\u00e9ration du keytab sur Windows :</p> <pre><code>ktpass -princ user@CASDDS.CASD -mapuser user -crypto AES256-SHA1 -ptype KRB5_NT_PRINCIPAL -pass * -out user.keytab\n</code></pre> <p>Transfert vers Debian :</p> <pre><code>scp user.keytab user@debian.casdds.casd:/tmp/\n</code></pre> <p>Installation et s\u00e9curisation du keytab :</p> <pre><code>sudo cp /tmp/user.keytab /etc/\nsudo chmod 644 /etc/user.keytab\nsudo chown root:root /etc/user.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-4-configuration-de-sssd-pam-et-kerberos","title":"\u00c9tape 4 : Configuration de SSSD, PAM et Kerberos","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#41-fichier-etcsssdsssdconf","title":"4.1. Fichier <code>/etc/sssd/sssd.conf</code>","text":"<pre><code>[sssd]\nservices = nss, pam\ndomains = casdds.casd\nconfig_file_version = 2\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n\n[domain/casdds.casd]\nldap_sasl_authid = user@CASDDS.CASD\nkrb5_keytab = /etc/user.keytab\ndefault_shell = /bin/bash\nkrb5_store_password_if_offline = True\ncache_credentials = True\nkrb5_realm = CASDDS.CASD\nrealmd_tags = manages-system joined-with-adcli\nid_provider = ad\nfallback_homedir = /home/%u@%d\nad_domain = casdds.casd\nuse_fully_qualified_names = False\nldap_id_mapping = True\naccess_provider = ad\nldap_group_nesting_level = 2\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#42-configurer-pam","title":"4.2. Configurer PAM","text":"<p>Modifier les fichiers dans <code>/etc/pam.d/</code> pour inclure <code>pam_sss.so</code> </p> <pre><code>### /etc/pam.d/common-auth\nsudo: unable to resolve host debian118: Name or service not known\nauth      sufficient  pam_unix.so try_first_pass\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-account\nsudo: unable to resolve host debian118: Name or service not known\naccount   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <pre><code>### /etc/pam.d/common-password\nsudo: unable to resolve host debian118: Name or service not known\npassword  sufficient  pam_unix.so\npassword  sufficient  pam_sss.so\npassword  required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-session\nsudo: unable to resolve host debian118: Name or service not known\nsession   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#43-fichier-etckrb5conf","title":"4.3. Fichier <code>/etc/krb5.conf</code>","text":"<pre><code> [libdefaults]\n        default_realm = CASDDS.CASD\n\n        default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        permitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        proxiable = true\n        ticket_lifetime = 24h\n        dns_lookup_realm = true\n        dns_lookup_kdc = true\n        dns_canonicalize_hostname = false\n        rdns = false\n         allow_weak_crypto = true\n# The following libdefaults parameters are only for Heimdal Kerberos.\n        fcc-mit-ticketflags = true\n\n[realms]\n        CASDDS.CASD = {\n                kdc = 10.50.5.64\n                admin_server = 10.50.5.64\n        }\n\u2026..\n[domain_realm]\n\u2026.\n        .casdds.casd = CASDDS.CASD\n        casdds.casd = CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-5-configuration-ssh-avec-gssapi","title":"\u00c9tape 5 : Configuration SSH avec GSSAPI","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#51-sur-debian","title":"5.1. Sur Debian","text":"<p>Modifier <code>/etc/ssh/sshd_config</code> :</p> <pre><code>UsePam yes\nGSSAPIAuthentication yes # l'authentification bas\u00e9e sur GSSAPI pour les connexions SSH\nGSSAPICleanupCredentials yes # la suppression automatique des identifiants temporaires obtenus via GSSAPI apr\u00e8s leur utilisation pour renforcer la s\u00e9curit\u00e9\nGSSAPIKeyExchange yes # s\u00e9curiser l'\u00e9change de cl\u00e9s, prot\u00e9geant ainsi le processus de n\u00e9gociation contre les interceptions\nGSSAPIStrictAcceptorCheck no # D\u00e9sactive la v\u00e9rification stricte de l'identit\u00e9 de l'acceptateur, facilitant les connexions dans des environnements o\u00f9 les noms de principal peuvent varier\n</code></pre> <p>Modifier <code>/etc/ssh/ssh_config</code>:</p> <pre><code>   Host *\n       \u2026\n       GSSAPIAuthentication yes\n       GSSAPIDelegateCredentials yes # d\u00e9l\u00e9guer les identifiants GSSAPI du client au serveur pour \n</code></pre> <p>Red\u00e9marrer les services :</p> <pre><code>sudo systemctl restart sshd sssd\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#52-sur-windows","title":"5.2. Sur Windows","text":"<p>Installer OpenSSH via PowerShell:</p> <pre><code>Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\nStart-Service sshd\nSet-Service -Name sshd -StartupType 'Automatic'\n</code></pre> <p>Activer GSSAPI dans <code>C:\\ProgramData\\ssh\\sshd_config</code> :</p> <pre><code>GSSAPIAuthentication yes\nGSSAPICleanupCredentials yes\n</code></pre> <p>Red\u00e9marrer le service:</p> <pre><code> Restart-Service sshd\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-6-validation","title":"\u00c9tape 6 : Validation","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#61-test-kerberos","title":"6.1. Test Kerberos","text":"<p>Sur Debian :</p> <pre><code> kinit user@CASDDS.CASD  # Authentifier avec le mot de passe AD\nklist    \n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#62-connexion-ssh","title":"6.2. Connexion SSH","text":"<p>Depuis Windows :</p> <pre><code>ssh -K user@debian.casdds.casd  # -K active la d\u00e9l\u00e9gation Kerberos\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#appendix-notes-importantes","title":"Appendix Notes importantes :","text":"<ul> <li>Permissions SSSD : V\u00e9rifier que <code>/etc/sssd/sssd.conf</code> a les droits <code>600</code> :</li> </ul> <pre><code>sudo chmod 600 /etc/sssd/sssd.conf\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/","title":"Integrate kerberos into a hadoop cluster","text":"<p>In this tutorial, we will show how to integrate kerberos into a hadoop cluster. The goal is to use the kerberos tickets to authenticate users, hosts(e.g. namenode, datanode, resourceManager, etc.) and services(e.g. hdfs, yarn). </p> <p>We have different strategy for different kinds of users. For service account, we will generate <code>.keytab</code> files to generate kerberos tickets automatically. For user account, a password maybe required to generate the ticket.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#1-prerequisite","title":"1. Prerequisite","text":"<p>Before we start, we need to clarify the hadoop cluster context. Because the <code>AD/Ldap account</code> and <code>kerberos principal</code> naming conventions strongly depends on the cluster architecture.</p> <p>Suppose we have three servers, in each server we run different services: - spark-m01.casdds.casd: name-node(hdfs), resource-manager(yarn), history-server(spark) - spark-m02.casdds.casd: data-node(hdfs), node-manager(yarn) - spark-m03.casdds.casd: data-node(hdfs), node-manager(yarn)</p> <p>We suppose you already join these machines into the AD/krb realm. For more details, you can check this  doc </p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#11-prepare-service-account-and-their-keytab","title":"1.1 Prepare service account and their keytab","text":"<p>By convention, we recommend you to create <code>a dedicated AD/Ldap account and kerberos principal for each service</code>.  This ensures <code>secure authentication</code> and <code>proper ticket management</code>. It's also easier to monitor access and avoid  unexpected situations. Technically, an AD/Ldap account can be associated with one or more kerberos principals.</p> <p>Below is a list of all AD/Ldap accounts and kerberos principal you need to create:</p> Service Hadoop Role Host Kerberos Principal AD account name HDFS NameNode spark-m01.casdds.casd nn/spark-m01.casdds.casd@CASDDS.CASD hdfs-nn HDFS DataNode spark-m02.casdds.casd dn/spark-m02.casdds.casd@CASDDS.CASD hdfs-dn1 HDFS DataNode spark-m03.casdds.casd dn/spark-m03.casdds.casd@CASDDS.CASD hdfs-dn2 HDFS HTTP Service spark-m01.casdds.casd http/spark-m01.casdds.casd@CASDDS.CASD http-nn HDFS HTTP Service spark-m02.casdds.casd http/spark-m02.casdds.casd@CASDDS.CASD http-dn1 HDFS HTTP Service spark-m03.casdds.casd http/spark-m03.casdds.casd@CASDDS.CASD http-dn2 YARN ResourceManager spark-m01.casdds.casd rm/spark-m01.casdds.casd@CASDDS.CASD yarn-rn YARN NodeManager spark-m02.casdds.casd nm/spark-m02.casdds.casd@CASDDS.CASD yarn-nm1 YARN NodeManager spark-m03.casdds.casd nm/spark-m03.casdds.casd@CASDDS.CASD yarn-nm2 Spark History Server spark-m01.casdds.casd jhs/spark-m01.casdds.casd@CASDDS.CASD spark-jhs HOST None spark-m01.casdds.casd host/spark-m01.casdds.casd@CASDDS.CASD spark-m01 HOST None spark-m02.casdds.casd host/spark-m02.casdds.casd@CASDDS.CASD spark-m02 HOST None spark-m03.casdds.casd host/spark-m03.casdds.casd@CASDDS.CASD spark-m03 <p>The AD account name cannot contain special character such as <code>@</code> and <code>.</code>, so we can't use the principal name as  AD account name. </p> <p>You can create an AD account in windows with the below command</p> <pre><code># create AD account and kerberos principal\nNew-ADUser -Name \"hdfs-nn\" -SamAccountName \"hdfs-nn\" -UserPrincipalName \"nn/spark-m01.casdds.casd@CASDDS.CASD\" -Enabled $true -PasswordNeverExpires $true -CannotChangePassword $true -ChangePasswordAtLogon $false -PassThru | Set-ADAccountControl -PasswordNotRequired $true\n\n# create corresponding keytab\nktpass -princ nn/spark-m01.casdds.casd@CASDDS.CASD -mapuser hdfs-nn -crypto ALL -ptype KRB5_NT_PRINCIPAL -pass Password! -out hdfs-nn.keytab\n</code></pre> <p>After you generate the required keytab files for all principals, you need to copy them to the target server. For example, for server <code>spark-m01.casdds.casd@CASDDS.CASD</code>, you need to copy the keytab file for principals: - nn/spark-m01.casdds.casd@CASDDS.CASD - HTTP/spark-m01.casdds.casd@CASDDS.CASD - rm/spark-m01.casdds.casd@CASDDS.CASD - jhs/spark-m01.casdds.casd@CASDDS.CASD - host/spark-m01.casdds.casd@CASDDS.CASD</p> <p>The general rule is straightforward, you need to check the host fqdn name in the principals </p> <p>You can test the validity of the keytab file by asking a kerberos ticket. Below command is an example</p> <pre><code>kinit -kt /etc/hdfs-nn.keytab nn/spark-m01.casdds.casd@CASDDS.CASD\n</code></pre> <p>To show the details of a keytab file, you can use the below command: </p> <pre><code>klist -e -k -t /etc/yarn.keytab\n\n# expected output\nKeytab name: FILE: /etc/yarn.keytab\n KVNO Timestamp         Principal\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (AES-256 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (AES-128 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (ArcFour with HMAC/md5)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (AES-256 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (AES-128 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (ArcFour with HMAC/md5)\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#12-merge-the-keytab-files","title":"1.2 Merge the keytab files","text":"<p>To avoid managing many keytab files, you can merge the multi keytab files into one by using <code>ktutil</code> tool.</p> <pre><code># start a ktuitl shell with sudo right\nsudo ktutil\n\n# load credentials from the keytab files\nrkt /tmp/yarnm02.keytab\nrkt /tmp/hostm02.keytab\n\n# output the loaded credential to a new keytable file\nwkt /tmp/merged.keytab\n\n# exit the ktuitl shell\nq\n</code></pre> <p>You can test the content of the merged keytab file with the below command</p> <pre><code>sudo klist -k /tmp/merged.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#13-check-if-the-hadoop-servers-are-in-the-ad-dns","title":"1.3. Check if the hadoop servers are in the AD DNS","text":"<p>Normally, when the linux servers have joined the AD/Krb realm, their AD/DNS configuration are done automatically.</p> <p>Just to make sure, you can open the DNS manager on the <code>Domain controller</code> where AD/Krb is located. Below figure is an example of the <code>DNS manager GUI</code></p> <p></p> <p>You need to check the <code>value of FQDN and ip</code> for each server in <code>forward and reverse lookup zones</code>.</p> <p>Below figure is an example for the Forward loopup zone definition of a server</p> <p></p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#14-check-kerberos-client","title":"1.4. Check kerberos client","text":"<p>Normally, you should have a valid krb5 client and config on each hadoop node.</p> <p>Below is an example of the krb5 client conf(<code>/etc/krb5.conf</code>).</p> <pre><code>[libdefaults]\ndefault_realm = CASDDS.CASD\ndefault_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\ndefault_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\npermitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\nkdc_timesync = 1\nccache_type = 4\nforwardable = true\nticket_lifetime = 24h\ndns_lookup_realm = true\ndns_lookup_kdc = true\n\n#allow_weak_crypto = true\n\n[realms]\nCASDDS.CASD = {\n    kdc = 10.50.5.64\n    admin_server = 10.50.5.64\n}\n\n[domain_realm]\n.CASDDS.CASD = CASDDS.CASD\nCASDDS.CASD = CASDDS.CASD\n</code></pre> <p>You can enable allow_weak_crypto = true, if the AD/Krb can't use advance crypto algorithm</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#2-enable-ssl-in-the-hadoop-cluster","title":"2. Enable SSL in the hadoop cluster","text":"<p>To secure communication between services in the hadoop cluster, we can enable SSL.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#21-generate-certificate","title":"2.1 Generate certificate","text":"<p>Generate a key pair and store it in a java key store</p> <pre><code>sudo keytool -genkeypair \\\n  -alias hadoop \\\n  -keyalg RSA \\\n  -keysize 2048 \\\n  -validity 365 \\\n  -keystore /opt/hadoop/keystore.jks \\\n  -storepass changeit\n</code></pre> <p>Check the keystore content </p> <pre><code>sudo keytool -list -keystore /opt/hadoop/keystore.jks -storepass changeit\n</code></pre> <p>Export the certificate</p> <pre><code>sudo keytool -export -alias hadoop -keystore /opt/hadoop/keystore.jks -file /opt/hadoop/hadoop-cert.pem -storepass changeit\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#22-configuration-of-ssl-in-hadoop-cluster","title":"2.2 Configuration of SSL in hadoop cluster","text":"<p>The ssl configuration file in hadoop cluster is <code>$HADOOP_HOME/etc/hadoop/ssl-server.xml</code> and  <code>$HADOOP_HOME/etc/hadoop/ssl-client.xml</code>. In our case, we only need to modify <code>ssl-server.xml</code>.</p> <p>Below is an example of the <code>ssl-server.xml</code></p> <pre><code>sudo vim ssl-server.xml\n</code></pre> <p>Add the below lines</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/keystore.jks&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n    &lt;value&gt;jks&lt;/value&gt;\n    &lt;description&gt;(Optionnel) Format du keystore (par d\u00e9faut \u00ab jks \u00bb).&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.exclude.cipher.list&lt;/name&gt;\n    &lt;value&gt;\n      TLS_ECDHE_RSA_WITH_RC4_128_SHA,\n      SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,\n      SSL_RSA_WITH_DES_CBC_SHA,\n      SSL_DHE_RSA_WITH_DES_CBC_SHA,\n      SSL_RSA_EXPORT_WITH_RC4_40_MD5,\n      SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,\n      SSL_RSA_WITH_RC4_128_MD5\n    &lt;/value&gt;\n    &lt;description&gt;(Optionnel) Liste des suites de chiffrement faibles \u00e0 exclure.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>This needs to be done all nodes of the hadoop cluster</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#2-integrate-kerberos-in-to-hadoop-cluster","title":"2. Integrate kerberos in to Hadoop cluster","text":"<p>Here, we suppose you already have a working hadoop cluster, the below steps only shows how to integrate kerberos into Hadoop. If you want to learn how to deploy a hadoop cluster, you need to follow this  doc</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#21-edit-hadoop-envsh","title":"2.1 Edit hadoop-env.sh","text":"<p>The <code>hadoop-env.sh</code> file specifies all environment variables related to the hadoop cluster</p> <p>Below is a list of variables you need to check </p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext\"\n# use krb client config \nexport HADOOP_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf $HADOOP_OPTS\"\nexport HDFS_NAMENODE_USER=hadoop\nexport HDFS_DATANODE_USER=hadoop\nexport HDFS_SECONDARYNAMENODE_USER=hadoop\nexport JSVC_HOME=$(dirname $(which jsvc))\nexport HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport HADOOP_SECURITY_LOGGER=INFO,RFAS,console\nexport JAVA_TOOL_OPTIONS=\"$JAVA_TOOL_OPTIONS --add-opens=java.base/sun.net.dns=ALL-UNNAMED\"\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#22-updates-the-security-configuration-of-jdk","title":"2.2 Updates the security configuration of JDK","text":"<p>For kerberos interoperate well with JDK, we need to update the default security conf of the JDK. </p> <pre><code>sudo vim $JAVA_HOME/conf/security/java.security\n\n# in our case, we use openjdk in debian 11. We can use the absolute path\nsudo vim /usr/lib/jvm/java-11-openjdk-amd64/conf/security/java.security\n\n# you need to add the below lines\ncrypto.policy=unlimited\nsun.security.krb5.disableReferrals=true\n</code></pre> <p>By default, the <code>RC4</code> encryption algo is disabled, because it's weak. But some Windows server still uses it. You can find the line <code>jdk.jar.disabledAlgorithms</code> and <code>jdk.tls.disabledAlgorith</code>, then remove the <code>RC4</code> from the  disabled algo list.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#23-update-the-hadoop-service-configuration","title":"2.3 Update the hadoop service configuration","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#231-for-name-nodes","title":"2.3.1 For Name nodes","text":"<p>For <code>Name nodes</code>, you need to edit the below config files: - core-site.xml - hdfs-site.xml - yarn-site.xml</p> <pre><code>sudo vim core-site.xml \n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/etc/hadoop/ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;\n    &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/httpm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.security.AuthenticationFilterInitializer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@casdds\\.casd)s/@casdds\\.casd//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers le nom d\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim hdfs-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n    &lt;description&gt;Augmentation de la file d\u2019attente pour g\u00e9rer davantage de connexions clients.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour s\u00e9curiser l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:9870&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim yarn-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#232-for-datanode","title":"2.3.2 For DataNode","text":"<p>For data nodes, you need to edit the below configuration files: - core-site.xml - hdfs-site.xml - yarn-site.xml</p> <pre><code>sudo vim core-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@CASDDS\\.CASD)s/@CASDDS\\.CASD//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers l\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim hdfs-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;ip-10-50-5-203.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode sur le DataNode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n    &lt;value&gt;750&lt;/value&gt;\n    &lt;description&gt;Permissions requises sur les r\u00e9pertoires de donn\u00e9es.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim yarn-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m02.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#refresh-user-to-group-mappings","title":"Refresh User to group mappings","text":"<pre><code>hdfs dfsadmin -refreshUserToGroupsMappings\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#reference","title":"Reference","text":"<ul> <li>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</li> <li>http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.5/security-reference/content/kerberos_nonambari_adding_security_information_to_configuration_files.html</li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#repo-test","title":"Repo test","text":"<p>https://github.com/CASD-EU/admin_sys/tree/test/dev/roles</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/","title":"Integrate kerberos in hadoop cluster fr","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#integration-de-kerberos-dans-un-cluster-hadoop","title":"Int\u00e9gration de Kerberos dans un cluster Hadoop","text":"<p>Cette documentation d\u00e9crit les \u00e9tapes pour s\u00e9curiser l\u2019authentification des composants d\u2019un cluster Hadoop (NameNode, DataNode, ResourceManager, etc.) \u00e0 l\u2019aide de Kerberos et SSL.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#1-contexte-et-architecture","title":"1. Contexte et architecture","text":"<ul> <li> <p>Cluster : 3 n\u0153uds (spark-m01, spark-m02, spark-m03) dans le domaine CASDDS.CASD</p> </li> <li> <p>spark-m01 : NameNode, ResourceManager, HistoryServer</p> </li> <li> <p>spark-m02 &amp; spark-m03 : DataNode, NodeManager</p> </li> <li> <p>But :</p> </li> <li> <p>Authentification forte via Kerberos</p> </li> <li>Chiffrement des communications via SSL/TLS</li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#2-prerequis","title":"2. Pr\u00e9requis","text":"<ol> <li> <p>AD/Kerberos</p> </li> <li> <p>Les serveurs Linux doivent \u00eatre joints au domaine AD/Kerberos <code>CASDDS.CASD</code></p> </li> <li> <p>Un contr\u00f4leur de domaine (KDC + DNS) configur\u00e9 pour forward/reverse lookup</p> </li> <li> <p>Logiciels</p> </li> <li> <p>Java\u00a011 (OpenJDK)</p> </li> <li>Hadoop 3.x</li> <li>Utilitaires Kerberos (<code>kinit</code>, <code>klist</code>, <code>ktpass</code>, <code>ktutil</code>)</li> <li>Keytool (Java)</li> </ol>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#3-configuration-kerberos","title":"3. Configuration Kerberos","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#31-creation-des-comptes-service-et-keytabs","title":"3.1. Cr\u00e9ation des comptes service et keytabs","text":"<p>Pour chaque service, cr\u00e9ez un compte AD d\u00e9di\u00e9 et g\u00e9n\u00e9rez un fichier <code>.keytab</code> :</p> Service R\u00f4le FQDN Principal Kerberos AD User HDFS NameNode NameNode spark-m01.casdds.casd <code>hdfs/spark-m01.casdds.casd@CASDDS.CASD</code> <code>hdfs-m01</code> HDFS DataNode DataNode spark-m02.casdds.casd <code>hdfs/spark-m02.casdds.casd@CASDDS.CASD</code> <code>hdfs-m02</code> HDFS DataNode DataNode spark-m03.casdds.casd <code>hdfs/spark-m03.casdds.casd@CASDDS.CASD</code> <code>hdfs-m03</code> HTTP HTTP Service spark-m01.casdds.casd <code>HTTP/spark-m01.casdds.casd@CASDDS.CASD</code> <code>http-m01</code> YARN RM ResourceManager spark-m01.casdds.casd <code>yarn/spark-m01.casdds.casd@CASDDS.CASD</code> <code>yarn-m01</code> YARN NM NodeManager spark-m0X.casdds.casd <code>yarn/spark-m0X.casdds.casd@CASDDS.CASD</code> <code>yarn-m0x</code> HOST Host principal spark-m0X.casdds.casd <code>host/spark-m0X.casdds.casd@CASDDS.CASD</code> <code>host-m0X</code> <p>Commande Windows (AD) :</p> <pre><code>New-ADUser -Name \"hdfs-m01\" -SamAccountName \"hdfs-m01\" \\\n  -UserPrincipalName \"hdfs/spark-m01.casdds.casd@CASDDS.CASD\" \\\n  -Enabled $true -PasswordNeverExpires $true -CannotChangePassword $true\n\nktpass -princ hdfs/spark-m01.casdds.casd@CASDDS.CASD \\\n  -mapuser hdfs-m01 -crypto ALL -ptype KRB5_NT_PRINCIPAL \\\n  -pass \"Password!\" -out hdfs-m01.keytab\n\nscp hdfs-m01.keytab user@spark-m0x.casdds.casd:/tmp/\n</code></pre> <p>Copiez chaque keytab sous <code>/etc/</code> sur le serveur correspondant, avec permissions <code>root:hadoop</code>, <code>chmod 640</code>.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#32-verification-des-keytabs","title":"3.2. V\u00e9rification des keytabs","text":"<pre><code># Authentication sans mot de passe\nkinit -kt /etc/hdfs-m01.keytab hdfs/spark-m01.casdds.casd@CASDDS.CASD\n# Afficher tickets\nklist\n# Lister contenu d'un keytab\nklist -e -k -t /etc/hdfs-m01.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#33-fusion-de-plusieurs-keytabs","title":"3.3. Fusion de plusieurs keytabs","text":"<pre><code>sudo ktutil\nrkt /tmp/yarn-m02.keytab\nrkt /tmp/host-m02.keytab\nwkt /etc/merged.keytab\nq\nsudo klist -k /etc/merged.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#4-configuration-du-client-kerberos-etckrb5conf","title":"4. Configuration du client Kerberos (/etc/krb5.conf)","text":"<pre><code>[libdefaults]\n  default_realm = CASDDS.CASD\n  default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  permitted_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  kdc_timesync = 1\n  ticket_lifetime = 24h\n  forwardable = true\n  dns_lookup_realm = true\n  dns_lookup_kdc = true\n  rdns = false\n  #allow_weak_crypto = true\n\n[realms]\n  CASDDS.CASD = {\n    kdc = @ip\n    admin_server = @ip\n  }\n\n[domain_realm]\n  .casdds.casd = CASDDS.CASD\n  casdds.casd = CASDDS.CASD\n</code></pre> <p>D\u00e9commentez <code>allow_weak_crypto = true</code> si n\u00e9cessaire pour RC4.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#5-securisation-ssltls","title":"5. S\u00e9curisation SSL/TLS","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#51-generation-du-keystore-java","title":"5.1. G\u00e9n\u00e9ration du keystore Java","text":"<pre><code>sudo keytool -genkeypair \\\n  -alias hadoop -keyalg RSA -keysize 2048 -validity 365 \\\n  -keystore /opt/hadoop/keystore.jks -storepass changeit\nsudo keytool -list -keystore /opt/hadoop/keystore.jks -storepass changeit\nsudo keytool -export -alias hadoop \\\n  -file /opt/hadoop/hadoop-cert.pem -keystore /opt/hadoop/keystore.jks \\\n  -storepass changeit\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#52-configuration-ssl-serverxml","title":"5.2. Configuration <code>ssl-server.xml</code>","text":"<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/keystore.jks&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n        &lt;value&gt;jks&lt;/value&gt;\n        &lt;description&gt;Optional. The keystore file format, default value is \"jks\".&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.exclude.cipher.list&lt;/name&gt;\n        &lt;value&gt;TLS_ECDHE_RSA_WITH_RC4_128_SHA,SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,\n        SSL_RSA_WITH_DES_CBC_SHA,SSL_DHE_RSA_WITH_DES_CBC_SHA,\n        SSL_RSA_EXPORT_WITH_RC4_40_MD5,SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,\n        SSL_RSA_WITH_RC4_128_MD5&lt;/value&gt;\n        &lt;description&gt;Optional. The weak security cipher suites that you want excludedfrom SSL communication.&lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>R\u00e9p\u00e9tez l\u2019op\u00e9ration sur tous les n\u0153uds.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#6-configuration-hadoop","title":"6. Configuration Hadoop","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#61-hadoop-envsh","title":"6.1. <code>hadoop-env.sh</code>","text":"<pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext\"\nexport HADOOP_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf $HADOOP_OPTS\"\nexport HDFS_NAMENODE_USER=hadoop\nexport HDFS_DATANODE_USER=hadoop\nexport HDFS_SECONDARYNAMENODE_USER=hadoop\nexport JSVC_HOME=$(dirname $(which jsvc))\nexport HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport HADOOP_SECURITY_LOGGER=INFO,RFAS,console\nexport JAVA_TOOL_OPTIONS=\"$JAVA_TOOL_OPTIONS --add-opens=java.base/sun.net.dns=ALL-UNNAMED\"\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#62-politiques-de-securite-java","title":"6.2. Politiques de s\u00e9curit\u00e9 Java","text":"<pre><code># $JAVA_HOME/conf/security/java.security\ncrypto.policy = unlimited\nsun.security.krb5.disableReferrals = true\n# Retirer RC4 de jdk.jar.disabledAlgorithms / jdk.tls.disabledAlgorithms si besoin\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#63-configuration-des-services","title":"6.3. Configuration des services","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#631-namenode-core-sitexml-hdfs-sitexml-yarn-sitexml","title":"6.3.1. NameNode (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code>)","text":"<pre><code>&lt;!-- core-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/etc/hadoop/ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;\n    &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/http-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.security.AuthenticationFilterInitializer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@casdds\\.casd)s/@casdds\\.casd//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers le nom d\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- hdfs-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n    &lt;description&gt;Augmentation de la file d\u2019attente pour g\u00e9rer davantage de connexions clients.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour s\u00e9curiser l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:9870&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- yarn-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#632-datanode-core-sitexml-hdfs-sitexml-yarn-sitexml","title":"6.3.2. DataNode (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code>)","text":"<pre><code>&lt;!-- core-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@CASDDS\\.CASD)s/@CASDDS\\.CASD//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers l\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- hdfs-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;ip-x.x.x.x.casdds.casd:50470&lt;/value&gt;  &lt;!-- @ip namdenode --&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode sur le DataNode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n    &lt;value&gt;750&lt;/value&gt;\n    &lt;description&gt;Permissions requises sur les r\u00e9pertoires de donn\u00e9es.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- yarn-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m0x.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#7-validation-et-tests","title":"7. Validation et tests","text":"<ol> <li>Tester kinit sur chaque n\u0153ud/service</li> <li>D\u00e9marrer Hadoop en mode s\u00e9curis\u00e9 </li> <li>V\u00e9rifier que les services \u00e9coutent en HTTPS et que <code>klist</code> montre les tickets actifs</li> <li>Rafra\u00eechir les mappings utilisateurs-groupes :</li> </ol> <p><code>bash    hdfs dfsadmin -refreshUserToGroupsMappings</code></p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#8-references","title":"8. R\u00e9f\u00e9rences","text":"<ul> <li> <p>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</p> </li> <li> <p>http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.5/security-reference/content/kerberos_nonambari_adding_security_information_to_configuration_files.html</p> </li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#9-repo-ansible","title":"9. Repo Ansible","text":"<ul> <li>https://github.com/CASD-EU/admin_sys/tree/test/dev/roles</li> </ul>"}]}