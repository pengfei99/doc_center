{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Pengfei doc center","text":"<p>This website is build by using <code>mkdocs</code>. For more information about <code>mkdocs</code>, please visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Onyxia/01.Provisioning_Infrastructure/","title":"01. Provisioning infrastructure","text":"<p>To determine how many servers you need to set up an appropriate infrastructure, you can follow the official doc</p> <p>In this tutorial, I will have 3 master node, 3 worker node.</p>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#11-create-the-vm","title":"1.1 Create the vm","text":"<p>Here I suppose you use the on-premise cloud infrastructure to creat the VMs. For testing you can use any solution (e.g virtual box). You can find some docs on vm creation and network setup for virtual box here.</p>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#12-configure-the-vm","title":"1.2 Configure the vm","text":"<p>To allow kubespray to install packages, you need to set up the vm with some specific configuration such as</p> <ul> <li>selinux</li> <li>firewall</li> <li>ssh connection</li> </ul>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#121-disable-selinux-centos","title":"1.2.1 Disable selinux (Centos )","text":""},{"location":"Onyxia/01.Provisioning_Infrastructure/#122-firewall-debianubuntu","title":"1.2.2 firewall (debian/ubuntu)","text":"<p>The default firewall in ubuntu is called <code>ufw</code>, you can use below command to start/stop it or check its status.</p> <pre><code># enable/disable\nsudo ufw enable/disable\n\n# get the status of the firewall\nsudo ufw status\n\n# for more details, you can add `verbose`\nsudo ufw status verbose\n</code></pre>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#123-configure-ssh-tunnel","title":"1.2.3 Configure ssh tunnel","text":"<p>Ansible needs ssh tunnel to install and configure everything. So having a ssh is essential</p>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#install-openssh-server","title":"install openssh-server","text":"<pre><code># update package repo cache\nsudo apt update\n\n# intall the server\nsudo apt install openssh-server\n\n# check the ssh server status\nsudo systemctl status ssh\n\n# start/stop the ssh server\nsudo systemctl start/stop ssh\n\n# adding and removing the ssh server in the system startup list\nsudo systemctl enable/disable ssh\n</code></pre>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#generate-ssh-key","title":"generate ssh key","text":"<p>Below command will allow you to generate a new Asymmetric keypair by using <code>ecdsa</code> algo, the key size will be 521 bits with a comment (mail of the owner).</p> <pre><code>ssh-keygen -t ecdsa -b 521 -C \"your_email@domain.com\"\n\n# you should see below output\nYour identification has been saved in /home/pliu/.ssh/id_ecdsa\n\nYour public key has been saved in /home/pliu/.ssh/id_ecdsa.pub\n\nThe key fingerprint is:\n\nSHA256:0932dP01k2JEPAHWcDjWtfLcnanLW+KtOPo+7R6tG+8 pliu@debian01\n\nThe key's randomart image is:\n\n+---[ECDSA 521]---+\n\n|           +B+o. |\n\n|          .+o=  .|\n\n|          . .o.. |\n\n|         . ...+ B|\n\n|        S . .ooOB|\n\n|         .  ..+o*|\n\n|            .= oo|\n\n|           o+.X  |\n\n|         .++=@=E |\n\n+----[SHA256]-----+\n</code></pre> <p>The private key should be stored in the server which runs kubepray. The public key should be stored in the server (e.g. master node, worker node) of the k8s cluster.</p>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#copy-the-generate-public-key-to-k8s-cluster-servers","title":"Copy the generate public key to k8s cluster servers","text":"<p>The best way is to use <code>ssh-copy-id</code> tool. The syntax is:</p> <pre><code>ssh-copy-id username@target_url/ip\n\n# for example\nssh-copy-id pliu@10.0.2.4\n\n# now try to ssh to the \nssh pliu@10.0.2.4\n</code></pre> <p>If you don't have <code>ssh-copy-id</code> tool, you can copy the public key Manually.</p> <p>In the target server, create a file (~/.ssh/authorized_keys) inside the user namespace which you want to use to ssh.</p> <p>Get the content of public key</p> <pre><code>cat ~/.ssh/id_ecdsa.pub \n\n# You should see something like this\necdsa-sha2-nistp521 AAAAE2VjZHNhLXNoYTItbmlzdHA1MjEAAAAIbmlzdHA1MjEAAACFBADCrz1ud4x0++fZ18KTnYBoqlwe3Qj9Y0TLF87oZj1xyOCnBimj0mkutMUy4+Mmu3yVDfPs23wH31vqx+jV31H5OwCydxd/KBz8fu+fOStt8HIyFix8lq45DhzDnBbaf/etOsWHTz4r13WYazADLVDJAOKTG7SGy9JBFFSyBEDx6ZjUbA== pliu@debian01\n</code></pre> <p>Now copy the public key to the file (~/.ssh/authorized_keys). Note if you are using root account to do the copy, don't forget to change the owner and group of the file. Otherwise, the user can't open it, because it belongs to root.</p> <pre><code>chown -R pliu:pliu /home/pliu/.ssh\n</code></pre>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#disable-password-auth-optional","title":"Disable password auth (Optional)","text":"<p>In the <code>/etc/ssh</code> folder, you can find the sshd_config file (the main config of ssh server). Add below line to disable password ssh auth</p> <pre><code>PasswordAuthentication no\n</code></pre>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#restart-ssh-service","title":"Restart ssh service","text":"<p>To make the change effective, you need to restart the ssh server</p> <pre><code>sudo systemctl restart ssh\n</code></pre>"},{"location":"Onyxia/01.Provisioning_Infrastructure/#disable-swap","title":"Disable swap","text":"<p>Kubelet can't manage swap. So you need to disable swap on all nodes.</p> <pre><code># Check the current status of the file system\nlsblk -o +PARTTYPE\n\n# disable swap if you find any\n$ sudo swapoff -a\n$ sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n</code></pre>"},{"location":"Onyxia/02.1.Manage_TLS_CERT_in_k8s/","title":"Manage TLS Certificates in a Cluster","text":"<p>Kubernetes provides a <code>certificates.k8s.io</code> API, which lets you provision TLS certificates signed by a Certificate Authority (CA) that you control. These CA and certificates can be used by your workloads to establish trust.</p> <p><code>certificates.k8s.io</code> API uses a protocol that is similar to the ACME draft.</p> <p>Certificates created using the <code>certificates.k8s.io</code> API are signed by a dedicated CA. It is possible to configure your cluster to use the cluster root CA for this purpose, but you should never rely on this. Do not assume that these certificates will validate against the cluster root CA.</p>"},{"location":"Onyxia/02.1.Manage_TLS_CERT_in_k8s/#requesting-a-certificate","title":"Requesting a certificate","text":""},{"location":"Onyxia/02.1.Manage_TLS_CERT_in_k8s/#creating-a-certificate-authority","title":"Creating a certificate authority","text":""},{"location":"Onyxia/02.1.Manage_TLS_CERT_in_k8s/#setup-a-default-ssl-certificate","title":"Setup a default ssl certificate","text":"<pre><code># if you already have ingress deployed via helm, you can edit it with below command\nkubectl edit deployment nginx-ingress-controller -n &lt;namespace&gt; \n\n# and add the --default-ssl-certificate arg into args: \n\n# Then restart nginx ingress controller.\n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/","title":"2. Configuration of kubespary (Ansible role)","text":"<p>In step 1, we have provisioned three server:</p> <pre><code>[k8s]\nk8s-01 ansible_host=10.0.2.7\nk8s-02 ansible_host=10.0.2.8\nk8s-03 ansible_host=10.0.2.9\n</code></pre> <p>In ansible_installation tutorial, we have installed ansible. So we have all the requirements to run a kubespary installation.</p> <p>Now we just need to configure kubespray correctly to install k8s on these server correctly.</p> <p>The configuration of kubespary contains two parts:</p> <ul> <li>ansible roles (a set of task ansible will run on these servers)</li> <li>inventory (a list of server which ansible will run the roles)</li> </ul>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#21-deploy-a-k8s-cluster","title":"2.1 Deploy a k8s cluster","text":"<p>Kubespray provide a default configuration which is located at <code>kubespray/inventory/sample</code></p> <p>Let's copy it to a new folder</p> <pre><code>cd /path/to/kubespray\ncp -rfp inventory/sample inventory/mycluster\n\nls inventory/mycluster\n# you should see below contents\ngroup_vars  inventory.ini\n</code></pre> <p>The inventory.ini contains a list of server with fake IP. You need to replace them with your real server IP.</p> <p>The group_vars contains a list of roles(tasks) that kubespary will install on your cluster k8s.</p> <p>For now, we only modify the inventory. Ansible accepts many different file format as inventory. I prefer the .yaml. In our case the file name will be inventory.yaml and has below content</p> <pre><code>all:\n  hosts:\n    controlplane1:\n      ansible_host: 10.0.2.7\n      ip: 10.0.2.7\n      access_ip: 10.0.2.7\n    worker1:\n      ansible_host: 10.0.2.8\n      ip: 10.0.2.8\n      access_ip: 10.0.2.8\n    worker2:\n      ansible_host: 10.0.2.9\n      ip: 10.0.2.9\n      access_ip: 10.0.2.9\n  children:\n    kube_control_plane:\n      hosts:\n        controlplane1:\n    kube_node:\n      hosts:\n        worker1:\n        worker2:\n    etcd:\n      hosts:\n        controlplane1:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n</code></pre> <p>The controlplane1is the <code>master node</code> which will host <code>etcd</code>, <code>api server</code>, etc. The <code>worker1</code> and <code>worker2</code> are the worker node, which will host the pods deployed by the users.</p> <p>Now we can run below command to start the installation</p> <pre><code>ansible-playbook -i inventory/pengfei_cluster/inventory.yaml --become --become-user=root cluster.yml\n</code></pre> <p>The option \"--become, --become-user\" allow us to run ansible with a non root user which has sudo rights.</p> <p>If everything works well, this will take 20 mins to install all the necessary on the three servers to run a k8s cluster.</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#22-add-new-nodes-to-the-cluster","title":"2.2 Add new nodes to the cluster","text":"<p>The k8s cluster can be easily scalable horizontally. With below command, we can add a new worker node to the cluster</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#221-add-new-node-to-the-inventory","title":"2.2.1 Add new node to the inventory","text":"<p>For example, now we have a new node <code>worker3:10.0.2.10</code> which we want to add to our cluster</p> <pre><code>all:\n  hosts:\n    controlplane1:\n      ansible_host: 10.0.2.7\n      ip: 10.0.2.7\n      access_ip: 10.0.2.7\n    worker1:\n      ansible_host: 10.0.2.8\n      ip: 10.0.2.8\n      access_ip: 10.0.2.8\n    worker2:\n      ansible_host: 10.0.2.9\n      ip: 10.0.2.9\n      access_ip: 10.0.2.9\n    worker3:\n      ansible_host: 10.0.2.10\n      ip: 10.0.2.10\n      access_ip: 10.0.2.10\n  children:\n    kube_control_plane:\n      hosts:\n        controlplane1:\n    kube_node:\n      hosts:\n        worker1:\n        worker2:\n        worker3:\n    etcd:\n      hosts:\n        controlplane1:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#222-apply-the-change-with-kubespray","title":"2.2.2 Apply the change with kubespray","text":"<pre><code># Note you need to activate the kubespray virtual env to run below command\nansible-playbook -i inventory/pengfei_cluster/inventory.yaml --become --become-user=root cluster.yml --limit worker3\n\n# enable kubespary virtual env\nsource /path/to/kubespary-venv/bin/activate\n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#install-kubectl-and-helm-client","title":"Install kubectl and helm client","text":"<p>During the k8s installation, you can install the kubectl and helm client on your pc</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#install-kubectl","title":"Install kubectl","text":"<p>Get the kubectl binary</p> <pre><code># download the kubectl binary\nwget https://storage.googleapis.com/kubernetes-release/release/v1.24.4/bin/linux/amd64/kubectl -O kubectl\n\n# add execution rights\nchmod a+x kubectl\n\n# move it to /bin\nmv kubectl ~/.local/bin/\n\n# if you .local/bin is not in your path, you need to add following line to your .bashrc or .bash_profile\n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#configure-kubectl","title":"Configure kubectl","text":"<p>Kubectl requires a config file to connect to the k8s api server. You can get the config file from the controlplane at <code>/etc/kubernetes/admin.conf</code>. This file should look like this:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3lOakV6TXpRMU5sb1hEVE15TURreU16RXpNelExTmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS2J2ClJraXoDdmRyeXFnTzdKNTJzeG9ONFgyTnJCYWdBQzBjcy83a3RjMzZBVjYKTVFxb2Y3M0JQQWRBNHAyVE9Eb0tsTGNkVWs4K2FiOFVYanhRbHZjeVJ4VjZOV2daamRlZHlYNTlYdWtUcWxEagp0RFdTWlJQcmRwcVhoWDMwVzFDN3haWms1SDJnRnF1MHRERmgwRUVpbWlST3p4S3h6NU1kUnE0V0NBYTg4d2dkCnlMWXFMQ2JZT2N5MTEvZEl6SXRaNUVPTlVBR2I4akdRTk9qcDZ6SHFRdlVaODRXUnp5Qkl1azB3MUJRcVFBVFMKY3I4PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://&lt;ip_controlplane&gt;:6443\n  name: cluster.local\ncontexts:\n- context:\n    cluster: cluster.local\n    user: kubernetes-admin\n  name: kubernetes-admin@cluster.local\ncurrent-context: kubernetes-admin@cluster.local\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJU1JVMUxhTVJlT1l3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBNU1qWXhNek0wTlRaYUZ3MHlNekE1TWpZeE16TTBOVGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVJYXF6VG9FSlpsQTlMZEMvYnNTRjZRcXFyajhIago0WE5VdUY1ams5TGZRQ0JFWjVuU1pnNTJVcnJ3OHQ0bDVDZ0dSd3A1MVFCT095aEtWVWdwSktHZi85TEdRLzVpCjdxNkkrblROREFXRDZtYjYwc3dDZEpxVXkyZ0xUQ3ZGM1FRUEhSb1VacFFzeFRUU3ZoNkdJeDMzSlRxejhHQXMKalovTTk1ck5jcTRVbVJYVTB0WnQweDB2dmlaUXFYTmdoZUlad3NFa1U2UHlvVEUzSnc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBb1ErZXJyaE5rV25SaXVYazVJeU84UXJZK2lxOHdDM3A4aXR5MlFzL2kxNHNkaGE4CjZiTXczL1FQNTVKYlZmSjJxMnFOb2pwaDlCZ2ErRytyRlBoQWp4SDVPUGZxZStNYWlLWXpSWnlJSWlVR0twaFAKMmR3TXZmL0xNdkNBWCthZUJyMUVUOFF4RFkyWWxqMURqT1NLYThnZXdsSnJKN2NJNXd2Mm5PSGtFM01XaC9CSApEN3BkT3dobFJRSUdRMDdEbWJtcTFUOHN4L0lrNm1oK1liM2tvN3BJTmJ1T1EwYWRqNzJWU0lsMVZTMFJsRWdNCndjNWk1WG1IZmVaIvZzZQYVF3aC9RTHJvd1dkdlQ3d2pNU0cvbGQ0ejNrRQpVQVlKUlFRWmN1QjFrYzlRdGZHS0xKeHFZeCtteEJuam0vRjYyRVpMa0RTTFowMmxZMkt1VldSdkNlQ2VXRTk5CkRCdnJBb0dCQUpxMDBiaWQ1OS92d3VoUzBjN05YcFkvcENQVERTS20zdE0xcGhLbnFrSkFsYzI5VWNxYWxqVngKVjZlUGhoakR1RGgvODl6WTZWWmFLeW9laS9VbFJJVVZ3M1pieFVoZ0diZkF3YWhWUXRtMkFsY2UzcHAyempWVgpVWlZiVHdta1JnV3lEUlhQZFZnUkgzNCtEZllmaUZkdFc3TEYvUVd2YmxFRm5salBHVG15Ci0tLS0tRU5EIFJTQSBQUklWQVRFIEtFWS0tLS0tCg==\n</code></pre> <p>In the pc where you have installed kubectl, do the following command</p> <pre><code>mkdir -p ~/.kube/\n\nvim ~/.kube/config\n</code></pre> <p>Then copy the above <code>admin.conf</code> file. Note you need to modify the <code>server url</code>. For example, if the IP of your controlplane is 10.50.5.58. You need to put this IP on the config file</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#install-helm","title":"Install helm","text":"<p>See doc post_k8s_installation_config.md</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#test-your-k8s-cluster","title":"Test your k8s cluster","text":"<p>After the installation is finished, you can test your k8s cluster</p> <pre><code>kubectl get nodes\n\n# You will get all available nodes in your k8s cluster (master and worker)\nNAME            STATUS   ROLES                  AGE    VERSION\n\ncontrolplane1   Ready    control-plane,master   142m   v1.23.7\n\nworker1         Ready    &lt;none&gt;                 139m   v1.23.7\n\nworker2         Ready    &lt;none&gt;                 139m   v1.23.7\n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#certificate-management-in-k8s","title":"Certificate management in k8s","text":"<p>The components of the created cluster use certificates to communicate with each other. By default, Client  certificates <code>generated by kubeadm expire after 1 year</code>. We need to renew them. You can find the full doc here https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#check-certificate-expiration","title":"Check certificate expiration","text":"<pre><code>kubeadm certs check-expiration\n\n# Output \nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Nov 08, 2024 15:39 UTC   361d            ca                      no      \napiserver                  Nov 08, 2024 15:39 UTC   361d            ca                      no      \napiserver-kubelet-client   Nov 08, 2024 15:39 UTC   361d            ca                      no      \ncontroller-manager.conf    Nov 08, 2024 15:39 UTC   361d            ca                      no      \nfront-proxy-client         Nov 08, 2024 15:39 UTC   361d            front-proxy-ca          no      \nscheduler.conf             Nov 08, 2024 15:39 UTC   361d            ca                      no      \n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Sep 23, 2032 13:34 UTC   8y              no      \nfront-proxy-ca          Sep 23, 2032 13:34 UTC   8y              no      \n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#automatic-certificate-renewal","title":"Automatic certificate renewal","text":"<p>kubeadm renews all the certificates during <code>control plane upgrade</code>.</p> <p>This feature is designed for addressing the simplest use cases; if you don't have specific requirements on certificate  renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade), kubeadm will  take care of keeping your cluster up to date and reasonably secure.</p>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#manual-certificate-renewal","title":"Manual certificate renewal","text":"<p>You can renew your certificates manually at any time with the <code>kubeadm certs renew</code> command, with the appropriate  command line options.</p> <p><code>kubeadm certs renew</code> can renew any specific certificate or, with the subcommand all, it can renew all of them, as shown below:</p> <pre><code>kubeadm certs renew all\n</code></pre>"},{"location":"Onyxia/02.Setup_k8s_kubespray/#external-ca-mode","title":"External CA mode","text":"<p>Todo</p>"},{"location":"Onyxia/02_1.Setup_ingress/","title":"Set up a reverse proxy","text":"<p>A reverse proxy is essential for a k8s cluster. Otherwise, the app deployed in the cluster are not accessible from  outside world. There are many possible reverse proxy solutions such as: - Kong (commercial alternative): https://konghq.com/ - Traefik (commercial alternative): https://traefik.io/</p> <p>In this tutorial, we choose ingress-nginx.</p>"},{"location":"Onyxia/02_1.Setup_ingress/#the-ingress-nginx-controller-mode","title":"The ingress nginx controller mode","text":"<p>There are three modes to set up the proxy and reverse proxy for a k8s cluster: - host - load balancer - nodePort</p>"},{"location":"Onyxia/02_1.Setup_ingress/#host-mode","title":"Host mode","text":"<p>In host mode, the <code>Ingress controller</code> it uses the host's network namespace. This means that the Ingress  controller binds directly to the host's network interfaces and ports. </p> <p>The advantage of the host mode is that it can achieve higher performance compared to other modes,  as it eliminates the overhead of the kube-proxy layer.</p> <p>The disadvantage is that you cannot run multiple instances of the Ingress controller on the same host with the  same ports, as there would be port conflicts.</p> <p>To view the detailed configuration of host mode, check the section of <code>hostNetwork: true</code> section in the <code>values.yaml</code>  template.</p>"},{"location":"Onyxia/02_1.Setup_ingress/#load-balancer-mode","title":"Load balancer mode","text":"<p>In the load balancer mode, the Ingress controller typically runs as a service, and an <code>external load balancer</code> (normally provided by the cloud provider) is provisioned to distribute incoming traffic to the Ingress controller service.</p> <p>This mode is suitable for cloud environments where a load balancer service can be provisioned dynamically (e.g., AWS ELB, GCP Load Balancer). The external load balancer takes care of distributing traffic to the nodes running the Ingress controller service.</p> <p>To view the detailed configuration, check the section of <code>appProtocol:True</code> section</p>"},{"location":"Onyxia/02_1.Setup_ingress/#nodeport-mode","title":"nodePort mode","text":"<p>In NodePort mode, the Ingress controller service is exposed on a static port on each node in the cluster.  This port is accessible from outside the cluster, and the traffic is then forwarded to the Ingress controller.</p> <p>This mode is often used in on-premises or bare-metal environments where cloud load balancers are not available or  in development/testing scenarios.</p> <p>While it provides external access, it might not be as suitable for production environments due to potential  challenges in scaling and managing external access.</p>"},{"location":"Onyxia/02_1.Setup_ingress/#our-selected-mode-is-host-mode","title":"Our selected mode is Host mode","text":"<p>In this tutorial, we choose the host mode. So the <code>ingress-nginx</code> listens to the network interface of the host server. As result, only one ingress nginx pod can be deployed on each node. And we don't want to have too many pod. So  we added <code>node selector</code> on the ingress nginx controller service.</p>"},{"location":"Onyxia/02_1.Setup_ingress/#select-which-node-to-deploy-the-ingress-nginx-controller","title":"Select which node to deploy the ingress nginx controller","text":"<p>As we added <code>node selector</code> on the ingress nginx controller service, we also need to label one or m ore <code>worker node (host server)</code>, otherwise no pods will be deployed.</p> <p>In our case, we label only one node. Because we need to set up a dns resolver entry so the query can be redirected to the node which contains the ingress controller. Even we have two pods of Ingress controller, the other one which not in the DNS will never be used (wast of resource). </p> <pre><code># FQDN for k8s Ingress controller\n10.50.5.68   *.casd.local\n</code></pre> <p>Below command show you how to label a node with a specific label</p> <pre><code># to label a node, \nkubectl label node &lt;nodename&gt; &lt;label-key&gt;=&lt;label-value&gt;\n\n# example\nkubectl label node worker2 public=true\n\n# to un-label a node, you can use below command\nkubectl label node &lt;nodename&gt; &lt;labelname&gt;-\n\n# example\nkubectl label node worker2 public-\n\n# after labeling, you will see new pod of nginx gets created.\nkubectl get all -n ingress-nginx -w\nkubectl get pods -n ingress-nginx -w\n</code></pre>"},{"location":"Onyxia/02_1.Setup_ingress/#deploy-the-ingress-nginx-controller-service","title":"Deploy the ingress nginx controller service","text":"<p>You can use below commands to deploy the ingress nginx controller service</p> <pre><code># add ingress-nginx helm repo\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n\n# update the repo content\nhelm repo update\n\n# list available release \nhelm search repo\n\n# we want it to run is the namespace ingress-nginx, so we create a namespace\nkubectl create namespace ingress-nginx\n</code></pre>"},{"location":"Onyxia/02_1.Setup_ingress/#configure-the-ingress-nginx-controller","title":"Configure the ingress-nginx controller","text":"<p>With the above ingress-nginx repo, we can install an <code>ingress-nginx controller</code> service in out cluster.</p> <p>You can find the full doc https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/</p> <p>You can find the values.yaml template here https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml</p>"},{"location":"Onyxia/02_1.Setup_ingress/#a-minimum-config-example","title":"A minimum config example","text":"<p>Note the below ingress_values.yaml is an example of how our cluster configure the ingress-nginx controller. </p> <pre><code>controller:\n  watchIngressWithoutClass: true\n  allowSnippetAnnotations: false\n  # for installing nginx on the specific worker\n  # all workers that have label public:\"true\" will have a replicas of the nginx \n  nodeSelector:\n    public: \"true\"\n  config:\n    error-log-level: \"info\"\n    ignore-invalid-headers: \"false\"\n    proxy-request-buffering: \"off\"\n    proxy-body-size: \"0\"\n    large-client-header-buffers: \"4 16k\"\n\n  hostNetwork: true\n  extraEnvs:\n    - name: MY_POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n  kind: DaemonSet\n  service:\n    enabled: true\n    type: ClusterIP\n  ingressClassResource:\n    name: nginx\n    enabled: true\n    default: true\n    controllerValue: \"k8s.io/ingress-nginx\"\n\nrbac:\n  create: true\npodSecurityPolicy:\n  enabled: false\n</code></pre> <p>With the above configuration, you can have a minimum running ingress controller</p> <pre><code># we deploy the ingress service with above\nhelm install ingress-nginx ingress-nginx/ingress-nginx -f ingress_values.yaml -n ingress-nginx \n\n# \nkubectl get all -n ingress-nginx\n</code></pre> <p>After the pod of ingress are created, you can try to send request to the worker where nginx is deployed.</p> <pre><code># If the ip address of the worker is 10.0.2.8, you can try below command\ncurl 10.0.2.8\n\n# if you see below output, it means ingress-nginx is running and answering request\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>You can try to deploy the mario app, and check the certificate. You will notice, ingress will assign a <code>fake certificat</code>.</p> <p>We need to replace this certificate by our valid certificate.</p>"},{"location":"Onyxia/02_1.Setup_ingress/#add-a-default-certificate-for-all-services","title":"Add a default certificate for all services","text":"<p>Create a secret to host the certificate and private key. In this tutorial, we name the secret as  <code>casd-wildcard-certificate</code>, you can use the below command</p> <pre><code># general form\nkubectl create secret tls &lt;secret-name&gt; --namespace &lt;namespace-name&gt; --key=pathTo/ingress-tls.key --cert=pathTo/ingress-tls.crt -o yaml\n\n# example\nkubectl create secret tls casd-wildcard-certificate --key=wildcard-casd.key --cert=wildcard-casd.crt -o yaml -n ingress-nginx \n\n# view the content of the secret, the certificate and private value is in base64, you need to decode it to view the\n# value. No encryption at all, so we need to pay attention on who can view this secret.\nkubectl get secret casd-wildcard-certificate -o jsonpath='{.data}' -n ingress-nginx \n\n# you can also edit the value directly\nkubectl edit secret casd-wildcard-certificate -n ingress-nginx \n</code></pre> <p>To tell the ingress to use the given certificate, you need to use extraArgs.default-ssl-certificate config. Below is a full example. Then you need to update the ingress controller with new configuration</p> <pre><code>controller:\n  watchIngressWithoutClass: true\n  allowSnippetAnnotations: false\n  # for installing nginx on the specific worker\n  # all workers that have label public:\"true\" will have a replicas of the nginx \n  nodeSelector:\n    public: \"true\"\n  config:\n    error-log-level: \"info\"\n    ignore-invalid-headers: \"false\"\n    proxy-request-buffering: \"off\"\n    proxy-body-size: \"0\"\n    large-client-header-buffers: \"4 16k\"\n  # enable host mode \n  hostNetwork: true\n  extraEnvs:\n    - name: MY_POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n  kind: DaemonSet\n  service:\n    enabled: true\n    type: ClusterIP\n  ingressClassResource:\n    name: nginx\n    enabled: true\n    default: true\n    controllerValue: \"k8s.io/ingress-nginx\"\n\n    # Parameters is a link to a custom resource containing additional\n    # configuration for the controller. This is optional if the controller\n    # does not require extra parameters.\n    parameters: {}\n  # add a default certificate for all ingress\n  # no need to use  `- secretName: casd-test-tls-secret` in ingress.yaml\n  # to specify a custom certificate\n  # the default certificate should be a wildcard which covers your domain\n  extraArgs:\n    default-ssl-certificate: \"ingress-nginx/casd-wildcard-certificate\"\nrbac:\n  create: true\npodSecurityPolicy:\n  enabled: false\n</code></pre>"},{"location":"Onyxia/02_1.Setup_ingress/#update-existing-ingress-nginx-deployment","title":"Update existing ingress-nginx deployment","text":"<p>The best way to update a deployment (deployed via helm chart) is to modify the <code>values.yaml</code>. Then call the below command</p> <pre><code># general form\nhelm upgrade &lt;deployment-name&gt; &lt;chart-name&gt; -f &lt;config-file&gt; -n &lt;namespace&gt;\n\n# example\nhelm upgrade ingress-nginx ingress-nginx/ingress-nginx -f ingress_values.yaml -n ingress-nginx\n\n# to delete \nhelm delete ingress-nginx\n</code></pre>"},{"location":"Onyxia/03.Install_onyxia/","title":"3 Install Onyxia","text":"<p>Once we have installed the k8s cluster with reverse proxy, we can start to intall Onyxia</p>"},{"location":"Onyxia/03.Install_onyxia/#31-what-is-onyxia","title":"3.1 What is Onyxia?","text":"<p>Onyxia is a web application that provides a catalog of service that help users to run supported services on a k8s cluster. For now the catalog contains maily data oriented services (analytics, data viz, machine learning, etc.). You can find the official git repo of Onyxia here.</p> <p>Onyxia contains two main components:</p> <ul> <li>onyxia-api, in charge of the API of Onyxia (wrapper for k8s api for helm install).</li> <li>onyxia-web, in charge of the UI of Onyxia.</li> </ul>"},{"location":"Onyxia/03.Install_onyxia/#32-why-onyxia","title":"3.2 Why Onyxia?","text":"<p>The services in Onyxia catalog can be deployed into any k8s cluster even without Onyxia. But configuring and deploying these service via command line tools is not very user friendly for data professionial such as analyste and scientist. Onyxia provides a user friendly web interface which abstract the complexity of managing services in Kubernetes cluster. It can allow data scientists to focus on data analyzing task other than k8s cluster exploitation.</p>"},{"location":"Onyxia/03.Install_onyxia/#33-what-service-onyxia-provides","title":"3.3 What service Onyxia provides?","text":"<ul> <li>Quick and easy deployment of all service in catalog (e.g. jupyter, r-studio, mlflow, etc.). Api calls on helm install.</li> <li>All course available in the formation page.</li> <li>Flexible resource configuration on all services (e.g. CPU, GPU, RAM, and disk (ephemeral or PVCs)).</li> <li>Possiblity to add initialization scripts for the deployments;</li> <li>Possibility to create your own service catalog.</li> <li>Flexible user authentication system via Keycloak that can use various backend (e.g. AD, openldap, rdbms, etc.), and provids SSO OpenID Connect;</li> <li>MinIO for S3 object storage;</li> <li>Vault for secret management.</li> </ul>"},{"location":"Onyxia/03.Install_onyxia/#34-requirements","title":"3.4 Requirements","text":"<p>Onyxia runs inside a k8s cluster as other services (deployed via a helm chart). So you can easily deploy an instance of Onyxia via <code>helm install</code>. Before diving into the deployment of Onyxia, several requirements must be met. These requirements are detailed in the following sections of the Onyxia installation.</p> <ul> <li>A domain name must be registered in the form of a wildcard DNS. This is necessary to allow users to access the deployed services on the k8s cluster via proxy/reverse-proxy (ingress-nginx).</li> <li>Keycloak must be installed and configured with the proper settings to store users in a PostgreSQL database, and to use <code>assumeRoleWithWebIdentity</code> allowing for single sign-on (SSO) with Onyxia, MinIO, and Vault.</li> <li>MinIO must be installed and configured with the correct policy to allow Onyxia to create buckets and objects. (Optional)</li> <li>Vault must be installed and configured with the correct policy to allow Onyxia to view, access, and manage secrets. (Optional)</li> </ul>"},{"location":"Onyxia/03.Install_onyxia/#35-install-onyxia","title":"3.5 Install onyxia","text":"<p>Below command will install an instance of onyxia with the minimun configuration basic.yaml</p> <pre><code>helm repo add inseefrlab https://inseefrlab.github.io/helm-charts\n# the url datalab.casd.fake will be the domaine name that allow us to connect to onyxia\nhelm install onyxia inseefrlab/onyxia -f basic.yaml\n</code></pre>"},{"location":"Onyxia/03.Install_onyxia/#36-access-onyxia-web-ui","title":"3.6 Access Onyxia web UI","text":"<p>Note all the network traffic to access services which are <code>exposed by the ingress in k8s cluster</code> are managed by the <code>nginx proxy/reverse-proxy</code>. So you need to send all your query to the node that host the worker which runs the nginx service. Because we choose  </p> <p>For test purpose, if you don't have a dns server, you can change your <code>/etc/hosts</code> to fake it. The ip address must be one of the worker which has ingress running.</p> <p>Add below line to /etc/hosts</p> <pre><code>10.50.5.59   datalab.casd.local\n</code></pre> <p>To test it you can run</p> <pre><code>curl datalab.casd.fake\n</code></pre> <p>This workaround is not recommended for production, because it does not support wildcard. It means for the newly created services that is not in the /etc/hosts are not accesible.</p> <p>Check Chapiter 4 for installing a wildcard dns server.</p>"},{"location":"Onyxia/03.Install_onyxia/#37-enable-oidc-in-onyxia","title":"3.7 Enable OIDC in onyxia","text":"<p>You need to have a OIDC servcie up and running. If not, please follow the 06.Setup_oidc.md to install a keycloak instance. After that you need to add new configuration into the helm deployment.</p> <p>You can find a full example in oidc.yaml</p> <pre><code>ingress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  hosts:\n    - host: datalab.casd.local\nui:\n  image:\n    name: inseefrlab/onyxia-web\n  env:\n    KEYCLOAK_REALM: casd-onyxia\n    KEYCLOAK_CLIENT_ID: onyxia-client\n    KEYCLOAK_URL: https://keycloak.datalab.casd.local/auth\napi:\n  env:\n    keycloak.realm: casd-onyxia\n    keycloak.disable-trust-manager: \"true\"\n    keycloak.auth-server-url: https://keycloak.datalab.casd.local/auth\n    authentication.mode: \"openidconnect\"\n    springdoc.swagger-ui.oauth.clientId: onyxia-client\n</code></pre> <p>Then run below command to apply the new configuration</p> <pre><code>helm upgrade onyxia inseefrlab/onyxia -f oidc.yaml\n</code></pre> <p>This will set the onyxia-api to connect to the keycloak server for authentication. You have to check two points 1. If your certificate is auto signed, you need to add keycloak.disable-trust-manager: \"true\" to ask onyxia-api disable certificat check 2. Make sure the onyxia-api is able to connect to (https://keycloak.datalab.casd.local/auth). DNS must be setup correctlly.</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/","title":"4. Setup wildcard dns resolution","text":"<p>A k8s cluster uses proxy/reverse-proxy (e.g. ingress-nginx) to expose the hosted service with an url. Thus having a dns server that can resolve the specific domain name and send query to the nodes that host the proxy/reverse-proxy is essentail for a k8s cluster.</p> <p>For k8s in public cloud, you can use the dns server offered by the cloud. But in our case, we need to install a dns server by ourselves.</p> <p>For now, we choose DNSMasq as our local DNS server for local name resolution</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#41-understanding-the-dns-name-reolution-process","title":"4.1 Understanding the dns name reolution Process","text":"<p>When you type an URL in your web browser, the web browser will first use the DNS protocol to contact the DNS server that has been configured in your web brower (or PC).</p> <p>The DNS server will respond to the resolution request with the IP address of the server that host the website or return a not found message.</p> <p>Once the browser has the IP address of the website it can connect to it. If the DNS process fails you get a server not found displayed in the browser.</p> <p>This process is the same for all applications (e.g. email, Skype etc) that use <code>URL</code>. They all rely on DNS working in the background to map URL to an IP address.</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#42-tools-for-url-testing","title":"4.2 Tools for url testing","text":"<p>For linux, the two popular tool for url testing is nslookup and dig.</p> <p>You can find a complete tutorial of dig here</p> <pre><code>nslookup &lt;url&gt; &lt;dns-server-IP&gt;\n\ndig &lt;url&gt; @&lt;dns-server-IP&gt;\n\n# example \nnslookup google.fr 8.8.8.8\ndig google.fr @8.8.8.8\n</code></pre> <p>If you don't specify the dns-server-ip, they will use the dns server defined in <code>/etc/resolv.conf</code>. Note you way find multi dns server in this file. The server on top will be used first, so order matters in this file.</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#43-install-dnsmasq","title":"4.3 Install DNSMasq","text":""},{"location":"Onyxia/04.Setup_wildcard_dns/#disable-systemd-resolve","title":"Disable systemd-resolve","text":"<p>For recent release of debian and ubuntu, it may already have a service <code>systemd-resolve</code> that binds to port 53. You need to disable it, otherwise it will cause conflict with <code>DNSMasq</code>.</p> <pre><code># check status\nsudo systemctl status systemd-resolved\n\n# remove it from boot start service list\nsudo systemctl disable systemd-resolved\n\n# stop the service\nsudo systemctl stop systemd-resolved\n\n# for some ubuntu version, you need to check also the /etc/resolv.conf\nls -lh /etc/resolv.conf \nlrwxrwxrwx 1 root root 39 Aug  8 15:52 /etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf\n\n# if you see there is a symlink, you need to remove the link\nsudo unlink /etc/resolv.conf\n\n# then create a new, 8.8.8.8 is the default IP of the dns server of google\necho nameserver 8.8.8.8 | sudo tee /etc/resolv.conf\n</code></pre>"},{"location":"Onyxia/04.Setup_wildcard_dns/#install-and-configure-dnsmasq-package","title":"Install and configure dnsmasq package","text":"<p>**Suppose your dnsmasq will be installed on server 10.50.5.55 ** Dnsmasq is available on the apt repository, easy installation can be done by running:</p> <pre><code>sudo apt update\n\nsudo apt install dnsmasq\n</code></pre> <p>The main configuration file for Dnsmasq is /etc/dnsmasq.conf. Configure Dnsmasq by modifying this file.</p> <p>Below is an example of minimun setup of a dns server</p> <pre><code># Setup the specific port (the standard DNS port is 53). Setting this to zero completely \n# disables DNS function, leaving only DHCP and/or TFTP.\n\nport=53\n\n# Enable checks on the income query domain name, if the name is not valid\n# Never forward the unvalid names (without a dot or domain part)\n\ndomain-needed\n\n# Never forward addresses in the non-routed address spaces.\nbogus-priv\n\n# Setup a dns server resolver list file, if not set, the default path /etc/resolv.conf will be used, \n# dnsmasq will send queries to any of the upstream servers it knows about and tries to favour servers to # are known to be up. \n\nresolv-file=/etc/dnsmasq-dns.conf\n\n# This forces dnsmasq to try each query with each server strictly in the order they appear in the file\n\nstrict-order\n\n# add a mapping file of host url and ip address, the default one is /etc/hosts\n# note this will not support wildcard. It means if a 10.0.2.10 casd.fake is set. The dnsmasq \n# will not resolv toto.casd.fake, it can only resolv casd.fake   \naddn-hosts=/etc/dnsmasq-hosts.conf\n\n# Set this (and domain: see below) if you want to have a domain\n# automatically added to simple names in a hosts-file.\nexpand-hosts\n\n# Set the domain for dnsmasq. this is optional, but if it is set, it\n# does the following things.\n# 1) Allows DHCP hosts to have fully qualified domain names, as long\n#     as the domain part matches this setting.\n# 2) Sets the \"domain\" DHCP option thereby potentially setting the\n#    domain of all systems configured by DHCP\n# 3) Provides the domain part for \"expand-hosts\"\n\ndomain=casd.test\n\n\n# wild card rules, it means all url like *.casd.local will be translate to 10.50.5.59\naddress=/casd.local/10.50.5.59\n\n\n# enable dns log, the default log path is /var/log/messages\nlog-queries\n</code></pre> <p>Because we don't use the defautl resolv and hosts config file, so we need to create our own.</p> <p>Below is an example of dnsmasq-dns.conf (=/etc/resolv.conf)</p> <pre><code># note the order is important\nnameserver 8.8.8.8\n# the ip of the dnsmasq host\nnameserver 10.50.5.55\n</code></pre> <p>Below is an example of dnsmasq-hosts.conf (=/etc/hosts)</p> <pre><code># note below rules are not wildcard\n127.0.0.1 localhost\n10.50.5.59 datalab.casd.test\n</code></pre> <p>You need to restart the service</p> <pre><code>sudo systemctl restart dnsmasq \n\n# you can also add it in start list\nsudo systemctl enable dnsmasq  \n</code></pre>"},{"location":"Onyxia/04.Setup_wildcard_dns/#test-the-wildcard-resolution","title":"Test the wildcard resolution","text":"<p>If you are testing the dns on another server, you need to setup dns address of the server to 10.50.5.55(the server that dnsmasq is running)</p> <pre><code># install dig\nsudo apt install dnsutils\n\n# test if your dns is setup correctlly\ndig toto.datalab.casd.local\n\n# You should see below output\n# &lt;&lt;&gt;&gt; DiG 9.16.33-Debian &lt;&lt;&gt;&gt; toto.datalab.casd.local\n# ;; global options: +cmd\n# ;; Got answer:\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n;; QUESTION SECTION:\n;toto.datalab.casd.local.       IN      A\n\n;; ANSWER SECTION:\ntoto.datalab.casd.local. 0      IN      A       10.50.5.59\n\n;; Query time: 0 msec\n;; SERVER: 10.50.5.58#53(10.50.5.58)\n;; WHEN: Tue Sep 27 10:37:18 CEST 2022\n;; MSG SIZE  rcvd: 68\n\n\n# if not you can specify the dns server address\ndig toto.datalab.casd.local @10.50.5.55\n</code></pre>"},{"location":"Onyxia/04.Setup_wildcard_dns/#config-client-server-to-use-the-dns-server","title":"Config client server to use the dns server","text":"<p>In debian/ubuntu, there are two ways to specify which dns server the network device will use - /etc/resolv.conf - /etc/network/interfaces</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#resolvconf","title":"resolv.conf","text":"<p>Below is an example of resolv.conf</p> <pre><code>search casd.local\nnameserver 10.50.5.58\nnameserver 8.8.8.8\n</code></pre> <p>The order is important here, the search will be in order.</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#interfaces","title":"interfaces","text":"<p>Below is an example.</p> <pre><code>iface eth0 inet static\n    address 10.50.5.103/24\n    gateway 10.50.0.1\n    dns-nameservers 10.50.5.58 8.8.8.8\n</code></pre>"},{"location":"Onyxia/04.Setup_wildcard_dns/#setup-k8s-cluster-dns","title":"Setup k8s cluster dns","text":"<p>The official doc for dns debugging</p> <p>There are two kinds of services in k8s cluster to manage dns resolution. </p> <ul> <li>coredns (official doc): This pod runs a k8s cluster dns</li> <li>nodelocaldns (official doc): This pod runs a dns caching agent on each node of the k8s cluster to reduce dns resolution time. It will query coredns (or kube-dns) service for cache misses of cluster hostnames periodically.  </li> </ul> <p>Note <code>coredns</code> is the default dns server to replace the the old <code>kube-dns</code>.</p> <p>The coredns will use the host dns setting (/etc/resolv.conf) as its dns setting.</p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#solution-1-modify-host-etcresolvconf","title":"Solution 1: Modify host /etc/resolv.conf","text":"<p>After modifying the host <code>/etc/resolv.conf</code>, you need to restart all pods of <code>coredns</code> and <code>nodelocaldns</code></p>"},{"location":"Onyxia/04.Setup_wildcard_dns/#solution-2-modify-kubelet-config","title":"Solution 2: Modify kubelet config","text":"<p>In each node, kubelete also has a <code>config file(/var/lib/kubelet/config.yaml)</code> that allows you to configure cluster dns.</p> <p>You can find below yaml as an example. In it we set <code>169.254.25.10</code> as an example</p> <pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/ssl/ca.crt\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 0s\n    cacheUnauthorizedTTL: 0s\ncgroupDriver: systemd\nclusterDNS:\n- 169.254.25.10\nclusterDomain: cluster.local\ncpuManagerReconcilePeriod: 0s\nevictionPressureTransitionPeriod: 0s\nfileCheckFrequency: 0s\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 0s\nimageMinimumGCAge: 0s\nkind: KubeletConfiguration\nlogging:\n  flushFrequency: 0\n  options:\n    json:\n      infoBufferSize: \"0\"\n  verbosity: 0\nmemorySwap: {}\nnodeStatusReportFrequency: 0s\nnodeStatusUpdateFrequency: 0s\nrotateCertificates: true\nruntimeRequestTimeout: 0s\nshutdownGracePeriod: 0s\nshutdownGracePeriodCriticalPods: 0s\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 0s\nsyncFrequency: 0s\nvolumeStatsAggPeriod: 0s\n</code></pre> <p>After modifying the <code>/var/lib/kubelet/config.yaml</code>, you need to restart all pods of <code>coredns</code> and <code>nodelocaldns</code></p>"},{"location":"Onyxia/05.Setup_block_storage/","title":"5. Storage in k8s (Storage class with CephFS)","text":"<p>The official doc of k8s is here. In this documentation, we only covers: - Storage class: Allow PV to use physique device to allocate volume dynamically - Persistent Volume (PV): PV is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using <code>Storage Classes</code>. Note a PV is not directly linked to a pod. A Pod will request a <code>PVC(Persistent volume claim)</code> which will consume a PV. - Persistent Volume Claim (PVC): PVC is a request for storage by a user (Pod). PVCs consume PV resources. PVCs can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).</p> <p>This step is essential for Onyxia to work properly. Most of the services in Onyxia require storage (<code>persistent volume</code>). They are all provisioned dynamically by using <code>StorageClass</code>. As a result we need to create a storage class in the k8s cluster.</p>"},{"location":"Onyxia/05.Setup_block_storage/#cephfs-installation","title":"CephFs Installation","text":"<p>You need to follow this doc to install ceph file system. </p>"},{"location":"Onyxia/06.Setup_oidc/","title":"6 Setup a service OIDC","text":"<p>Onyxia requires a <code>OIDC serivce</code> for the user authentication. In this tutorial, we will install a <code>keycloak</code> as onyxia OIDC service</p>"},{"location":"Onyxia/06.Setup_oidc/#61-introduction","title":"6.1 Introduction","text":"<p>We don't recommend to use the embedded postgresql deployment of this helm chart. It's hard to configure it. So we install a postgresql server by using the bitnami chart. You can find the complete doc here. After installing PostgreSQL, we can start to install [Keycloak]. Here we use the helm chart of codecentric. The detailed configuration can be found in keycloak.yaml. This configuration file will deploy an instance of keycloak with the following features:</p> <ul> <li>Version 19.0.1-legacy;</li> <li>Ingress on <code>https://auth.casd.local</code>;</li> <li>Connection to PostgreSQL database;</li> <li>adding local CA certificate;</li> <li>Custom realm import with custom clients for Onyxia, MinIO, and Vault.</li> </ul>"},{"location":"Onyxia/06.Setup_oidc/#62-setting-secrets","title":"6.2 Setting Secrets","text":"<p>We need to create five secrets this time:</p> <ul> <li><code>keycloak-ca</code>: the CA root certificate of the cluster;</li> <li><code>wildcard</code>: TLS certificate for the Keycloak endpoint, should be the same as the nginx-ingress one;</li> <li><code>postgresql-creds</code>: PostgreSQL credentials to access the database;</li> <li><code>keycloak-creds</code>: Keycloak credentials to log in to the console;</li> </ul> <p>These secrets are registered under the <code>keycloak</code> namespace.</p> <pre><code># create secret for root ca\nkubectl create secret generic casd-ca \\\n    --namespace keycloak \\\n    --from-file=ca.crt=ca.pem\n\n# create secret for wildcard\nkubectl create secret tls casd-wildcard-tls-secret \\\n    --namespace keycloak \\\n    --key wildcard.key \\\n    --cert wildcard.crt\n\n# create secret for postgres connection\n# This can be omitted if you already have a secret when you create the postgresql server\nkubectl create secret generic postgresql-creds \\\n    --namespace keycloak \\\n    --from-literal=user=keycloak \\\n    --from-literal=password=keycloak-postgres\n\n# create secret for keycloak admin account\nkubectl create secret generic keycloak-admin-creds \\\n  --namespace keycloak \\\n  --from-literal=user=admin \\\n  --from-literal=password=YVqs7p4bJaim3rQ2FSI8\n</code></pre> <p>The root CA certifcate and wildcard may not be in the same namespace of the keycloak. So you may need to sync/copy them from other namespace. You can use this doc as a guid. </p>"},{"location":"Onyxia/06.Setup_oidc/#63-keycloak-installation","title":"6.3 Keycloak Installation","text":"<p>We use the codecentric Helm chart to deploy a keycloak instance, with the provided configuration in keycloak.yaml. Below are the commands:</p> <pre><code># create a namespace for keycloak\nkubectl create namespace keycloak\n\n# add repo\nhelm repo add codecentric https://codecentric.github.io/helm-charts\n\n# install chart\nhelm install keycloak codecentric/keycloak -n keycloak --values keycloak.yaml\n\n# update chart\nhelm upgrade keycloak codecentric/keycloak -n keycloak --values keycloak.yaml\n</code></pre> <p>This installation makes the Keycloak console accessible through <code>https://auth.casd.local</code>.</p>"},{"location":"Onyxia/06.Setup_oidc/#64-configuration-explanation","title":"6.4 Configuration explanation","text":"<p>You can find the official doc here.</p> <p>The default values.yaml is a good start as a base config. In our case, we rename it as <code>keycloak.yaml</code> file.</p>"},{"location":"Onyxia/06.Setup_oidc/#641-resources-and-images","title":"6.4.1 Resources and images","text":"<p>Keycloak's image is version fixed to <code>19.0.1-legacy</code>. The default value is the version of the chart. Its resources are also limited to not overflow the resources of the cluster.</p> <pre><code>image:\n  tag: \"19.0.1-legacy\"\n\nresources:\n  requests:\n    cpu: \"500m\"\n    memory: \"1024Mi\"\n  limits:\n    cpu: \"800m\"\n    memory: \"2048Mi\"\n</code></pre>"},{"location":"Onyxia/06.Setup_oidc/#642-mount-secrets","title":"6.4.2 Mount secrets","text":"<p>By using <code>extraVolumes</code> and <code>extraVolumeMounts</code>, we can mount the secrets to the Keycloak deployment. Once they're mounted, we add them as environment variables to access the console and the PostgreSQL database.</p> <pre><code># Mounting the volumes\nextraVolumeMounts: |\n  - name: certs\n    mountPath: /etc/x509/https\n    readOnly: true\n  - name: db-creds\n    mountPath: /secrets/db-creds\n    readOnly: true\n  - name: kc-creds\n    mountPath: /secrets/kc-creds\n    readOnly: true\n\n# Extra volumes for the secrets\nextraVolumes: |\n  - name: certs\n    secret:\n      secretName: casd-ca\n  - name: db-creds\n    secret:\n      secretName: postgresql-creds\n  - name: kc-creds\n    secret:\n      secretName: keycloak-admin-creds\n</code></pre>"},{"location":"Onyxia/06.Setup_oidc/#643-setup-env-var","title":"6.4.3 Setup env var","text":"<p>To set up high availability with Keycloak, we decided to enable autoscaling which scales the number of pods relatively to the traffic. <code>KUBE_PING</code> is used to discover the other Keycloak pods, no matter their number.</p> <pre><code>extraEnv: |\n# setup env var for network\n  - name : KEYCLOAK_HTTP_PORT \n    value : \"80\"\n  - name: KEYCLOAK_HTTPS_PORT\n    value: \"443\"\n  - name : KEYCLOAK_HOSTNAME\n    value : auth.casd.local\n  - name: PROXY_ADDRESS_FORWARDING\n    value: \"true\"\n  - name: JGROUPS_DISCOVERY_PROTOCOL\n    value: kubernetes.KUBE_PING\n  - name: KUBERNETES_NAMESPACE\n    valueFrom:\n      fieldRef:\n        apiVersion: v1\n        fieldPath: metadata.namespace\n  - name: CACHE_OWNERS_COUNT\n    value: \"2\"\n  - name: CACHE_OWNERS_AUTH_SESSIONS_COUNT\n    value: \"2\"\n# Set the env var for secrets\n  - name: KEYCLOAK_USER_FILE\n    value: /secrets/kc-creds/user\n  - name: KEYCLOAK_PASSWORD_FILE\n    value: /secrets/kc-creds/password\n  - name: X509_CA_BUNDLE\n    value: /etc/x509/https/ca.crt\n  - name: DB_VENDOR\n    value: postgres\n  - name: DB_ADDR\n    value: keycloak-postgres-postgresql\n  - name: DB_PORT\n    value: \"5432\"\n  - name: DB_DATABASE\n    value: keycloak\n  - name: DB_USER_FILE\n    value: /secrets/db-creds/user\n  - name: DB_PASSWORD_FILE\n    value: /secrets/db-creds/password\n</code></pre>"},{"location":"Onyxia/06.Setup_oidc/#643-ingress","title":"6.4.3 Ingress","text":"<p>To expose the Keycloak console, we need to create an Ingress resource. This is done in the <code>keycloak.yaml</code> file under the <code>ingress</code> section, managed by Helm.</p> <pre><code>ingress:\n  enabled: true\n  ingressClassName: \"nginx\"\n  annotations:\n    ## Resolve HTTP 502 error using ingress-nginx:\n    ## See https://www.ibm.com/support/pages/502-error-ingress-keycloak-response\n    nginx.ingress.kubernetes.io/proxy-buffer-size: 128k\n  rules:\n    - host: \"auth.casd.local\"\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - hosts:\n        - \"auth.casd.local\"\n</code></pre> <p>Don't foget to change the root password</p>"},{"location":"Onyxia/06.Setup_oidc/#65-configure-keycloak-for-onxyia","title":"6.5 Configure keycloak for onxyia","text":"<p>Now you need to configure keycloak to secure onyxia</p>"},{"location":"Onyxia/06.Setup_oidc/#step-1-login-to-the-admin-console","title":"Step 1. Login to the admin console","text":"<p>Go to the Keycloak Admin Console and login with the username and password you created earlier in the <code>values.yaml</code>.</p> <p>The url is the host name in the <code>values.yaml</code>. In our case, it should be https://keycloak.datalab.casd.local/</p>"},{"location":"Onyxia/06.Setup_oidc/#step-2-create-a-realm","title":"Step 2. Create a realm","text":"<p>A realm in Keycloak is the equivalent of a tenant. It allows creating isolated groups of applications and users.  By default there is a single realm in Keycloak called master. This is dedicated to manage Keycloak and should not be  used for your own applications.</p> <p>Let\u2019s create our first realm. Open the Keycloak Admin Console</p> <ol> <li> <p>Hover the mouse over the dropdown in the top-left corner where it says Master, then click on Add realm</p> </li> <li> <p>Fill in the form with the following values:</p> <pre><code>Name: casd-onyxia\n</code></pre> </li> <li> <p>Click Create</p> </li> </ol>"},{"location":"Onyxia/06.Setup_oidc/#step-3-create-a-user","title":"Step 3. Create a user","text":"<p>Initially there are no users in a new realm, so let\u2019s create one: Open the Keycloak Admin Console</p> <ol> <li> <p>Click Users (left-hand menu). Then Click Add user (top-right corner of table)</p> </li> <li> <p>Fill in the form with the following values:</p> <ul> <li> <p>Username: john</p> </li> <li> <p>First Name: john</p> </li> <li> <p>Last Name: doe</p> </li> </ul> </li> <li> <p>Click Save</p> </li> </ol> <p>Onyxia does only allow username with alphanumeric [a-z][0-9] characters. Special characters and space are not allowed</p>"},{"location":"Onyxia/06.Setup_oidc/#step-4-setup-initial-password-for-user","title":"Step 4. Setup initial password for user","text":"<ol> <li> <p>Click Credentials (top of the page)</p> </li> <li> <p>Fill in the Set Password form with a password</p> </li> <li> <p>Click ON next to Temporary to prevent having to update password on first login</p> </li> <li> <p>Click on button <code>set password</code> to confirm.</p> </li> </ol>"},{"location":"Onyxia/06.Setup_oidc/#step-5-login-to-account-console","title":"Step 5. Login to account console","text":"<p>Let\u2019s now try to login to the account console to verify the user is configured correctly.</p> <ol> <li> <p>Open the Keycloak Account Console(https://keycloak.datalab.casd.local/realms/casd-onyxia/account/#/)</p> </li> <li> <p>Login with <code>john</code> and the password you created earlier</p> </li> </ol> <p>You should now be logged-in to the account console where users can manage their accounts.</p>"},{"location":"Onyxia/06.Setup_oidc/#step-6-create-a-client-for-onyxia","title":"Step 6. Create a client for onyxia","text":"<p>To secure onyxia with keycloak, we need to create a specific client in our realm, as shown in below Figure.  </p> <p>A client in Keycloak represents a resource that particular users can access, whether for authenticating a user,  requesting identity information, or validating an access token.</p> <p>Follow below step to register a new client :</p> <ol> <li> <p>Open the Keycloak Admin Console (http://keycloak.datalab.casd.local/admin/master/console/)</p> </li> <li> <p>Click 'Clients'</p> </li> <li> <p>Fill in the form with the following values:</p> <ul> <li> <p>Client ID: onyxia-client</p> </li> <li> <p>Client Protocol: openid-connect</p> </li> <li> <p>Root URL: https://datalab.casd.local</p> </li> <li> <p>Valid redirect URIs : https://datalab.casd.local/, http://datalab.casd.local/</p> </li> <li> <p>Web-origins : https://datalab.casd.local, http://datalab.casd.local</p> </li> </ul> </li> <li> <p>Click Save</p> </li> </ol>"},{"location":"Onyxia/06.Setup_oidc/#clients-access-type","title":"Clients Access Type","text":"<p>The access type of onyxia-client must be public.</p> <p>For other client, you can set the access type to <code>confidential</code>. You can see a more detailed form as shown in below figure.</p> <p></p> <ul> <li> <p>Change Access Type: set its value to confidential.</p> </li> <li> <p>Advance settings: Set access token lifeSpan </p> </li> </ul> <p></p> <ul> <li>Authentication flow overrides: set <code>Direct Grant Flow</code> value to direct grant.</li> </ul> <p></p> <ul> <li>update the client's credentials: Use Client Id and Secret for <code>Client Authenticator field</code>.</li> </ul> <p></p>"},{"location":"Onyxia/06.Setup_oidc/#step-7-test-the-created-client","title":"Step 7. Test the created client","text":"<p>Note there is modification for the endpoint that generate the JWT token. </p> <pre><code># For keycloak &gt;= 17.0. The new endpoint is:\nhttps://$HOSTNAME/realms/$REALM_NAME/protocol/openid-connect/token\n\n# For keycloak &lt; 17.0. The legacy endpoint is\nhttps://$HOSTNAME/auth/realms/$REALM_NAME/protocol/openid-connect/token\n</code></pre> <p>Now we can test our newly created client through the REST API to simulate a simple login. Base on your keycloak version, the authentication URL could be in below form:</p> <pre><code># Before 17.0\nhttp://localhost:8080/auth/realms/pengfei-test/protocol/openid-connect/token\n\n# Since 17.0\nhttp://localhost:8080/realms/pengfei-test/protocol/openid-connect/token\n</code></pre> <p>Fill out the parameters and set our client_id and client_secret with our username and password:</p> <p>In our case, client_id=pengfei-dv-app client_secret=enifviJDIpbN5230yfcPo7h2zsifTa2z username=pengfei password=toto</p> <pre><code>curl -L -X POST 'http://localhost:8080/auth/realms/pengfei-test/protocol/openid-connect/token' \\\n-H 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'client_id=pengfei-dv-app' \\\n--data-urlencode 'grant_type=password' \\\n--data-urlencode 'client_secret=enifviJDIpbN5230yfcPo7h2zsifTa2z' \\\n--data-urlencode 'scope=openid' \\\n--data-urlencode 'username=pengfei' \\\n--data-urlencode 'password=toto'\n</code></pre> <p>Or you can check some example scripts in <code>../command</code>.</p>"},{"location":"Onyxia/06.Setup_oidc/#66-disable-keycloak-https-check","title":"6.6 Disable keycloak https check","text":"<p>In the <code>master</code> realm, over login tab. Change <code>Require SSL</code> property to none.</p> <p>If you can not access locally to keycloak and it is configured with a database for instance Postgres, then execute the following SQL sentence.</p> <pre><code>update REALM set ssl_required = 'NONE' where id = 'master';\n</code></pre> <p>It is necessary to restart keycloak</p>"},{"location":"Onyxia/06.Setup_oidc/#67-some-key-information","title":"6.7 Some key information","text":"<p>After all the above configuration, you need to note below information:</p> <p>realm id: casd-onyxia client id: onyxia-client keycloak url: https://keycloak.datalab.casd.local/auth</p>"},{"location":"Onyxia/07.Setup_object_storage/","title":"7. Setup Object storage","text":"<p>The Onyxia uses object storage to store data outside a pod (unlike block storage for pod disk attachement).</p> <p>In this doc, we will deploy a MinIO cluster as the object storage.</p>"},{"location":"Onyxia/07.Setup_object_storage/#72-deploy-minio-cluster-via-helm-chart","title":"7.2 Deploy minio cluster via helm chart","text":"<p>In k8s, MinIO is deployed with two layers:</p> <ul> <li>The MinIO Operator, which orchestrates and deploys one or multiple MinIO Tenants;</li> <li>The MinIO Tenant, an isolated instance of object storage.</li> </ul> <p>While the orchestrator provides common resources, the Tenant hosts the actual object storage instance. It also hosts the services for the object storage API and the web console.</p> <p>As the MinIO Operator is fit for Kubernetes deployments and multi-tenancy, we chose to use it. We also chose to install the MinIO Operator through Helm, as it helps centralize the configuration.</p> <p>To sum things up, the MinIO installation process goes as follows:</p> <ul> <li>Deploying the MinIO Operator through Helm;</li> <li>Deploying the MinIO Tenant through Helm.</li> </ul> <p>Disks need to be formatted beforehand. This is done by deploying and using DirectPV.</p>"},{"location":"Onyxia/07.Setup_object_storage/#73-prerequisites","title":"7.3 Prerequisites","text":""},{"location":"Onyxia/07.Setup_object_storage/#731-k8s-version","title":"7.3.1 K8s version","text":"<p>For minio v4.0.0+, the Kubernetes infrastructure and the kubectl CLI tool must have the version of 1.19.0+.</p> <p>You need to have the necessary access to the cluster (cluster admin) to complete below commands</p>"},{"location":"Onyxia/07.Setup_object_storage/#732-k8s-tls-certificate-api","title":"7.3.2 k8s TLS certificate API","text":"<p>The MinIO Operator automatically generates TLS Certificate Signing Requests (CSR) and uses the Kubernetes <code>certificates.k8s.io</code> TLS certificate management API to create signed TLS certificates.</p> <p>The MinIO Operator therefore requires that the Kubernetes kube-controller-manager configuration include the following configuration settings:</p> <ol> <li> <p><code>--cluster-signing-key-file</code> - Specify the PEM-encoded RSA or ECDSA private key used to sign cluster-scoped certificates.</p> </li> <li> <p><code>--cluster-signing-cert-file</code> - Specify the PEM-encoded x.509 Certificate Authority certificate used to issue cluster-scoped certificates.</p> </li> </ol> <p>The Operator cannot complete initialization if the k8s cluster is not configured to respond to a generated CSR.</p> <p>To verify whether the kube-controller-manager has the required settings, use the following command. Replace $CLUSTER-NAME with the name of the Kubernetes cluster:</p> <pre><code>kubectl get pod kube-controller-manager-$CLUSTERNAME -n kube-system -o yaml\n\n# for example controlplane1\nkubectl get pod kube-controller-manager-controlplane1 -n kube-system -o yaml\n</code></pre> <p>The output should contain below two lines:</p> <pre><code>spec:\n containers:\n - command:\n     - kube-controller-manager\n     - ...\n     - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n     - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#733-install-the-minio-kubernetes-plugin","title":"7.3.3 Install the MinIO Kubernetes Plugin","text":"<p>The <code>MinIO Kubernetes Plugin</code> provides a command for initializing the MinIO Operator.</p> <p>Krew is a kubectl plugin manager developed by the <code>Kubernetes SIG CLI</code> group. </p> <p>Use below command to install Krew (must have git and kubectl)</p> <pre><code># Step1 : Install Krew binary\n(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n# Step2 : Add Krew to PATH\nexport PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n\n# Step3 : Verify that Krew's install is successful by updating the plugin index\nkubectl krew update\n</code></pre> <p>For more details, please read the krew installation documentation for installation.</p>"},{"location":"Onyxia/07.Setup_object_storage/#74-formatting-disks-with-directpv","title":"7.4 Formatting disks with DirectPV","text":"<p>A <code>StorageClass</code> needs to be prepared to provision the PVCs generated by the MinIO Operator when deploying a Tenant.</p> <p>DirectPV is a <code>CSI driver</code> developed by the MinIO team. At the most basic level, it is a distributed persistent volume manager, and not a storage system like SAN or NAS. DirectPV is used to discover, format, mount, schedule and monitor hard drives across servers.</p> <p>For more details about directPV, please visit this page</p> <p>Befor you formatting disk, make sure you get the hard drive name right. In Below example,  - <code>sda</code> is the system disk. - <code>sdb</code> is the hard drive formated by ceph cluster - <code>sdc</code> is not used.</p> <p>So you can only format <code>sdc</code> in this worker. If you format other disk, you will destroy other service's data.</p> <pre><code>pliu@worker1:~$ lsblk -f\nNAME   FSTYPE         FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINT\nsda\n\u251c\u2500sda1 vfat           FAT32       B052-39EE                             507.6M     1% /boot/efi\n\u2514\u2500sda2 ext4           1.0         7bbbaa47-535d-4e32-968d-ea078a0dd460   99.6G    14% /\nsdb    ceph_bluestore\nsdc\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#741-installing-directpv","title":"7.4.1 Installing DirectPV","text":"<p>Install directPV. Tolerations and selectors are needed to target the object storage nodes and drives:</p> <pre><code># Step 1: install DirectPV\nkubectl krew install directpv\n\n# Step 2: setup node-selector and tolerations on directpv deployment\nkubectl directpv install --node-selector 'storage-node=true' --tolerations 'storage-node=true:NoSchedule'\n</code></pre> <p>The above command will create a new <code>namespace</code> called direct-csi-min-io. You should see below pods created in this namespace</p> <pre><code>kubectl get pods -n direct-csi-min-io\nNAME                                 READY   STATUS    RESTARTS   AGE\ndirect-csi-min-io-69947d45bb-2cpr4   2/2     Running   0          92s\ndirect-csi-min-io-69947d45bb-8f5sh   2/2     Running   0          92s\ndirect-csi-min-io-69947d45bb-989gb   2/2     Running   0          92s\n</code></pre> <p>Now lets run below command</p> <pre><code>kubectl directpv drives ls --all\n\n# The output\n DRIVE  CAPACITY  ALLOCATED  FILESYSTEM  VOLUMES  NODE  ACCESS-TIER  STATUS\n</code></pre> <p>The output is empty, because we have not labeled the workers with <code>storage-node=true</code></p> <p>Below command will label the worker</p> <pre><code>kubectl label node worker1 storage-node=true\nkubectl label node worker2 storage-node=true\n</code></pre> <p>Now re-run below command:</p> <pre><code>kubectl directpv drives ls\n\n\n DRIVE     CAPACITY  ALLOCATED  FILESYSTEM      VOLUMES  NODE     ACCESS-TIER  STATUS\n /dev/sdb  512 GiB   -          ceph_bluestore  -        worker1  -            Available\n /dev/sdc  512 GiB   -          -               -        worker1  -            Available\n /dev/sdb  512 GiB   -          ceph_bluestore  -        worker2  -            Available\n /dev/sdc  512 GiB   -          -               -        worker2  -            Available\n</code></pre> <p>This time you can see, it detects 4 possible hard drive. As we explained before, <code>/dev/sdb</code> is reserved for ceph cluster. So we can only use <code>/dev/sdc</code></p>"},{"location":"Onyxia/07.Setup_object_storage/#742-formatting-disks","title":"7.4.2 Formatting disks","text":"<p>Select the drives to be formatted and managed by DirectPV. In our case, the following command selects the right disks:</p> <pre><code># format the target disk\nkubectl directpv drives format --drives \"/dev/sdc\"\n\n# get the hard drives list\n kubectl directpv drives ls\n\n# now you can see the status of /dev/sdc is ready. \n DRIVE     CAPACITY  ALLOCATED  FILESYSTEM      VOLUMES  NODE     ACCESS-TIER  STATUS\n /dev/sdb  512 GiB   -          ceph_bluestore  -        worker1  -            Available\n /dev/sdc  512 GiB   -          xfs             -        worker1  -            Ready\n /dev/sdb  512 GiB   -          ceph_bluestore  -        worker2  -            Available\n /dev/sdc  512 GiB   -          xfs             -        worker2  -            Ready\n</code></pre> <p>The hard drives which have status <code>Ready</code> can now be automatically used for PVs and PVCs with MinIO.</p>"},{"location":"Onyxia/07.Setup_object_storage/#75-install-minio-operator","title":"7.5 Install minio operator","text":"<p>The MinIO Operator installs a <code>Custom Resource Document (CRD)</code> to support describing <code>MinIO tenants</code> as a Kubernetes object. See the MinIO Operator CRD Reference for complete documentation on the MinIO CRD.</p> <p>Run below command to install a minio k8s operator.</p> <pre><code># update the plugin list\nkubectl krew updata\n\n# install the minio plugin\nkubectl krew install minio\n\n# check minio plugin version\nkubectl minio version\n\n# try to get all service in name space minio-operator\nkubectl get pods -n minio-operator\nNAME                              READY   STATUS    RESTARTS   AGE\nconsole-c4cd64b8c-zvp2d           1/1     Running   0          16h\nminio-operator-7d59b88ffb-4bsg6   1/1     Running   0          16h\nminio-operator-7d59b88ffb-c9p69   1/1     Running   0          16h\n</code></pre> <p>If you want to custom the configuration, you can edit a <code>values.yaml</code>. The official example can be found here</p> <p>Below <code>operator-values.yaml</code> is a simple configuration example:</p> <pre><code># Selector and tolerations to deploy the Operator console on storage nodes\nnodeSelector:\n  storage-node: \"true\"\ntolerations:\n  - key: \"storage-node\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoSchedule\"\n\n# Exposing the console through an Ingress\nconsole:\n  ingress:\n    enabled: true\n    ingressClassName: \"nginx\"\n    labels: { }\n    annotations:\n      # The following annotation is required to let MinIO communicate with the NGINX Ingress controller\n      # when using external certificates. See Knowledge base: 8dc2998d-5699-4be6-bed0-b2384a87fe9e\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    tls:\n      - hosts:\n          - minio-operator-console.casd.local\n    host: minio-operator-console.casd.local\n    path: /\n    pathType: Prefix\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#751-minio-operator-console","title":"7.5.1 Minio Operator console","text":"<p>Minio operator provides a console (web GUI), you can use below command to temporarily forward traffic from the MinIO Operator Console service to your local machine:</p> <pre><code># this will return an access token which allows you to login to the web page\nkubectl minio proxy\n</code></pre> <p>By default, the operator console does not enable ingress. So if you want to have ingress, you need to enable it with above config.</p> <p>You can follow this tutorial to creat a minio tenant by using minio proxy. </p>"},{"location":"Onyxia/07.Setup_object_storage/#76-install-minio-tenant","title":"7.6 Install minio tenant","text":"<p>The <code>MinIO Tenant</code> is what supports the actual object storage instance. A MinIO Tenant deploys <code>multiple pool pods</code> on <code>storage nodes</code>, as well as the <code>REST API service</code> to contact the object storage and the <code>console service</code> to access its web UI.</p> <p>In the above section, we used the <code>minio proxy</code> to deploy a minio tenant. But <code>GUI</code> configuration is hard to document. So we will use the classic helm chart to deploy a <code>minio tenant</code>.  </p> <p>You can find the official default minio tenant config values.yaml</p> <p>Below command use the MinIO Tenant Helm Chart and a configuration file tenant-values.yaml:</p> <pre><code>helm repo add minio https://operator.min.io/\n\n# install helm chart\nhelm install minio --namespace casd-minio minio/tenant --values tenant-values.yaml\n\n# update helm chart\nhelm upgrade minio minio/tenant -n casd-minio --values tenant-values.yaml \n</code></pre> <p>The MinIO Operator must be installed and running correctlly.</p>"},{"location":"Onyxia/07.Setup_object_storage/#761-tls-ssl-certificate-configuration","title":"7.6.1 TLS SSL certificate configuration","text":"<p>With the above configuration, the helm chart will generate a <code>certificate</code> and <code>private key</code>. They are stored in a <code>secret</code>, which has name such as <code>&lt;tenant-name&gt;-tls</code>, in our case, it will be <code>casd-minio-tls</code>.</p> <p>The content of the secert looks like below</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n name: casd-minio-tls\n namespace: casd-minio\ndata:\n public.crt: LS0tLS1CRUdJTiBD...\n private.key: LS0tLS1CRUdJTiB...\n</code></pre> <p>This secret will be mounted as a volume callded <code>casd-minio-tls</code> on <code>/tmp/certs</code> on each pod(e.g. casd-minio-pool-0-0) of the minio tenant. </p> <p>The default minio setting is to put certs under <code>${user-home-who-run-minio}/.minio/certs</code>. But the above helm chart will mount it under <code>/tmp/certs</code></p> <p>The content of certs folder</p> <pre><code>ls certs\n\n# The public.crt is the certificate from the secert\n# The private.key is the private key from the secert\n# CAs folder host the root CA certificate which signes the certifcate\n# By default it assume it's a autosigned, so it just put the public.crt in CAs.\nCAs  private.key  public.crt\n</code></pre> <p>The official doc says that we could use</p>"},{"location":"Onyxia/07.Setup_object_storage/#the-work-around","title":"The work around","text":"<p>If you want to use a custom certificate, you need to overwrite the secret casd-minio. You need to replace the public.crt and private.key with your own certificate and private key in the <code>casd-minio-tls</code> secret. </p>"},{"location":"Onyxia/07.Setup_object_storage/#762-sso-keycloak-configuration","title":"7.6.2 SSO (Keycloak) configuration","text":"<p>Minio support other OIDC sources, in this tutorial we only focus on the <code>Keycloak</code> configuration. For minio to use keycloak, we need to do two things:</p> <ul> <li>Create a minio auth client in Keycloak (with policy claim)</li> <li>Configure minio to use the Keycloak auth client for authentication</li> </ul>"},{"location":"Onyxia/07.Setup_object_storage/#7621-create-minio-auth-client-in-keycloak","title":"7.6.2.1 Create minio auth client in Keycloak","text":"<p>The official doc of Keycloak configuration for minio is not very useful for us, because we will use onyxia to connect to minio. </p>"},{"location":"Onyxia/07.Setup_object_storage/#step-1-create-a-new-auth-client-for-minio","title":"Step 1 Create a new auth client for minio","text":"<ol> <li>To create a new auth client, you need to log in to keycloak admin console</li> <li>Choose a realm (it's better to put it in the same realm of onyxia auth client)</li> <li>Click on <code>Clients</code> (on the left side panel), then click on <code>create</code> (right side panel, on top of client list)</li> <li> <p>In <code>Client ID</code>, you put the name of this auth client. In our case we use <code>minio</code>. In <code>Root URL</code>, you put the url of the service which will use this auth client. In our case, it's <code>https://minio-console.casd.local</code>. Then click on <code>save</code>.</p> </li> <li> <p>Now, you should see a larger form to fill. You need to pay attentions to </p> </li> <li>Root URL: <code>https://minio-console.casd.local</code></li> <li>Valid Redirect URIs: <code>http://minio-console.casd.local/*</code> | <code>https://minio-console.casd.local/*</code></li> <li>Web-origins: <code>http://minio-console.casd.local</code>   | <code>https://minio-console.casd.local</code>   Don't forget to change the <code>Access Type</code> from <code>public</code> to <code>confidential</code>. After this, you should see a new tab <code>Credentials</code>, in it you will find the <code>secret</code> to access this auth client.</li> <li>In <code>advanced settings</code>, you can change the <code>access token lifespan</code>. </li> </ol> <p>Below figure is an example </p>"},{"location":"Onyxia/07.Setup_object_storage/#step-2-create-a-policy-claim","title":"Step 2 Create a policy claim","text":"<p>To make the generated token to contain minio access policy. You need to create a mapper inside the minio's auth client which you just created</p> <ol> <li>To create a new <code>mapper</code>, click on the minio auth client. On the right panel, click on <code>Mappers</code> (on top). Then click on <code>create</code>, it will popup a form which you need to fill.</li> <li>Name: <code>policy</code>;   Mapper Type: <code>Hardcoded claim</code>; Token Claim Name: <code>policy</code>; Claim value: <code>stsonly</code></li> <li><code>Add to ID Token</code> need to be <code>Enabled</code> </li> <li>Click on <code>save</code>. </li> </ol> <p>Below figure is an example </p> <p>Note, this claim requires the minio server contains a policy named <code>stsonly</code>. We will show you how in below section.</p>"},{"location":"Onyxia/07.Setup_object_storage/#7622-configure-minio-to-use-the-keycloak","title":"7.6.2.2 Configure minio to use the Keycloak","text":"<p>To enable oidc auth in minio, you need to modify below env var of the minio server.</p> <pre><code> Tenant:\n  #...\n  #... \n  env:\n     # keycloak url realms name=casd-onyxia\n    - name: MINIO_IDENTITY_OPENID_CONFIG_URL\n      value: \"https://auth.casd.local/auth/realms/casd-onyxia/.well-known/openid-configuration\"\n      # keycloak auth client id for minio\n    - name: MINIO_IDENTITY_OPENID_CLIENT_ID\n      value: \"minio\"\n      # secret of the auth client(if it's confidential)\n    - name: MINIO_IDENTITY_OPENID_CLIENT_SECRET\n      value: \"changeMe\"\n      # Token Claim Name\n    - name: MINIO_IDENTITY_OPENID_CLAIM_NAME\n      value: \"policy\"\n    - name: MINIO_IDENTITY_OPENID_REDIRECT_URI\n      value: \"https://minio-console.casd.local/oauth_callback\"\n    - name: MINIO_IDENTITY_OPENID_SCOPES\n      value: \"openid, profile, email, roles\"\n    - name: MINIO_DOMAIN\n      value: \"minio.casd.local\"\n    - name: MINIO_BROWSER\n      value: \"on\" # to turn-off browser\n      # minio consol url\n    - name: MINIO_BROWSER_REDIRECT_URL\n      value: \"https://minio-console.casd.local\"\n      # minio api url\n    - name: MINIO_SERVER_URL\n      value: \"https://minio.casd.local\"\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#763-details-on-minio-tenant-configuration","title":"7.6.3 Details on minio tenant configuration","text":"<p>In this tutorial, we only show the details on <code>certificate and SSO</code> configuration. For other configuration, you can visit sys_admin/12.Minio_installation_details.md.</p> <p>You can get a baseline of the configuration file by downloading the MinIO Tenant's Helm Chart repository's default configuration file. Name the file <code>tenant-values.yaml</code>:</p> <p>Note that the Datalab cluster provides an already-prepared file for the Helm chart.</p>"},{"location":"Onyxia/07.Setup_object_storage/#77-minio-client","title":"7.7 Minio client","text":""},{"location":"Onyxia/07.Setup_object_storage/#771-the-minio-web-console","title":"7.7.1 The minio web console","text":"<p>If you have ingress configured, you should be able to access the minio web console via the configured url (e.g. https://minio-console.casd.local).</p>"},{"location":"Onyxia/07.Setup_object_storage/#772-the-minio-cli","title":"7.7.2 The minio CLI","text":"<p>Minio also provide a command line client.</p>"},{"location":"Onyxia/07.Setup_object_storage/#install","title":"Install","text":"<p>Download the <code>mc client</code> and install it to a location on your system PATH such as <code>/usr/local/bin</code>.</p> <pre><code># get the minio client bin\nwget https://dl.min.io/client/mc/release/linux-amd64/mc\n\n# grant execution right\nchmod +x mc\n\n# move it to local bin\nsudo mv mc /usr/local/bin/mc\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#set-server-connection-alias","title":"Set server connection alias","text":"<p>Use <code>mc alias set</code> to create a new alias associated to your minio deployment. You can run <code>mc</code> commands against this alias:</p> <pre><code># general form\nmc alias set &lt;alias-name&gt; &lt;cluster-url&gt; &lt;Access-key&gt; &lt;Secret-key&gt;\n\n# below is an example\nmc alias set test https://minio-test.casd.local:9000 minio mypasswd\n\n# check the alias status, this only works if you have admin rights\nmc admin info test\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#use-minio-client","title":"Use minio client","text":"<p>The use of minio client can be divided in two parts. - minio client: the official doc can be found here - minioa admin client: the official doc can be found here</p>"},{"location":"Onyxia/07.Setup_object_storage/#use-minio-admin-client-to-add-access-control-policy","title":"Use minio admin client to add access control policy","text":"<p>Onyxia currently uses a basic <code>userid</code> &lt;&gt; <code>bucketid</code> permissions system. Basically each user has access to the bucket with the id equals to it's <code>userid</code>. Below <code>stsonly.json</code> file is an example of such policy.</p> <p>The policy is stored inside a json file. </p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n     {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n       \"s3:*\"\n      ],\n      \"Resource\": [\n       \"arn:aws:s3:::${jwt:preferred_username}\",\n       \"arn:aws:s3:::${jwt:preferred_username}/*\"\n      ]\n     }\n    ]\n}\n</code></pre> <p>To apply the above policy, you can use below command</p> <pre><code># use mc admin to create a policy, the keycloak claim will use this policy to generate jwt token\n# with appropriate access rights. Below is a general form\nmc admin policy add &lt;cluster-alias-name&gt; &lt;policy-name&gt; &lt;rule-file-name&gt;\n\n# below is an example, note the name of the policy is `stsonly`, the keycloak claim value\n# must have exactly the same value \nmc admin policy add s3 stsonly stsonly.json\n</code></pre> <p>If successful, you should be able to login to the minio console and create the bucket that has the same name as your user id.</p> <p>Onyxia currently supports <code>id_token</code> but not <code>access_token</code>. This parameter is located in the <code>identity_openid</code> object as <code>claim_userinfo</code>.</p> <pre><code>mc admin config get s3 identity_openid\n\n# you should see below output\n# MINIO_IDENTITY_OPENID_CONFIG_URL=https://auth.casd.local/auth/realms/casd-onyxia/.well-known/openid-configuration\n# MINIO_IDENTITY_OPENID_CLIENT_ID=minio\n# MINIO_IDENTITY_OPENID_CLIENT_SECRET=changeMe\n# MINIO_IDENTITY_OPENID_CLAIM_NAME=policy\n# MINIO_IDENTITY_OPENID_REDIRECT_URI=https://minio-console.casd.local/oauth_callback\n# MINIO_IDENTITY_OPENID_SCOPES=openid, profile, email, roles\nidentity_openid enable= display_name= config_url= client_id= client_secret= claim_name=policy claim_userinfo= role_policy= claim_prefix= redirect_uri= redirect_uri_dynamic=off scopes= vendor= keycloak_realm= keycloak_admin_url=\n</code></pre> <p>To disable it if it was enabled:</p> <pre><code>mc admin config set s3 identity_openid claim_userinfo=off\nmc admin service restart s3\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#78-integrate-minio-in-onyxia","title":"7.8 Integrate minIO in onyxia","text":"<p>Onyxia provide an integrated minio console which allows you to upload, download files. To make this console works, we need to set two mappers inside <code>onyxia's keycloak auth client</code>: - Audience: for minio authentication - Policy claim: for minio access control policy</p> <p><code>onyxia-client</code></p> <p>### 7.8.1 Set up an audience</p> <p>To create an audience : 1. click on the <code>onyxia auth client</code>, in our case the name is <code>onyxia-client</code>. On the right panel, click on <code>Mappers</code> (on top). Then click on <code>create</code>, it will popup a form which you need to fill. 2. Name: <code>audience-minio</code>;   Mapper Type: <code>Audience</code>; Included Client Audience: <code>minio</code> 3. <code>Add to ID Token</code> need to be <code>Enabled</code>  4. Click on <code>save</code>.</p> <p>Note if you named your minio auth client other than <code>minio</code>, you should change the <code>Included Client Audience</code> value accordingly.</p> <p>Below figure is an example</p> <p></p>"},{"location":"Onyxia/07.Setup_object_storage/#782-set-up-policy-claim","title":"7.8.2 Set up policy claim","text":"<p>To create a hardcoded claim  1. click on the <code>onyxia auth client</code>, in our case the name is <code>onyxia-client</code>. On the right panel, click on <code>Mappers</code> (on top). Then click on <code>create</code>, it will popup a form which you need to fill. 2. Name: <code>policy</code>;   Mapper Type: <code>Hardcoded claim</code>; Token Claim Name: <code>policy</code>; Claim value: <code>stsonly</code> 3. <code>Add to ID Token</code> need to be <code>Enabled</code>  4. Click on <code>save</code>. </p> <p>Below figure is an exampl</p> <p></p> <ul> <li>You probably want to extend token's duration as the <code>5 minutes</code> default value is not enough for users to use the token inside their services. This can be done either <code>realm wide</code> (realm settings =&gt; tokens =&gt; access_token_lifespan) or <code>per client</code> (settings =&gt; advanced settings).</li> <li>If you intend to use groups or want to work around this bug : https://github.com/InseeFrLab/onyxia-web/issues/263, you may want to add a <code>groups</code> client scope with a <code>Group membership</code> mapper.</li> </ul>"},{"location":"Onyxia/07.Setup_object_storage/#783-link-minio-to-onyxia","title":"7.8.3 Link minio to Onyxia","text":"<p>To link minio server with Onyxia minio consol, we need to modify the Onyxia's UI configuration. The most important thing is to set <code>MINIO_URL: https://minio.casd.local</code>.</p> <p>The complete configuration file <code>minio.yaml</code> can be found here </p> <p>To apply the new configuration, you need to run</p> <pre><code>helm upgrade onyxia inseefrlab/onyxia -f minio.yaml\n</code></pre>"},{"location":"Onyxia/07.Setup_object_storage/#references","title":"References","text":"<ul> <li>MinIO Operator repository Helm charts folder, for both operator and tenant</li> <li>MinIO Tenant Helm chart's README.md</li> <li>MinIO Tenant Helm chart default values file</li> <li>Install mimio on debian</li> </ul>"},{"location":"Onyxia/08.Setup_secret_management/","title":"8. Secret management in Onyxia","text":"<p>Onyxia requires a secret management service to store user's secret inside the cluster. For now <code>HashiCorp Vault</code> is the solution that Onyxia uses.</p>"},{"location":"Onyxia/08.Setup_secret_management/#81-vault-introduction","title":"8.1 Vault introduction","text":"<p>You can find the principal Vault use cases in this page</p>"},{"location":"Onyxia/08.Setup_secret_management/#811-vault-main-functionalities","title":"8.1.1 Vault main functionalities","text":"<p>If you never used <code>Vault</code> before, you can follow this tutorial to play with it.</p>"},{"location":"Onyxia/08.Setup_secret_management/#812-vault-internal-architecture","title":"8.1.2 Vault internal architecture","text":""},{"location":"Onyxia/08.Setup_secret_management/#82-vault-on-k8s","title":"8.2 Vault on k8s","text":"<p>The vault service on k8s has more feature than a vault on traditional server. For example, Vault agent containers can inject secret into <code>Pods</code>. </p> <p>This doc is very good in explaining vault on k8s </p> <p>You can find many vault on k8s tutorial on this page</p>"},{"location":"Onyxia/08.Setup_secret_management/#821-deploy-vault-via-helm-chart","title":"8.2.1 Deploy vault via helm chart","text":"<p>In this doc, we will use the hashicorp vault-helm to deploy a vault on k8s. The default configuration file can be found here</p> <p>This tutorial is a good start, if you are new to vault</p> <pre><code># creat name space\nkubectl create namespace vault\n\n# add hashicorp repo\nhelm repo add hashicorp https://helm.releases.hashicorp.com\n\n# check available chart version\nhelm search repo hashicorp/vault --versions\n\n# use a dry-run to find out default config\nhelm install vault hashicorp/vault --namespace vault --dry-run\n\nhelm install vault hashicorp/vault --namespace vault --values vault-casd.yaml\n</code></pre>"},{"location":"Onyxia/08.Setup_secret_management/#822-tls-certificates","title":"8.2.2 TLS certificates","text":"<p>If a <code>private Certificate Authority (CA)</code> is used, you can pass the path to the CA Cert using the environment variable VAULT_CACERT through the use of the server.extraEnvironmentVars attribute, such as:</p> <pre><code>server:\n  extraEnvironmentVars:\n    VAULT_CACERT: /vault/userconfig/tls-ca/ca.crt\n</code></pre> <p>To create this file and path inside the Vault Pods, you can create a Kubernetes Secret from the contents of the TLS Certificate file and this can be mounted using the server.extraVolumes attribute.</p> <p>The Kubernetes Secret needs to be created before the installation of the Vault Helm chart</p> <p>Below is an example on how to create a such secret</p> <pre><code>kubectl -n vault create secret tls vault-ca-crt --cert ./ca.pem --key ./ca-key.pem\n</code></pre> <p>You can mount </p>"},{"location":"Onyxia/08.Setup_secret_management/#83-vault-init","title":"8.3 Vault init","text":"<p>After all the pods are created, you should be able to access the vault ui via <code>https://vault.casd.local</code>. Then you need to initial the vault with a root key and a number share keys.</p> <p>Vault uses Sharmir's secret sharing algo to share the secret. You will need a majority of the share keys to <code>unseal</code> the vault.</p> <p>Below is an example, I choose 3 share key and 2 as the majority.</p> <pre><code>- root key: hvs.jf0pP8bHd16frMJVImQ65yUF\n- share key1: UYGy5Ke1G2Grew5uPdYzD/rJh7ncB29i6p4XISkcUxNO\n- share key2: Mgvif9S1+7ZvBASMHusgAUabat8l5VlmactMNJd9qogQ\n- share key3: +7qtRVm1brRjuqOc3Nsut69RA8gZ9QF3NVprrvsxlrLp\n</code></pre>"},{"location":"Onyxia/08.Setup_secret_management/#84-integrate-vault-into-onyxia","title":"8.4 Integrate Vault into Onyxia","text":"<p>To integrate <code>vault</code> into Onyxia, we need to  - configure keycloak client for <code>vault</code> - configure <code>vault</code> (e.g. enable JWT,  setup user acl policies) - update <code>onyxia</code> helm deployment config values, and upgrade the helm chart</p>"},{"location":"Onyxia/08.Setup_secret_management/#841-vault-with-keycloak-integration","title":"8.4.1 Vault with keycloak integration","text":""},{"location":"Onyxia/08.Setup_secret_management/#8411-create-a-new-auth-client-for-vault","title":"8.4.1.1 Create a new auth client for vault","text":"<ol> <li>To create a new auth client, you need to login to keycloak admin console</li> <li>Choose a realm (its better to put it in the same realm of onyxia auth client)</li> <li>Click on <code>Clients</code> (on the left side panel), then click on <code>create</code> (right side panel, on top of client list)</li> <li> <p>In <code>Client ID</code>, you put the name of this auth client. In our case we use <code>vault</code>. In <code>Root URL</code>, you put the url of the service which will use this auth client. In our case, it's <code>https://vault.casd.local</code>. Then click on <code>save</code>.</p> </li> <li> <p>Now, you should see a larger form to fill. You need to pay attentions to </p> </li> <li>Root URL: <code>https://vault.casd.local</code></li> <li>Valid Redirect URIs: <code>https://vault.casd.local/*</code></li> <li>Web-origins:  <code>https://vault.casd.local</code>. Keep the <code>Access Type</code> as <code>public</code> </li> <li>In <code>advanced settings</code>, you can change the <code>access token lifespan</code>. </li> </ol> <p>Below figure is an example </p>"},{"location":"Onyxia/08.Setup_secret_management/#8412-create-a-mapper-in-vault-client","title":"8.4.1.2 Create a mapper in vault client","text":"<p>You need to create a mapper inside the <code>vault</code>'s auth client which you just created</p> <ol> <li>To create a new <code>mapper</code>, click on the vault auth client. On the right panel, click on <code>Mappers</code> (on top). Then click on <code>create</code>, it will popup a form which you need to fill.</li> <li>Name: <code>vault</code>;   Mapper Type: <code>Claims parameter Token</code>; </li> <li><code>Add to ID Token</code> need to be <code>Enabled</code> </li> <li>Click on <code>save</code>. </li> </ol> <p>Below figure is an example </p>"},{"location":"Onyxia/08.Setup_secret_management/#8413-create-a-mapper-audience-in-onyxia-client","title":"8.4.1.3 Create a mapper (audience) in onyxia client","text":"<p>To create an audience : 1. click on the <code>onyxia auth client</code>, in our case the name is <code>onyxia-client</code>. On the right panel, click on <code>Mappers</code> (on top). Then click on <code>create</code>, it will popup a form which you need to fill. 2. Name: <code>vault</code>;   Mapper Type: <code>Audience</code>; Included Client Audience: <code>vault</code> 3. <code>Add to ID Token</code> need to be <code>Enabled</code>  4. Click on <code>save</code>.</p> <p>Note if you named your <code>vault</code> auth client other than <code>vault</code>, you should change the <code>Included Client Audience</code> value accordingly.</p> <p>Below figure is an example</p> <p></p>"},{"location":"Onyxia/08.Setup_secret_management/#842-configure-vault","title":"8.4.2 Configure Vault","text":"<p>To complete below command, you will need the <code>vault CLI</code>. You can follow this doc to install a vault CLI.</p>"},{"location":"Onyxia/08.Setup_secret_management/#8421-enable-jwt-support","title":"8.4.2.1 Enable JWT support","text":"<p>First, we need to create a <code>JWT endpoint</code> in Vault, and writing information about <code>Keycloak</code> to the configuration. The default_role=\"onyxia-user\" will be used by <code>Onyxia</code> to connect to Vault.</p> <pre><code># enable jwt auth module\nvault auth enable jwt\n\n# configure the jwt provider, here we use a keycloak instance\n# the @ca.pem will read a local .pem file and pass the content of the file\n# as the argument to oidc_discovery_ca_pem\nvault write auth/jwt/config \\\n    oidc_discovery_url=\"https://auth.casd.local/auth/realms/casd-onyxia\" \\\n    oidc_discovery_ca_pem=@ca.pem\n    default_role=\"onyxia-user\"\n\n# if every works well, you should see below line\nSuccess! Data written to: auth/jwt/config\n</code></pre> <p>This will creat a new auth type, you can get all possible auth type via below command</p> <pre><code>vault auth list\n\n# You should see below output, note the Accessor is important to id an auth type\nPath      Type     Accessor               Description                Version\n----      ----     --------               -----------                -------\njwt/      jwt      auth_jwt_7572e714      n/a                        n/a\ntoken/    token    auth_token_c248f5e8    token based credentials    n/a\n</code></pre>"},{"location":"Onyxia/08.Setup_secret_management/#8422-set-the-acl-policy","title":"8.4.2.2 Set the ACL policy","text":"<ol> <li>Create a acl policy file called <code>onyxia-kv-policy.hcl</code>, and put the following content</li> </ol> <pre><code>path \"onyxia-kv/{{identity.entity.aliases.auth_jwt_7572e714.name}}/*\" {\n  capabilities = [\"create\",\"update\",\"read\",\"delete\",\"list\"]\n}\n\npath \"onyxia-kv/data/{{identity.entity.aliases.auth_jwt_7572e714.name}}/*\" {\n  capabilities = [\"create\",\"update\",\"read\"]\n}\n\npath \"onyxia-kv/metadata/{{identity.entity.aliases.auth_jwt_7572e714.name}}/*\" {\n  capabilities = [\"delete\", \"list\", \"read\"]\n}\n</code></pre> <ol> <li>Upload the policy to vault server</li> </ol> <pre><code>vault policy write onyxia-kv onyxia-kv-policy.hcl\n</code></pre> <ol> <li>Enable the <code>onyxia-kv</code> secret engine</li> </ol> <pre><code>vault secrets enable -path=onyxia-kv kv-v2\n</code></pre> <ol> <li>Add new role <code>onyxia-user</code> to jwt auth type</li> </ol> <p>To creat a new role, we need to specify  - role_type:  - bound_audiences: keycloak auth client name for vault server - user_claim:  - policies: The policy that we created  </p> <p>Below is an example</p> <pre><code>vault write auth/jwt/role/onyxia-user \\\n    role_type=\"jwt\" \\\n    bound_audiences=\"vault\" \\\n    user_claim=\"preferred_username\" \\\n    policies=\"onyxia-kv\"\n\n# if everything works well, you should see below output\nSuccess! Data written to: auth/jwt/role/onyxia-user\n</code></pre>"},{"location":"Onyxia/08.Setup_secret_management/#actives-cors-dans-vault","title":"Actives cors dans vault","text":"<p>The official doc can be found here</p> <pre><code># get current core settings\ncurl --header \"X-Vault-Token: changeMe\" https://vault.casd.local/v1/sys/config/cors\n</code></pre> <p>To update the core setting, we need to use <code>POST</code> on <code>/sys/config/cors</code></p> <p>Below is an example payload</p> <pre><code>{\n    \"allowed_origins\": \"https://datalab.casd.local\",\n    \"allowed_headers\": \"X-Custom-Header\"\n}\n</code></pre> <p>don't forget to use the complete url (https://). The <code>allowed_headers</code> is useful when you have custom headers which you want vault to accept. The standard headers are added by default.  </p> <p>Sample request</p> <pre><code>curl \\\n    --header \"X-Vault-Token: changeMe\" \\\n    --request POST \\\n    --data @cors.json \\\n    https://vault.casd.local/v1/sys/config/cors\n</code></pre> <p>To delete core settings</p> <pre><code>curl \\\n    --header \"X-Vault-Token: ...\" \\\n    --request DELETE \\\n    https://vault.casd.local/v1/sys/config/cors\n</code></pre>"},{"location":"Onyxia/08.Setup_secret_management/#843-update-the-onyxia-helm-chart","title":"8.4.3 Update the onyxia helm chart","text":"<p>The new onyxia helm config file can be found in vault.yaml.</p> <p>You need to add below lines in <code>api.regions.vault</code></p> <pre><code>\"vault\": { \n                \"URL\": \"https://vault.casd.local\",\n                \"kvEngine\": \"onyxia-kv\",\n                \"role\": \"onyxia-user\"\n        },\n</code></pre> <pre><code>helm upgrade onyxia inseefrlab/onyxia -f vault.yaml\n</code></pre>"},{"location":"Onyxia/09.Setup_local_image_registry/","title":"Setup local image registry","text":"<p>As the onyxia instance inside CASD secure bubble does not have internet connextion, so it can't use <code>docker hub</code> to pull the image. So CASD need to have its own local container image registry.</p> <p>After comparing docker registry and harbor, we choose harbor as our solution.</p>"},{"location":"Onyxia/09.Setup_local_image_registry/#harbor-install-and-config","title":"Harbor install and config","text":"<p>To install and config harbor, please read this Harbor_standalone_installation.md</p>"},{"location":"Onyxia/09.Setup_local_image_registry/#configure-k8s-to-use-harbor-as-image-registry","title":"Configure k8s to use harbor as image registry","text":"<p>There are a few challenges to be able to use a local image registry - TLS config: If you are useing a self-signed certificate to secure your local image registry, you need to make your k8s cluster container runtime to accept this certificate. - registry authentication: If your local registry requires authentication, you need to create a secret, then call it whenever you need to pull image from this registry</p>"},{"location":"Onyxia/09.Setup_local_image_registry/#add-self-signed-certificate-to-the-containerd-runtime","title":"Add self signed certificate to the containerd runtime","text":"<p>The best way to do this is to setup a daemonset which will <code>install and config the provided CA file on each worker node that runs containerd</code>.</p> <p>The complete daemonset config contains three part 1. configmap of the CA certificate 2. configmap of the shell command to copy/config certificate, and restart containerd daemon 3. A daemonset that reads the above configmap and run the shell command.</p> <p>Some ponits which need to be mentioned: - The daemonset uses a privileged security context and hostPID in order to be able to execute commands on the nodes. The <code>privileged commands are run in an initContainers</code>, then <code>an un-privileged container</code> is left running. - The shell commands is stored in a ConfigMap so they can be modified independantly of the DaemonSet. - The daemonset uses a vanilla base image (Debian). You can use any image that has the <code>nsenter tool</code> installed (also tested with Alpine). No need to build a custom image. - We don't want to modify the containerd configuration on the host server. Because if you are using a hosted Kubernetes cluster, like GKS, or Azure Kubernetes Service (AKS), you may not have the privilege to do so. Instead, add custom CA certificate as trusted CA has minimun impact and require less privilege. - For now, the containerd restart is mandatory. It may change for future release.</p>"},{"location":"Onyxia/09.Setup_local_image_registry/#first-configmap-that-stores-the-ca-certificate","title":"First configMap that stores the CA certificate","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: trusted-ca\n  namespace: kube-system\ndata:\n  ca.crt: |+\n    -----BEGIN CERTIFICATE-----\n    MIICGDCCAb6gAwIBAgIUEBGTVbGL5johNO8jXwYeCI5SH7cwCgYIKoZIzj0EAwIw\n    WDELMAkGA1UEBhMCRlIxETAPBgNVBAcTCE1hbGFrb2ZmMRswGQYDVQQKExJDQVNE\n    IFRydXN0IFNlcnZpY2UxGTAXBgNVBAMTEENBU0QgazhzIFJPT1QgQ0EwHhcNMjIx\n    MDEyMDcyMDAwWhcNMjcxMDExMDcyMDAwWjBYMQswCQYDVQQGEwJGUjERMA8GA1UE\n    BxMITWFsYWtvZmYxGzAZBgNVBAoTEkNBU0QgVHJ1c3QgU2VydmljZTEZMBcGA1UE\n    AxMQQ0FTRCBrOHMgUk9PVCBDQTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABEVz\n    2wmHEdIIpAYm3dSg/p8PThC3cASDrbsX0qTpcAEeU8JYtkxhhT1WQJp5XMlD2YDe\n    zCWq2xrcw2sjofcRQ1ujZjBkMA4GA1UdDwEB/wQEAwIBBjASBgNVHRMBAf8ECDAG\n    AQH/AgECMB0GA1UdDgQWBBSPMq1t72FePGfHaSWG66FAirMADDAfBgNVHSMEGDAW\n    gBSPMq1t72FePGfHaSWG66FAirMADDAKBggqhkjOPQQDAgNIADBFAiAVnUzppLzH\n    IICTW127nI/RgU2Us9FZKAWNzZa7PiseKQIhALu+UcPN1KXL4EfuLsgZFOcmPMBH\n    m6gYOKog2jghWFSz\n    -----END CERTIFICATE-----\n</code></pre>"},{"location":"Onyxia/09.Setup_local_image_registry/#second-configmap-that-stores-the-shell-command","title":"Second configMap that stores the shell command","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: setup-script\n  namespace: kube-system\ndata:\n  setup.sh: |\n    echo \"$TRUSTED_CERT\" &gt; /usr/local/share/ca-certificates/ca.crt &amp;&amp; update-ca-certificates &amp;&amp; systemctl restart containerd\n</code></pre>"},{"location":"Onyxia/09.Setup_local_image_registry/#daemonset-spec","title":"DaemonSet spec","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: kube-system\n  name: custom-ca-manager\n  labels:\n    k8s-app: custom-ca-manager\nspec:\n  selector:\n    matchLabels:\n      k8s-app: custom-ca-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: custom-ca-manager\n    spec:\n      hostPID: true\n      hostNetwork: true\n      initContainers:\n      - name: init-node\n        command: [\"nsenter\"]\n        args: [\"--mount=/proc/1/ns/mnt\", \"--\", \"sh\", \"-c\", \"$(SETUP_SCRIPT)\"]\n        image: debian\n        env:\n        - name: TRUSTED_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: trusted-ca\n              key: ca.crt\n        - name: SETUP_SCRIPT\n          valueFrom:\n            configMapKeyRef:\n              name: setup-script\n              key: setup.sh\n        securityContext:\n          privileged: true\n      containers:\n      - name: wait\n        image: k8s.gcr.io/pause:3.1\n</code></pre> <p>You can put the above three parts into one file. You can find the example here</p>"},{"location":"Onyxia/09.Setup_local_image_registry/#use-harbor-to-replicate-images-from-other-image-registry","title":"Use Harbor to replicate images from other image registry","text":"<p>Replication allows users to replicate images, between Harbor and non-Harbor registries, in both pull or push mode.</p> <ul> <li>In pull mode, harbor will pull(download) images from remote image registry (e.g. dockerhub). </li> <li>In push mode, when you push an image to harbor, harbor will push it to a remote registry.</li> </ul>"},{"location":"Onyxia/09.Setup_local_image_registry/#step-1-create-a-replication-endpoint","title":"Step 1. Create a replication endpoint","text":""},{"location":"Onyxia/09.Setup_local_image_registry/#step-2-create-a-replication-rule","title":"Step 2. Create a replication rule","text":"<p>For official image which does not have library before image name, you need to put the <code>source resource filter</code> as  library/redis instead of redis. For images that are located inside a library, you just use the origin name, for example for image inseefrlab/atlas, the source resource filter is inseefrlab/atlas.</p>"},{"location":"Onyxia/09.Setup_local_image_registry/#step-3-running-a-replication-task","title":"Step 3. Running a replication task","text":""},{"location":"Onyxia/vault/13.Vault_client/","title":"Vault client","text":"<p>Vault server proivdes a web UI and a CLI. In this tutorial, we will show how to  - install vault cli - create a secret - update a secret with version - get secret with version - delete secret</p>"},{"location":"Onyxia/vault/13.Vault_client/#install-vault-client","title":"Install vault client","text":"<p>The official installation doc can be found here</p> <pre><code># add hashicorp repo to the apt repo list\nwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\n# install vault via apt\nsudo apt update &amp;&amp; sudo apt install vault\n</code></pre>"},{"location":"Onyxia/vault/13.Vault_client/#use-the-default-vault-client-pods","title":"Use the default vault client pods","text":"<p>If you deploy vault via the hashicorp helm chart, you should find a pod called <code>vault-0</code> in the vault namespace. This pod contains a vault client and <code>pre-configured</code> token to connect to the vault server.</p> <pre><code># get a shell of the vault-0 pod\nkubectl exec -it vault-0 -n vault -- /bin/sh\n\n# try \nvault status\n</code></pre>"},{"location":"Onyxia/vault/13.Vault_client/#use-vault-client-to-connect-to-the-server","title":"Use vault client to connect to the server","text":"<pre><code># to set up the client connextion\nexport VAULT_ADDR='https://vault.casd.local'\nexport VAULT_TOKEN=\"changeMe\"\n\n# check your connextion\nvault status\n\n# you should see below output\nKey             Value\n---             -----\nSeal Type       shamir\nInitialized     true\nSealed          false\nTotal Shares    3\nThreshold       2\nVersion         1.12.0\nBuild Date      2022-10-10T18:14:33Z\nStorage Type    file\nCluster Name    vault-cluster-b5283424\nCluster ID      9ef14394-39ec-bd05-e26d-40f29737c5fb\nHA Enabled      false\n\n# In this case, the vault is already unsealed, if it's sealed, you need to unsealed with the shared secret. Below are some examples (in our case 2 shared secret is enough)\nvault operator unseal p9ySCyRaHXUhQQAw3PgkQhSvoe+mexRlZGILDi2ieLji\nvault operator unseal a8IQbNnbUWLx5mK//nkG0NIO4XtYbeqOFnS7R1STJhQg\nvault operator unseal 4Q+qgYNYTRqER5NdrzFwyYSmI6ZcQ4qYcvat0YKCGqJB\n</code></pre> <p>If you need to connect to a new <code>vault</code> server. You need to reset the env var value of <code>VAULT_ADDR</code> and <code>VAULT_TOKEN</code>.</p> <pre><code># remove the old value\nunset VAULT_ADDR\nunset VAULT_TOKEN\n\n# set new values\nexport VAULT_ADDR='https://vault.casd.local'\nexport VAULT_TOKEN=\"changeMe\"\n</code></pre>"},{"location":"Onyxia/vault/13.Vault_client/#use-client-to-create-kv-secret","title":"Use client to create kv secret","text":"<p>Vault provide many different types of secret. Below figure shows them . In this tutorial, we only show example on the <code>kv secret engine</code></p> <p>The below command can be realized via the vault web ui</p> <pre><code># activate kv secret engine\nvault secrets enable kv\n\n# create your first secret (in kv engine)\n# second kv/pengfei-cli is the name of the secret, k2=v2 is the content of the secret\nvault kv put kv/pengfei-cli k2=v2\n\n# get the secret content\nvault kv get kv/pengfei-cli\n\n# you should see below output\n=== Secret Path ===\nkv/data/pengfei-cli\n\n======= Metadata =======\nKey                Value\n---                -----\ncreated_time       2022-10-31T08:32:34.841417078Z\ncustom_metadata    &lt;nil&gt;\ndeletion_time      n/a\ndestroyed          false\nversion            1\n\n=== Data ===\nKey    Value\n---    -----\nk2     v2\n</code></pre> <p>In the <code>vault web ui</code>, you should see below figure</p> <p></p>"},{"location":"Onyxia/vault/13.Vault_client/#secret-versioning","title":"secret versioning","text":"<p>One of the important vault feature is the <code>secret versioning</code>. We will add version support on the above secret <code>pengfei-cli</code>.</p> <pre><code># active versioning\nvault kv enable-versioning kv/\n\n# try to update the value of the secret\nvault kv put kv/pengfei-cli k2=toto\n\n# by default, vault will return the latest version\n vault kv get kv/pengfei-cli\n\n# get a specific version\nvault kv get -version=1 kv/pengfei-cli\n</code></pre>"},{"location":"Onyxia/vault/13.Vault_client/#secret-deletion","title":"secret deletion","text":"<p>When a secret is deleted, all the key/value pair in it will be deleted.</p> <pre><code># delete a secret\nvault kv delete kv/pengfei-cli\n\n# delete a specific version\nvault kv delete -versions=1 kv/pengfei-cli\n\n# cancel delete\nvault kv undelete -versions=1 kv/pengfei-cli\n\n# delete permently, no possible cancel\nvault kv destroy -versions=1 kv/pengfei-cli\n</code></pre>"},{"location":"adminsys/core/Cron_jobs/","title":"Cron job","text":"<p>Cron is a time-based job scheduling daemon found in Unix-like operating systems, including Linux distributions. Cron runs in the background and operations scheduled with cron, referred to as \"cron jobs\", are executed automatically, making cron useful for automating maintenance-related tasks.</p> <p>## Installation </p> <pre><code># Debian\nsudo apt-get install cron\n\n# centos\nsudo yum install cronie\n\n# gentoo\nsudo emerge -av sys-process/cronie\n</code></pre> <p>## Understanding How Cron Works </p> <p>Cron jobs are recorded and managed in a special file known as a crontab. Each user profile on the system can have their own crontab where they can schedule jobs, which is stored under /var/spool/cron/crontabs/.</p> <p>To schedule a job, open up your crontab for editing and add a task written in the form of a cron expression. The syntax for cron expressions can be broken down into two elements: the schedule and the command to run.</p> <p>The command can be virtually any command you would normally run on the command line. The schedule component of the syntax is broken down into 5 different fields, which are written in the following order:</p> <pre><code># Example of job definition:\n# .---------------- minute (0 - 59)\n# |  .------------- hour (0 - 23)\n# |  |  .---------- day of month (1 - 31)\n# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat\n# |  |  |  |  |\n# *  *  *  *  *  user command to be executed\n\nmm hh jj MMM JJJ [user] task &gt; log\n</code></pre> <ul> <li>mm: 1st position, minute (0 - 59)</li> <li>hh: hour (0 - 23).</li> <li>jj: day of month (1 - 31).</li> <li>MMM: mmonth (1 - 12) OR jan,feb,mar,apr ...</li> <li>JJJ: day of the week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</li> <li>user (optional): uid of user who runs the task (with user right)</li> <li>task: command to be executed.</li> <li>log (optional): redirect the task output to a log file. if no file is specified, a mail will be sent to the user locally.</li> </ul> <p>Each line you add in the crontab will be transformed into a cronjob</p> <p>## Configuration </p> <p>### Managing Cron Job Output </p> <p>Because cron jobs are executed in the background, it isn\u2019t always apparent that they\u2019ve run successfully. So redirecting the output of cron jobs to help you track that they\u2019ve been executed successfully.</p> <p>#### send mail </p> <p>If you have a mail transfer agent, such as <code>Sendmail</code>, installed and properly configured on your server, you can set it up in your crontab</p> <pre><code># open your crontab editor\ncrontab -e\n\n# add below lines\nMAILTO=\"example@digitalocean.com\"\nSHELL=/bin/bash\nHOME=/\n\n* * * * * echo 'Run this command every minute'\n</code></pre> <p>#### Redirect to a log file </p> <p>To append a scheduled command\u2019s output to a log file, add &gt;&gt; to the end of the command followed by the name and location of a log file of your choosing, like this:</p> <pre><code># below command print data to std output, and std output is redirected to the given log file \n* * * * * echo \u2018Run this command every minute\u2019 &gt;&gt; /directory/path/file.log\n\n# if we want to direct the error to the log file, we can use the below line\n# 2 indicates standard error, &gt;&amp;1 means redirect std error to std out,\n# as std out is redirect to log, so all std error will be in log file too. \n* * * * * echo \u2018Run this command every minute\u2019 &gt;&gt; /directory/path/file.log  2&gt;&amp;1\n</code></pre>"},{"location":"adminsys/core/Cron_jobs/#access-control","title":"Access control","text":"<p>You can manage which users can use the crontab command with the cron.allow and cron.deny files, both of which are stored in the /etc/ directory. If the cron.deny file exists, any user listed in it will be barred from editing their crontab. If cron.allow exists, only users listed in it will be able to edit their crontabs. If both files exist and the same user is listed in each, the cron.allow file will override cron.deny and the user will be able to edit their crontab.</p> <p>For example, to <code>deny access to all users and then give access to the user ishmael</code>, you could use the following command sequence:</p> <pre><code>sudo echo ALL &gt;&gt;/etc/cron.deny\nsudo echo ishmael &gt;&gt;/etc/cron.allow\n</code></pre> <p>Legacy job is not impacted by the modification of the two files. For example, if a user has a crontab and some corn job running, if we delete this user's uid from <code>/etc/cron.allow</code>, The already running job will continue. To stop all You need to delete the crontab of this user which is located at /var/spool/cron/crontabs</p>"},{"location":"adminsys/core/Cron_jobs/#use-crontab","title":"Use crontab","text":"<p>Below are some useful commands for cron</p> <pre><code># list the existing cron jobs\ncrontab -l\n\n# Editing the cron jobs\n# this command will open a text file editor\n# each line you add will be transformed into a cronjob\ncrontab -e\n</code></pre> <p>## Cron job scheduler syntaxes </p> <p>In each column, you can use numbers and acronyms such as <code>JAN-DEC</code> and <code>SUN-SAT</code>. There are also a few special characters you can include in the schedule component of a cron expression to streamline scheduling tasks:</p> <ul> <li><code>*</code>: In cron expressions, an asterisk is a wildcard variable that represents \u201call.\u201d Thus, a task scheduled with * * * * * ... will run every minute of every hour of every day of every month.</li> <li><code>,</code>: Commas break up scheduling values to form a list. If you want to have a task run at the beginning and middle of every hour, rather than writing out two separate tasks (e.g., 0 * * * * ... and 30 * * * * ...), you could achieve the same functionality with one (0,30 * * * * ...).</li> <li><code>-</code>: A hyphen represents a range of values in the schedule field. Instead of having 30 separate scheduled tasks for a command you want to run for the first 30 minutes of every hour (as in 0 * * * * ..., 1 * * * * ..., 2 * * * * ..., and so on), instead, you could schedule it as 0-29 * * * * ....</li> <li><code>/</code>: You can use a forward slash with an asterisk to express a step value. For example, instead of writing out eight separate cron tasks to run a command every three hours (as in, 0 0 * * * ..., 0 3 * * * ..., 0 6 * * * ..., and so on), you could schedule it to run like this: 0 */3 * * * ....</li> </ul> <p>For each time column, we can use the three below expressions:   * start-end: This operator means the time range is from start to end. For example, if we put 2-4 in the <code>JJJ</code> column, the task will be executed every Tuesday, Wednesday, and Thursday. If we put 7-12 in the <code>MMM</code> column, the job will be executed every last 6 months of the year.   * start/step: This operator means every step from the start. For example, if 2/15 is in the <code>mm</code> column, this task will be executed every 15 mins starting at the 2nd min after every hour. if */8 is in the <code>hh</code> column, this task will be executed every 8 hours of every day.   * time1,time2,time3,...: This operator means run task at time1, time2 and time3, etc. </p> <p>### Some examples </p> <p>Here are some more examples of how to use cron\u2019s scheduling component:</p> <ul> <li> <ul> <li> <ul> <li> <ul> <li> <ul> <li> <ul> <li> <ul> <li>Run the command every minute.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>12 * * * * - Run the command 12 minutes after every hour.</li> <li>0,15,30,45 * * * * - Run the command every 15 minutes.</li> <li>*/15 * * * * - Run the command every 15 minutes.</li> <li>0 4 * * * - Run the command every day at 4:00 AM.</li> <li>0 4 * * 2-4 - Run the command every Tuesday, Wednesday, and Thursday at 4:00 AM.</li> <li>20,40 */8 * 7-12 * - Run the command on the 20th and 40th minute of every 8th hour every day of the last 6 months of the year.</li> </ul> <p>### Special Syntax </p> <p>There are also several shorthand commands you can use in your crontab file to help streamline job scheduling. They are essentially shortcuts for the equivalent numeric schedule specified:</p> <ul> <li>@hourly : 0 * * * *</li> <li>@daily : 0 0 * * *</li> <li>@weekly : 0 0 * * 0</li> <li>@monthly: 0 0 1 * *</li> <li>@yearly: 0 0 1 1 *</li> <li>@reboot: each reboot</li> </ul> <p>For example, if you add the below line into the crontab</p> <pre><code># this will be executed after each reboot\n@reboot echo \"System start-up\"\n</code></pre>"},{"location":"adminsys/core/Cron_jobs/#predefined-cron-scheduler","title":"Predefined cron scheduler","text":"<p>By default, cron already have 4 predefine</p> <ul> <li>hourly</li> <li>daily</li> <li>weekly</li> <li>monthly</li> </ul> <p>You can get the detail of these scheduler with the below command</p> <pre><code>cat /etc/crontab\n\n# output example\n# /etc/crontab: system-wide crontab\n# Unlike any other crontab you don't have to run the `crontab'\n# command to install the new version when you edit this file\n# and files in /etc/cron.d. These files also have username fields,\n# that none of the other crontabs do.\n\nSHELL=/bin/sh\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\n\n# Example of job definition:\n# .---------------- minute (0 - 59)\n# |  .------------- hour (0 - 23)\n# |  |  .---------- day of month (1 - 31)\n# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat\n# |  |  |  |  |\n# *  *  *  *  * user-name command to be executed\n17 *    * * *   root    cd / &amp;&amp; run-parts --report /etc/cron.hourly\n25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )\n47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )\n52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )\n</code></pre> <p>For example, the weekly scheduler will be executed at 6:47 am of every sunday (0 or 7).</p>"},{"location":"adminsys/core/Cron_jobs/#script-directory","title":"script directory","text":"<ul> <li>/etc/cron.d/ :    Put all scripts here and call them from /etc/crontab file.</li> <li>/etc/cron.daily/: Run all scripts once a day</li> <li>/etc/cron.hourly/:    Run all scripts once an hour</li> <li>/etc/cron.monthly/:   Run all scripts once a month</li> <li>/etc/cron.weekly/:    Run all scripts once a week</li> </ul> <p>Note you need to remove all file extentions such as <code>.sh</code></p>"},{"location":"adminsys/core/Cron_jobs/#test-your-cronjob","title":"test your cronjob","text":"<p>Add a shell or bash script (e.g. aptly_build_release) into directory <code>cron.daily</code>, then run the below command </p> <pre><code># add your script \nsudo vim /etc/cron.daily/aptly_build_release\n\n# add execution right\nsudo chmod a+x /etc/cron.daily/aptly_build_release\n\n# list available cron job\nrun-parts --test /etc/cron.daily\n\n# if you see below line, it means your script will be executed daily\n/etc/cron.daily//aptly_build_release\n</code></pre> <p>All script in cron daily/weekly/monthly will be run with user root. If your script need to be run with specific user privilege, you need to use below command </p> <pre><code>if [ \"$(id -u)\" -eq 0 ]; then\n            exec sudo -u &lt;uid&gt; &lt;command&gt; &lt;args&gt;;\nfi\n</code></pre>"},{"location":"adminsys/core/Systemd%28serviced%29/","title":"Systemd in Linux","text":"<p>In Debian (and other Linux distributions), we use systemd to manage services which should run as <code>background process</code> We use a <code>unit file</code> to define how the service should be start/stop/restart, and how to monitor the service status.  After editing and enabling the <code>unit</code> file, these services can be controlled by using the systemctl command.</p> <p>Systemd is developed to replace the old SysV init. </p>"},{"location":"adminsys/core/Systemd%28serviced%29/#what-is-a-service-in-linux","title":"What is a service in linux","text":"<p>A service is usually a <code>long running daemon (background program) process</code> like nginx, postgresql, or ssh.</p> <p>Each service has a corresponding <code>unit file</code> that tells <code>systemd</code> how to <code>start, stop, restart, or monitor it</code>.</p> <p>The Unit files live in:</p> <ul> <li><code>/lib/systemd/system/</code>: (packaged services)</li> <li><code>/etc/systemd/system/</code>: (local overrides/custom services)</li> </ul>"},{"location":"adminsys/core/Systemd%28serviced%29/#useful-systemd-commands","title":"Useful systemd commands","text":"<pre><code># list all active services\nsystemctl list-units --type=service\n\n# list all service(active and inactive)\nsystemctl list-units --type=service --all\n\n# start a service immediately\nsystemctl start ssh\n\n# stop immediately\nsystemctl stop ssh\n\n# restart \nsystemctl restart ssh\n\n# check service status\nsystemctl status ssh\n\n# Enable/disable a service at boot\nsystemctl enable/disable ssh\n\n# check if a service is enabled at boot\nsystemctl is-enabled ssh\n</code></pre>"},{"location":"adminsys/core/Systemd%28serviced%29/#why-we-use-systemd","title":"Why we use systemd","text":"<p>Compared to a bash script, the systemd has the below advantages: - Automatic management: Starts at boot, restarts if it fails. - Dependency awareness: Can wait for network/storage service before starting. - Logging integration: Logs go to journalctl -u postgresql. - Resource control: Built-in cgroups support (limit memory, CPU, I/O). - Monitoring tools: systemctl status postgresql shows PID, uptime, logs. - Unified interface: Consistent commands across all services.</p> <p>It has few disadvantages: - Less transparent: Logic hidden inside systemd unit file, not just a simple script. - Steeper learning curve: You need to learn how to write <code>unit files</code> and systemd internals. - Systemd dependency: If OS does not have it, the unit file won\u2019t work. - Overhead for simple cases \u2013 For a single local process, systemd may feel like \u201coverkill.\u201d</p>"},{"location":"adminsys/core/Systemd%28serviced%29/#systemd-unit-file","title":"Systemd Unit file","text":"<p>There is a more detailed doc on unit file here</p> <p>A systemd unit file is structured into <code>sections</code>. The three most common are [Unit], [Service], and [Install].  Each has a distinct purpose: - [Unit]: metadata + dependencies. When and under what conditions should this service start? - [Service]: execution details. How do we start, stop, restart, and monitor this process? - [Install]: startup integration. Should this run automatically at boot, and in which boot mode?</p>"},{"location":"adminsys/core/Systemd%28serviced%29/#unit-section","title":"[Unit] section","text":"<p>The unit section defines <code>metadata and dependencies</code> for the service. Below are the most common attributes of this section: - Description: Human-readable description of the service. - Documentation: Optional links to docs. - After: Order dependency (start this service after something else, e.g. network.target). - Requires: Hard dependency (if required service fails, this one stops too). - Wants: Soft dependency (preferred, but doesn\u2019t stop if missing).</p> <p>For example, the below conf means: - The <code>OpenMetadata Service</code> starts after the network stack is ready. - It needs <code>PostgreSQL and elasticsearch</code>; if PostgreSQL or elasticsearch fails, OpenMetadata stops too.</p> <pre><code>[Unit]\nDescription=OpenMetadata Service\nAfter=network.target\nRequires=postgresql.service, elasticsearch.service\n</code></pre>"},{"location":"adminsys/core/Systemd%28serviced%29/#service-section","title":"[Service] section","text":"<p>The service section defines how the actual service runs.</p> <ul> <li>ExecStart: The actual command to start the service.</li> <li>ExecStop: Command to stop it.</li> <li>ExecReload: Command to reload config without restart.</li> <li>WorkingDirectory: Where the service runs.</li> <li>User=/Group: Run as specific user/group (not root).</li> <li>Restart: Restart policy (no, on-failure, always).</li> <li>RestartSec: Delay before restart.</li> <li>Environment: Set environment variables.</li> <li>StandardOutput=/StandardError: Where logs go (journal, file, etc.).</li> <li>Type: How the service runs:          - simple: Default, process runs in foreground.          - forking: For daemons that fork themselves (e.g. old scripts).           - oneshot: For short-lived tasks (runs once, exits).</li> </ul> <pre><code>[Service]\nUser=openmeta\nGroup=openmeta\nWorkingDirectory=/opt/openmetadata\nExecStart=/opt/openmetadata/openmetadata.sh start\nExecStop=/opt/openmetadata/openmetadata.sh stop\nRestart=on-failure\nRestartSec=5\nStandardOutput=journal\nStandardError=journal\n</code></pre> <p>The user openmeta must exist, otherwise the service will not start. </p>"},{"location":"adminsys/core/Systemd%28serviced%29/#run-service-with-dedicated-user-and-group","title":"Run service with dedicated user and group","text":"<p>If we don't specify <code>User/group</code>, systemd will run the service as root with the user default group(e.g. root).  This can cause serious security problems. The best practice is that we always create a dedicated system user account and group.</p> <pre><code># create a system group\nsudo groupadd --system openmeta\n# create a system user with no login shell, no home dir.\nsudo useradd --system --no-create-home --shell /usr/sbin/nologin --gid openmeta openmeta\n</code></pre>"},{"location":"adminsys/core/Systemd%28serviced%29/#install-section","title":"[Install] section","text":"<p>The install section defines how the service integrates into system startup. - WantedBy: Which target this service should be part of when enabled.     - multi-user.target: Typical for servers (like runlevel 3 in SysV).     - graphical.target: For desktop services (like runlevel 5).</p> <ul> <li>RequiredBy: Like WantedBy, but hard dependency.</li> </ul> <p>For example, the below config means: When you run systemctl enable openmetadata.service, systemd creates symlinks  so the service starts automatically in multi-user mode (normal boot without GUI).</p> <pre><code>[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"adminsys/core/Systemd%28serviced%29/#full-example","title":"Full example","text":"<p>The below unit file shows how to run openmetadata as systemd service</p> <pre><code>[Unit]\nDescription=OpenMetadata Service\nAfter=network.target\nWants=network.target\nRequires=postgresql.service elasticsearch.service\n\n[Service]\nType=forking\n# Run as non-root user\nUser=openmeta\nGroup=openmeta\nWorkingDirectory=/opt/\n\n# load env var\nEnvironmentFile=-/opt/openmetadata/conf/openmetadata-env.sh\n\n# How to start and stop\nExecStart=/opt/openmetadata/openmetadata.sh start\nExecStop=/opt/openmetadata/openmetadata.sh stop\n\n# checking service health.\nExecReload=/opt/openmetadata/openmetadata.sh status\n# Optional: status and clean hooks\nExecStartPre=/opt/openmetadata/openmetadata.sh clean\n\n# Restart policy if the service crashes\nRestart=on-failure\nRestartSec=5\n\n# Logging: send stdout/stderr to systemd journal\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>We suppose the openmetadata app is installed under <code>/opt/openmetadata</code></p>"},{"location":"adminsys/core/Time_Synchronization_in_Debian/","title":"Set Up Time Synchronization on Debian","text":"<p><code>Time synchronization is essential for ensuring the reliability, security, and functionality</code> of Debian and  other Unix-like systems. The Network Time Protocol (NTP) is an essential element for time sync that  synchronizes the clocks of <code>client computers</code> with a <code>time server</code>.</p> <p>In debian system, there are three implementations of the NTP: - ntpd: Implement both server and client - chrony: Implement both server and client - timesyncd: Implement only client</p> <p>Most Linux distributions provide <code>a ntp client</code> and <code>a default configuration</code> that points to time servers that they maintain.  By default, in <code>debian 11</code>, the default time synchronization client is called timesyncd. </p> <p>All these clients use the pre-configured ntp server to synchronize time. For example you should probably see the below  urls in your conf :  - <code>2.fedora.pool.ntp.org</code> - <code>0.ubuntu.pool.ntp.org</code> </p> <p>The problem is that you need internet access to do the time synchronization. If your servers do not have internet access the default configuration will not work.</p> <p>The purpose of this tutorial is to set a local ntp server. And all client servers use this local ntp server to do the time syncronization.</p>"},{"location":"adminsys/core/Time_Synchronization_in_Debian/#1setup-a-time-serverntp-server","title":"1.Setup a time server(NTP server)","text":""},{"location":"adminsys/core/Time_Synchronization_in_Debian/#2configure-a-client-to-synchronize-with-the-time-server","title":"2.Configure a client to synchronize with the time-server","text":""},{"location":"adminsys/core/Time_Synchronization_in_Debian/#21-check-the-current-status-of-your-client-computer","title":"2.1 Check the current status of your client computer","text":"<p>You can use below command to check the current date time of your client computer</p> <pre><code># Check time on debian\ndate\n\n# the output is like\nMon Dec  4 16:16:40 CET 2023\n\n# for more details, you can use\ntimedatectl status --all\n\n# The output is like\n               Local time: Mon 2023-12-04 16:26:16 CET\n           Universal time: Mon 2023-12-04 15:26:16 UTC\n                 RTC time: Mon 2023-12-04 15:26:16\n                Time zone: Europe/Paris (CET, +0100)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n</code></pre> <p>CET(central european time) is the timezone acronym. </p>"},{"location":"adminsys/core/Time_Synchronization_in_Debian/#22-configuring-a-client-for-time-synchronization","title":"2.2 Configuring a client for time synchronization","text":""},{"location":"adminsys/core/Time_Synchronization_in_Debian/#221-configuration-of-timesyncd","title":"2.2.1 Configuration of timesyncd","text":""},{"location":"adminsys/core/Time_Synchronization_in_Debian/#get-current-status-of-systemd-timesyncd","title":"Get current status of systemd-timesyncd","text":"<pre><code># get current status of systemd-timesyncd\nsystemctl status systemd-timesyncd.service\n\n# the output looks like:\nsystemd-timesyncd.service - Network Time Synchronization\n     Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2023-11-28 15:52:13 CET; 6 days ago\n       Docs: man:systemd-timesyncd.service(8)\n   Main PID: 386 (systemd-timesyn)\n     Status: \"Initial synchronization to time server 213.5.132.231:123 (2.debian.pool.ntp.org).\"\n      Tasks: 2 (limit: 9413)\n     Memory: 1.7M\n        CPU: 666ms\n     CGroup: /system.slice/systemd-timesyncd.service\n             \u2514\u2500386 /lib/systemd/systemd-timesyncd\n</code></pre> <p>We could notice that this client use ntp server 2.debian.pool.ntp.org to synchronize time</p> <p>Some other useful command</p> <pre><code># enable at boot    \nsystemctl enable systemd-timesyncd\n\n# start the service\nsystemctl start systemd-timesyncd\n</code></pre>"},{"location":"adminsys/core/Time_Synchronization_in_Debian/#change-the-ntp-server","title":"Change the ntp server","text":"<p>The main configuration file of systemd-timesyncd.service is located at /etc/systemd/timesyncd.conf. We suppose the ntp server ip is .</p> <pre><code>sudo vim /etc/systemd/timesyncd.conf\n\n# add the following line\nNTP=10.50.5.57\n# you can put multiple line of ntp server, it will be used as backup if the first one does not work.\nNTP=...\n</code></pre> <p>As timesyncd is a light weight of ntp, it takes only one out of the NTP= list that works, and if all in there fail    try the list in FallbackNTP=. There is no cross server checks for better time sync. If you want that feature, use   ntpd or chrony.it sta</p>"},{"location":"adminsys/core/debian_admin_cheat_sheet/","title":"Debian amdin cheat sheet","text":""},{"location":"adminsys/core/debian_admin_cheat_sheet/#1-network-setup","title":"1. Network setup","text":"<p>If you have a dhcp server, it will distriubt you an IP address. But sometimes, you don't have a dhcp server. So you need to setup a static IP address.</p>"},{"location":"adminsys/core/debian_admin_cheat_sheet/#11-get-available-network-interfaces","title":"1.1 Get available network interfaces","text":"<pre><code># show all activated interfaces\nip a\n\n# show all interfaces, can be up or down \nip -c link\n</code></pre>"},{"location":"adminsys/core/debian_admin_cheat_sheet/#12-configure-static-ip-tested-on-debian-1011","title":"1.2 Configure static IP (tested on debian 10/11)","text":"<p>The main config of network interfaces are located at <code>/etc/network/interfaces</code></p> <pre><code>suod vim /etc/network/interfaces\n</code></pre> <p>You should see below text. In this example, the name of the interface is <code>eth0</code> and it uses <code>dhcp</code></p> <pre><code># This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nallow-hotplug eth0\niface eth0 inet dhcp\n</code></pre> <p>Change it from <code>dhcp</code> to static. You need to add all the required informaton for the ip address to work.</p> <p>Below is an example: (ip: 10.50.5.58, mask: 255.255.255.0, gateway: 10.50.0.1, dns:10.50.0.10)</p> <pre><code># This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nallow-hotplug eth0\niface eth0 inet static\n        address 10.50.5.58/24\n        gateway 10.50.0.1\n        # dns-* options are implemented by the resolvconf package, if installed\n        dns-nameservers 10.50.0.10 8.8.8.8\n        dns-search casd.eu casd.local\n</code></pre> <p>Note, you need to restart the service to activate the new configuration</p> <pre><code>systemctl restart networking.service; ifup eth0\n</code></pre>"},{"location":"adminsys/core/debian_admin_cheat_sheet/#13-dns-configuration","title":"1.3 DNS configuration","text":"<p>The dns configuration is located at <code>/etc/resolv.conf</code>. Below is an example</p> <pre><code>search casd.local\nnameserver 10.50.0.10\nnameserver 8.8.8.8\n</code></pre> <p>Normally, you don't need to edit this file directly. Debian provides two packages(<code>openresolv and resolvconf</code>), each of which contains a program named <code>resolvconf</code>, which may be used to control (or outright prevent) the alteration of the /etc/resolv.conf file by various networking programs. They conflict with each other, so you have to pick at most one of them.</p> <p>So if you modified the <code>/etc/network/interfaces</code> and restart network.service, the program ((<code>openresolv or resolvconf</code>)) will update the <code>/etc/resolv.conf</code>.</p> <p>You can view it to see exactly which dns you are using.</p>"},{"location":"adminsys/core/debian_admin_cheat_sheet/#2-apt-configuration","title":"2. Apt configuration","text":"<p>If your apt can't find any package, it means your repo is not pointing at the right server url. You need to edit <code>/etc/apt/sources.list</code>, and add appropriate repo url. Below is the minimun setup for apt to work.</p> <pre><code>deb http://deb.debian.org/debian/ bullseye main\ndeb-src http://deb.debian.org/debian/ bullseye main\ndeb http://security.debian.org/debian-security bullseye-security main contrib\ndeb-src http://security.debian.org/debian-security bullseye-security main contrib\ndeb http://deb.debian.org/debian/ bullseye-updates main contrib\ndeb-src http://deb.debian.org/debian/ bullseye-updates main contrib\n</code></pre>"},{"location":"adminsys/core/debian_admin_cheat_sheet/#3-security","title":"3. Security","text":""},{"location":"adminsys/core/debian_admin_cheat_sheet/#add-user-to-sudoer-list","title":"Add user to sudoer list","text":"<p>Solution1: The easiest way is to add user to the sudoer group</p> <pre><code># loign as root, because usermod file is in /usr/sbin\nsu -\n\n# add user to the sudoer group\nusermod -aG sudo user fbar\n\n# To ensure that the user has been added to the group\nsudo whoami\n</code></pre> <p>Solution2: You can also edit the <code>/etc/sudoers</code> (always use visudo)</p> <pre><code>visudo\n\n# in the file /etc/sudoers, add the following line\nusername  ALL=(ALL) NOPASSWD:ALL\n\n# save and quit the visudo editor\n</code></pre> <p>Note the user need to logout and re login to have the sudo group.</p>"},{"location":"adminsys/core/debian_admin_cheat_sheet/#add-trusted-root-ca","title":"Add trusted root ca","text":"<p>https://www.digitalocean.com/community/tutorials/how-to-set-up-and-configure-a-certificate-authority-ca-on-debian-11</p> <pre><code>sudo cp /tmp/ca.crt /usr/local/share/ca-certificates/\nsudo update-ca-certificates\n\n# you can force the refresh of the trusted ca store\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Installation/","title":"Maria DB installation","text":""},{"location":"adminsys/db_management/mariadb/Installation/#1-installation","title":"1. Installation","text":""},{"location":"adminsys/db_management/mariadb/Installation/#step-1","title":"Step 1:","text":"<p>Install software-properties-common if missing:</p> <pre><code>sudo apt update\nsudo apt install software-properties-common\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Installation/#step-2","title":"Step 2:","text":"<p>Run the command below to add Repository Key to the system</p> <pre><code>Import MariaDB gpg key and repo\nsudo apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8\n\n# Add the apt repository\n# Note the below apt-repository url is for ubuntu, for other distribution you may need to change the url\nsudo add-apt-repository \"deb [arch=amd64,arm64,ppc64el] http://mariadb.mirror.liquidtelecom.com/repo/10.4/ubuntu $(lsb_release -cs) main\"\n\nsudo apt update\nsudo apt -y install mariadb-server mariadb-client\n\n# You will be prompted to provide MariaDB root password, after the above command.\n# If you didn\u2019t receive password set prompt, then manually run the MySQL hardening script.\nsudo mysql_secure_installation \n\nNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB\n      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!\n\nIn order to log into MariaDB to secure it, we'll need the current\npassword for the root user. If you've just installed MariaDB, and\nhaven't set the root password yet, you should just press enter here.\n\nEnter current password for root (enter for none): \nOK, successfully used password, moving on...\n\nSetting the root password or using the unix_socket ensures that nobody\ncan log into the MariaDB root user without the proper authorisation.\n\nYou already have your root account protected, so you can safely answer 'n'.\n\nSwitch to unix_socket authentication [Y/n] y\nEnabled successfully!\nReloading privilege tables..\n ... Success!\n\n\nYou already have your root account protected, so you can safely answer 'n'.\n\nChange the root password? [Y/n] y\nNew password: \nRe-enter new password: \nPassword updated successfully!\nReloading privilege tables..\n ... Success!\n\n\nBy default, a MariaDB installation has an anonymous user, allowing anyone\nto log into MariaDB without having to have a user account created for\nthem.  This is intended only for testing, and to make the installation\ngo a bit smoother.  You should remove them before moving into a\nproduction environment.\n\nRemove anonymous users? [Y/n] y\n ... Success!\n\nNormally, root should only be allowed to connect from 'localhost'.  This\nensures that someone cannot guess at the root password from the network.\n\nDisallow root login remotely? [Y/n] y\n ... Success!\n\nBy default, MariaDB comes with a database named 'test' that anyone can\naccess.  This is also intended only for testing, and should be removed\nbefore moving into a production environment.\n\nRemove test database and access to it? [Y/n] y\n - Dropping test database...\n ... Success!\n - Removing privileges on test database...\n ... Success!\n\nReloading the privilege tables will ensure that all changes made so far\nwill take effect immediately.\n\nReload privilege tables now? [Y/n] y\n ... Success!\n\nCleaning up...\n\nAll done!  If you've completed all of the above steps, your MariaDB\ninstallation should now be secure.\n\nThanks for using MariaDB!\n</code></pre> <p>If you are not able to set up root password, you can follow</p>"},{"location":"adminsys/db_management/mariadb/Installation/#step-3-test-the-mariadb-installation","title":"Step 3: Test the mariaDB installation","text":"<pre><code># check daemon status\nsudo systemctl status mysql\n\n# connect to the server via mysql client\nmysql -u root -p\n\n# check your installation version\nSELECT VERSION();\n\n# exit the sql terminal\nQUIT\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Installation/#2-removepurge-old-installation","title":"2. Remove/Purge old installation","text":"<p>If you already have one installation of mysql or mariadb, and you need to install a new one. It's recommended to remove and purge all the dependencies of the old installation. Because you will have many conflicts which are not  expected.</p> <p>Just follow the below steps</p> <pre><code># 1. make sure that MySQL service is stopped.\nsudo systemctl stop mysql\n\n# 2. Remove MySQL related all packages completely.\nsudo apt-get purge mysql-server mysql-client mysql-common mysql-server-core-* mysql-client-core-*\n\n# 3. Remove MySQL configuration and data. If you have changed database location in your MySQL configuration, \n#    you need to replace /var/lib/mysql according to it.\nsudo rm -rf /etc/mysql /var/lib/mysql\n\n# 4. (Optional but recommended) Remove unnecessary packages.\nsudo apt autoremove\n\n# 5. (Optional) Remove apt cache.\nsudo apt autoclean\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Installation/#3-recover-your-root-password","title":"3. Recover your root password","text":"<p>Do not do this if you have other options. <code>You must have access to the Linux server running MySQL or MariaDB with a sudo user.</code></p> <pre><code># Identifying the Database Version\nmysql --version\n\n# Stopping the Database Server\nsudo systemctl stop mysql/mariadb\n\n# Restarting the Database Server Without Permission Checking\nsudo mysqld_safe --skip-grant-tables --skip-networking &amp;\n# note this will run the mysqld in the background, if you want to check the status, you can use\njobs\n# when you have the job id, you can use \nfg %&lt;job-id&gt;\n\n\n# Now you can connect to the database as the root user, which should not ask for a password.\nmysql -u root\n\n# For MySQL 5.7.6 and newer as well as MariaDB 10.1.20 and newer, use the following command.\n# don't forget to change the new_password to a value which you want\nALTER USER 'root'@'localhost' IDENTIFIED BY 'new_password';\n\n# For MySQL 5.7.5 and older as well as MariaDB 10.1.20 and older, use:\nSET PASSWORD FOR 'root'@'localhost' = PASSWORD('new_password');\n\n# In either case, you should see confirmation that the command has been successfully executed.\nOutput\nQuery OK, 0 rows affected (0.00 sec)\n\n# kill the mysqld unsafe daemon\nsudo kill `cat /var/run/mysqld/mysqld.pid`\n# for MariaDB\nsudo kill `/var/run/mariadb/mariadb.pid`\n\n# Restart the Database Server Normally\nsudo systemctl start mysql/mariadb\n\n# connect to the server with the new password\nmysql -u root -p\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Installation/#4-change-innodb_page_size","title":"4. Change innodb_page_size","text":"<p>When we encounter Row Size Too Large Errors with InnoDB error, we may need to increase the innodb_page_size</p> <p>To learn more details of this bug, you can visit this page https://mariadb.com/kb/en/troubleshooting-row-size-too-large-errors-with-innodb/</p> <pre><code># get a sql terminal\nmysql -u root -p\n\n# get the current database innodb_page_size\nshow variables like '%innodb_page_size%';\nexit;\n\n# stop the db daemon\nsudo systemctl stop mysql\n\n# backup the system database (e.g. innodb) data and log files\n# in debian os, the data ibdata1 and the log files (ib_logfile0 &amp; ib_logfile1) are located at /var/lib/mysql\ncd /var/lib/mysql/\nsudo mkdir /tmp/innodb_bkp\nsudo mv ibdata1 /tmp/innodb_bkp\nsudo sudo mv ib_logfile* /tmp/innodb_bkp\n\n# now change the innodb_page_size value\n# in debian, the mysql/mariadb conf file are located /etc/mysql/my.cnf\n# you can add the below line in the [mysqld]\n# suppose we want to put 8k, the default value for MariaDB-1:10.4.33 is 16k\n[mysqld]\ninnodb_page_size=8k\n\n# restart the mysql daemon\nsudo systemctl start mysql\n</code></pre> <p>after changing the innodb_page_size, the existing database may not work properly, so you may need to export/import   the existing database for safety.</p>"},{"location":"adminsys/db_management/mariadb/Management/","title":"Manage a mysql/mariadb database","text":""},{"location":"adminsys/db_management/mariadb/Management/#basic-commands","title":"Basic commands","text":"<pre><code># create a database\ncreate database &lt;db_name&gt;;\n\n# create a user with a password\nCREATE USER 'username'@'hostname' IDENTIFIED BY 'password';\n\n# Here\u2019s an example which allows user \u2018matthew\u2019 to connect from any host.\nCREATE USER 'matthew'@'%' IDENTIFIED BY 'supersecretpassword';\n\n# After creating the user, you will need to grant the necessary privileges to the user. \n# This is done using the GRANT statement, which has the following form:\nGRANT priv_type ON priv_level TO 'username'@'hostname';\n# priv_type is the type of privilege which you want to grant (such as SELECT, INSERT, UPDATE, etc.), \n# priv_level is the level at which the privilege should apply (such as a specific database or table), \n# and username and hostname with the values you used in the CREATE USER statement.\n\n# Here is an example of granting ALL permissions on all databases to our user, Matthew:\nGRANT ALL PRIVILEGES ON * . * TO 'matthew'@'%';\n\n# After granting the necessary privileges to the user, you can use the FLUSH PRIVILEGES statement to make the \n# changes take effect. This statement has the following form:\nFLUSH PRIVILEGES;\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Management/#enable-remote-access","title":"Enable remote access","text":""},{"location":"adminsys/db_management/mariadb/Management/#step1-update-server-bind-address","title":"Step1: Update server bind address","text":"<p>By default, mysql/mariadb only listens to local host, and forbid all remote access. To enable it, you need to change the default config.</p> <pre><code># verify current stat\nnetstat -ant | grep 3306\n\n# you should see something like this\ntcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      3731352/mysqld  \n\n# change the default config\nsudo vim /etc/mysql/my.cnf\n\n# find the line bind-address = 127.0.0.1 and change it to \nbind-address = 0.0.0.0\n\n# restart the service \nsudo systemctl restart mysql/mariadb\n\n# check the new bind ip\n$ netstat -ant | grep 3306\n\ntcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN\n</code></pre>"},{"location":"adminsys/db_management/mariadb/Management/#step2-update-user-authorization","title":"Step2: Update user authorization","text":"<p>By default, mysql set an acl for each user to a database with a list of authorize ip. If user try to connect to a server with an authorized ip address, the connexion will be denied.</p> <p>Below is an example to set proper acl to allow user to connect to a database with an authorized IP address.</p> <pre><code># First, log in to the MySQL/MariaDB server with the root privilege:\n\n$ mysql -u admin -p\n\n# create a new db\nMariaDB [(none)]&gt; CREATE DATABASE wpdb;\n\n# create a user \nMariaDB [(none)]&gt; CREATE USER  'wpuser'@'localhost' IDENTIFIED BY 'password';\n\n\n# you will need to grant permissions to the remote system with IP address 208.117.84.50 to connect to the database named wpdb as user wpuser. You can do it with the following command:\nMariaDB [(none)]&gt; GRANT ALL ON wpdb.* to 'wpuser'@'208.117.84.50' IDENTIFIED BY 'password' WITH GRANT OPTION;\n\n# Next, flush the privileges and exit from the MariaDB shell with the following command:\n\nMariaDB [(none)]&gt; FLUSH PRIVILEGES;\nMariaDB [(none)]&gt; EXIT;\n\n# If you want to grant remote access on all databases for wpuser, run the following command:\nMariaDB [(none)]&gt; GRANT ALL ON *.* to 'wpuser'@'208.117.84.50' IDENTIFIED BY 'password' WITH GRANT OPTION;\n\n# If you want to grant access to all remote IP addresses on wpdb as wpuser, use % instead of IP address (208.117.84.50) as shown below:\nMariaDB [(none)]&gt; GRANT ALL ON wpdb.* to 'wpuser'@'%' IDENTIFIED BY 'password' WITH GRANT OPTION;\n\n# If you want to grant access to all IP addresses in the subnet 208.117.84.0/24 on wpdb as user wpuser, run the following command:\nMariaDB [(none)]&gt; GRANT ALL ON wpdb.* to 'wpuser'@'208.117.84.%' IDENTIFIED BY 'password' WITH GRANT OPTION;\n</code></pre> <p>A brief explanation of each parameter is shown below:</p> <ul> <li>wpdb: It is the name of the MariaDB database that the user wants to connect to.</li> <li>wpuser: It is the name of the MariaDB database user.</li> <li>208.117.84.50: It is the IP address of the remote system from which the user wants to connect.</li> <li>password: It is the password of the database user.</li> </ul>"},{"location":"adminsys/db_management/mariadb/Management/#test-connection-from-remote-server","title":"Test connection from remote server","text":"<pre><code>$ sudo apt-get install mariadb-client -y\n\n# Once the installation is completed, connect to the MariaDB server by running the following command on the remote system:\nmysql -u &lt;uid&gt; -h &lt;db-ip&gt; -p\n</code></pre>"},{"location":"adminsys/db_management/postgres/Install_postgres_via_helm_chart/","title":"10. Deploy PostgreSQL Using Helm","text":"<p>Helm gives you a quick and easy way to deploy a PostgreSQL instance on your cluster.</p>"},{"location":"adminsys/db_management/postgres/Install_postgres_via_helm_chart/#101-add-helm-repository","title":"10.1 Add Helm Repository","text":"<p>Search Artifact Hub for a PostgreSQL Helm chart that you want to use. Add the chart's repository to your local Helm installation by typing:</p> <pre><code># general form\nhelm repo add [repository-name] [repository-address]\n\n# In this tutorial, we will use the chart of `Bitnami`.\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# update your local repo\nhelm repo update\n</code></pre>"},{"location":"adminsys/db_management/postgres/Install_postgres_via_helm_chart/#102-create-and-apply-persistent-storage-volume","title":"10.2 Create and Apply Persistent Storage Volume","text":"<p>This step can be ommitted if you use storageClass such as rook-ceph-block. You can create a pvc, a corresponding pv will be created automatically Manifest for creating the PV: <code>postgres-pv.yaml</code>.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgresql-pv\n  labels:\n    type: local\nspec:\n  storageClassName: rook-ceph-block\n  capacity:\n    storage: 20Gi\n  accessModes:\n    - ReadWriteOnce\n</code></pre> <p>Apply the configuration with kubectl:</p> <pre><code>kubectl apply -f postgres-pv.yaml\n</code></pre> <p>PV creation requires admin right, and it's a cluster level resource which can be consumed by PVC</p>"},{"location":"adminsys/db_management/postgres/Install_postgres_via_helm_chart/#103-create-and-apply-persistent-volume-claim","title":"10.3 Create and Apply Persistent Volume Claim","text":"<p>Create a Persistent Volume Claim (PVC) to request the storage allocated in the previous step.</p> <p>Manifest for creating the PVC: <code>postgres-pvc.yaml</code></p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgresql-pv-claim\nspec:\n  storageClassName: rook-ceph-block\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n</code></pre> <p>Apply the configuration with kubectl:</p> <pre><code>kubectl apply -f postgres-pvc.yaml\n\n# check the created pvc\nkubectl ger pvc\n</code></pre> <p>PVC is a namespaced resource, it needs to be created and used in a specific namespace</p>"},{"location":"adminsys/db_management/postgres/Install_postgres_via_helm_chart/#104-install-postgres-via-helm-chart","title":"10.4 Install postgres via Helm chart","text":"<p>You need to modify the default chart configuration values.yaml.</p> <pre><code>global:\n  postgresql:\n    auth:\n      postgresPassword: \"postgres\"\n      username: \"keycloak\"\n      password: \"changeMe\"\n      database: \"keycloak\"\n</code></pre> <pre><code>helm install keycloak-postgres bitnami/postgresql --set persistence.existingClaim=postgresql-pv-claim --set volumePermissions.enabled=true --values values.yaml -n keycloak\n</code></pre> <p>This helm chart should create below resources:</p> <pre><code>kubectl get all -n keycloak\n\n# one statefulset\nNAME                                            READY   AGE\nstatefulset.apps/keycloak-postgres-postgresql   1/1     47m\n\n# one pod of the statefulset\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/keycloak-postgres-postgresql-0   1/1     Running   0          47m\n\n# two services\nNAME                                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/keycloak-postgres-postgresql      ClusterIP   10.233.36.23   &lt;none&gt;        5432/TCP   47m\nservice/keycloak-postgres-postgresql-hl   ClusterIP   None           &lt;none&gt;        5432/TCP   47m\n\nkubectl get secret -n keycloak\n\n# It generate also a secret\nNAME                                      TYPE                                  DATA   AGE\nkeycloak-postgres-postgresql              Opaque                                2      46m\n</code></pre> <p>You can view the content of the secrete with below command</p> <pre><code># get the secret content in yaml format\nkubectl get secret -n keycloak keycloak-postgres-postgresql -o yaml\n\n# get the root password\nkubectl get secret -n keycloak keycloak-postgres-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 --decode\n\n# get the password of the user `keycloak`\nkubectl get secret -n keycloak keycloak-postgres-postgresql -o jsonpath=\"{.data.password}\" | base64 --decode\n\n# you can create env var\nexport POSTGRES_ROOT_PASSWORD=$(kubectl get secret -n keycloak keycloak-postgres-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 --decode)\n</code></pre>"},{"location":"adminsys/db_management/postgres/Install_postgres_via_helm_chart/#105-test-installed-database","title":"10.5 Test installed database","text":"<p>You can use this repo to create a psql client container.</p> <p>Copy below content in <code>pod.yml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgresql-client\n  labels:\n    app: postgresql-client\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: \"true\"    \nspec:\n  securityContext:\n    runAsNonRoot: true\n    supplementalGroups: [ 10001] \n    fsGroup: 10001    \n  containers:\n    - name: postgresql-client\n      image: liupengfei99/psql-client\n      imagePullPolicy: Always\n      securityContext:\n        runAsUser: 1000      \n      stdin: true\n      tty: true\n      command: [\"/bin/sh\"]\n</code></pre> <p>Use below command to deploy the pod</p> <pre><code># deploy pod\nkubectl apply -f pod.yml\n\n# get a shell of the \nkubectl exec -it postgresql-client -- /bin/sh\n\n# general form\npsql -h &lt;host_ip_address&gt; -p &lt;port&gt; -U &lt;user&gt; -W\n\n# connect to a postgresql svc (via service ip)\npsql -h 10.233.36.23 -U keycloak -W\n\n# via k8s service fqdn\npsql -h keycloak-postgres-postgresql.keycloak.svc.cluster.local -U keycloak -W\n</code></pre>"},{"location":"adminsys/fs/Minio/01.Introduction/","title":"The object storage","text":""},{"location":"adminsys/fs/Minio/01.Introduction/#1-definition-and-terms","title":"1. Definition and terms","text":"<p>Object storage, also known as object-based storage, is a computer data storage architecture designed to handle  large amounts of unstructured data. Unlike other architectures, it designates data as distinct units, bundled  with metadata and a unique identifier that can be used to locate and access each data unit. </p> <p>Two important terms in object storage: - Buckets : A bucket is a logical container used for storing objects(e.g. files, images, videos, etc.). Buckets         act as <code>top-level folders or repositories</code> within these object storage systems where data can be stored and         organized.  - Objects : An object in object storage is a piece of data that consists of Key(unique identifier), Data and Metadata.</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#important-properties-of-bucket","title":"Important properties of Bucket","text":"<p>Below are some important properties of A bucket:</p> <p>-Unique Names: Each bucket in an object storage system must have a unique name within that service. This  uniqueness is enforced across the entire storage service to ensure that each bucket can be easily identified.</p> <p>-Access Control: Bucket-level access control allows users to define who can access the objects within a specific                  bucket and what level of access they have (read, write, delete, etc.).</p> <p>-Storage Policies: Storage policies or settings can be applied at the bucket level, defining properties like                 <code>data redundancy, access permissions, encryption, and lifecycle management</code>                 (e.g., defining rules for object expiration or moving to less expensive storage tiers).</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#11-object-storage-vs-file-storage-vs-block-storage","title":"1.1 Object storage vs. file storage vs. block storage","text":""},{"location":"adminsys/fs/Minio/01.Introduction/#file-storagesystem","title":"File storage(system)","text":"<p>File storage stores and organizes data into folders.  It is one of the most common and traditional forms of storing  data on computers and other storage devices (e.g. Direct-Attached Storage (DAS), Network-Attached Storage (NAS), Storage Area Network (SAN)). </p> <ul> <li>Direct-Attached Storage (DAS): storage devices directly connects to a computer system (or server) without going             through a network. The physical storage devices include <code>hard disk drives (HDDs), solid-state drives (SSDs)</code>,            which are directly attached to a computer via interfaces like <code>SATA, USB, or PCIe</code>. For example, <code>EXT4, FAT32,            NTFS, etc</code>. are file systems which we use in DAS.</li> <li>Network-Attached Storage (NAS):  NAS is a storage device or server connected to a network that provides storage             and file system access to multiple clients and servers. It allows multiple users and devices to access              shared files simultaneously over a network. NFS is a typical file system which we use in NAS.</li> <li>Storage Area Network (SAN): SAN is a dedicated high-speed network or subnetwork that connects storage               devices to servers. It provides block-level storage accessible to multiple servers and offers features                like high performance, scalability, and centralized storage management.</li> </ul>"},{"location":"adminsys/fs/Minio/01.Introduction/#block-storage","title":"Block storage","text":"<p>Block storage improves on the performance of file storage, breaking files into separate blocks and storing them  separately. A block-storage system will assign a unique identifier to each chunk of raw data, which can then be used  to reassemble them into the complete file when you need to access it. Block storage doesn\u2019t require a single path to  data, so you can store it wherever is most convenient and still retrieve it quickly when needed. </p> <p>Block storage works well for organizations that work with large amounts of transactional data or mission-critical  applications that need minimal delay and consistent performance. However, it can be expensive, offers no metadata  capabilities, and requires an operating system to access blocks.</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#12-how-does-object-storage-work","title":"1.2 How does object storage work?","text":"<p>With object storage, the data blocks of a file are kept together as an object, together with its relevant metadata  and a custom identifier, and placed in a flat data environment known as a storage pool.</p> <ul> <li> <p>Data Structure: Object storage organizes data as objects. Each object consists of the data itself, metadata,             and a unique identifier. The data and metadata are stored together, making it self-contained. You can also             customize metadata, allowing you to add more context that is useful for other purposes, such as retrieval             for data analytics.  </p> </li> <li> <p>Access Method: Objects are accessed via <code>RESTful APIs, HTTP, and HTTPS</code> using unique identifiers (or metadata).             These APIs allow users to perform CRUD (Create, Read, Update, Delete) operations on the stored objects.             Object storage doesn\u2019t require a file hierarchy; instead, it uses a flat structure.</p> </li> </ul> <p>There are many object storage solutions, below are some of the most popular solutions: - Azure blob storage - Amazon s3 - Google Cloud Storage - minio</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#13-what-are-the-benefits-of-object-storage","title":"1.3 What are the benefits of object storage?","text":"<p>Compare to other types of storage, object storage has the following advantages:</p> <ul> <li>Scalability: The flat environment enables you to scale quickly, even for <code>petabyte or exabyte</code> loads. Storage pools             can be spread across multiple object storage devices and geographical locations, allowing for             unlimited scale. You simply add more storage devices to the pool as your data grows.</li> <li>Metadata: Object storage systems can store extensive metadata (key value pairs) for each object, providing rich            information about the stored data.</li> <li>Reduced complexity: Object storage has no folders or directories, removing much of the complexity that comes             with hierarchical systems. The lack of complex trees or partitions makes retrieving files easier as you             don\u2019t need to know the exact location.</li> <li>Redundancy and Durability: Object storage systems often provide redundancy and data durability by replicating             objects across multiple locations or using <code>erasure coding techniques</code>.</li> </ul>"},{"location":"adminsys/fs/Minio/01.Introduction/#2-what-is-minio","title":"2 What is MinIO?","text":"<p>MinIO is an easy-to-deploy open-source object storage solution. is a <code>Kubernetes-native</code> <code>high performance</code> <code>object store with an S3-compatible API</code>. Onyxia uses it as the official S3 object storage backend.</p> <p>There are two main ways to install minio - basic linux installation official doc - k8s operator (helm chart) installation official doc</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#21-main-features","title":"2.1 Main features","text":""},{"location":"adminsys/fs/Minio/01.Introduction/#22-erasure-set-for-high-availability-and-resiliency","title":"2.2 Erasure set for high availability and Resiliency","text":"<p>You can find more information in below docs on how Minio use Erasure coding to ensure HA and Resiliency. https://min.io/docs/minio/linux/operations/concepts/erasure-coding.html#minio-erasure-coding</p> <p>https://github.com/minio/minio/blob/master/docs/erasure/README.md</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#_1","title":"introduction","text":"<pre><code>mc sql --recursive --query \"select * from S3Object\" ALIAS/PATH\n\nmc sql --recursive --query \"select * from S3Object\" minio/casd/data_science/crime.csv\n</code></pre>"},{"location":"adminsys/fs/Minio/01.Introduction/#3-install-minio-cluster","title":"3 Install minio cluster","text":"<p>The minio installation on bare metal can be found here</p>"},{"location":"adminsys/fs/Minio/01.Introduction/#4-install-minio-client","title":"4. Install minio client","text":""},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/","title":"Deploy a minio cluster in bare_metal mode","text":"<p>In this tutorial, we will deploy a minio cluster in <code>bare_metal mode</code> with <code>multi-Node</code> and <code>multi-drive</code>.</p> <p>You can find the official doc here </p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#1-prerequisites","title":"1. Prerequisites","text":""},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#11-networking-and-firewalls","title":"1.1 Networking and Firewalls","text":""},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#111-communication-between-nodes","title":"1.1.1 Communication between nodes","text":"<p>Each node should have full bidirectional network access to every other node in the cluster</p> <p>For example, the following command explicitly opens the default MinIO server API port 9000 for servers running firewalld :</p> <pre><code>firewall-cmd --permanent --zone=public --add-port=9000/tcp\nfirewall-cmd --reload\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#112-user-interface","title":"1.1.2 User interface","text":"<p>If you set a static <code>MinIO Console</code> port (e.g. :9001) you must also grant access to that port to ensure  connectivity from external clients.</p> <p>MinIO strongly recomends using a load balancer to manage connectivity to the cluster. The Load Balancer  should use a <code>Least Connections</code> algorithm for routing requests to the MinIO deployment, since any MinIO node in  the deployment can receive, route, or process client requests.</p> <p>The following load balancers are known to work well with MinIO:</p> <ul> <li>NGINX </li> <li>HAProxy</li> </ul>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#12-sequential-hostnames","title":"1.2 Sequential Hostnames","text":"<p>MinIO requires using expansion notation {x...y} to denote a sequential series of MinIO hosts when creating a  server pool. MinIO supports using either a <code>sequential series of hostnames or IP addresses</code> to represent each minio  server process in the deployment.</p> <p>This procedure assumes use of sequential hostnames due to the lower overhead of management, especially in larger  distributed clusters.</p> <p>Create the necessary DNS hostname mappings prior to starting this procedure. For example, the following hostnames  would support a 4-node distributed deployment:</p> <ul> <li>minio-01.casd.local </li> <li>minio-02.casd.local </li> <li>minio-03.casd.local </li> <li>minio-04.casd.local</li> </ul>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#13-local-jbod-storage-with-sequential-mounts","title":"1.3 Local JBOD Storage with Sequential Mounts","text":"<p>MinIO strongly recommends direct-attached JBOD(just a bunch of disks) arrays with XFS-formatted disks for best performance.</p> <p>Important point to consider:</p> <ul> <li><code>Direct-Attached Storage (DAS)</code> has significant performance and consistency advantages over <code>networked storage (NAS, SAN, NFS)</code>.</li> <li>Use XFS as file system, other file system have lower performance while exhibiting unexpected or undesired behavior.</li> <li>RAID or similar technologies do not provide additional resilience or availability benefits when used with distributed MinIO deployments, and typically reduce system performance.</li> <li>Ensure all nodes in the cluster use the same type (NVMe, SSD, or HDD) of drive with identical capacity (e.g. N TB) .    MinIO does not distinguish drive types and does not benefit from mixed storage types. Additionally. MinIO limits the size used per drive to the smallest drive in the deployment. For example, if the deployment has 15 10TB drives and 1 1TB drive, MinIO limits the per-drive capacity to 1TB.</li> </ul>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#131-mount-disk-to-nodes","title":"1.3.1 Mount disk to nodes","text":"<p>MinIO requires using expansion notation {x...y} to denote a sequential series of drives when creating  the new deployment, where all nodes in the deployment have an identical set of mounted drives. MinIO also  requires that the ordering of physical drives remain constant across restarts, such that a given mount point  always points to the same formatted drive. MinIO therefore strongly recommends using /etc/fstab or  a similar file-based mount configuration to ensure that drive ordering cannot change after a reboot. For example:</p> <pre><code>$ mkfs.xfs /dev/sdb -L DISK1\n$ mkfs.xfs /dev/sdc -L DISK2\n$ mkfs.xfs /dev/sdd -L DISK3\n$ mkfs.xfs /dev/sde -L DISK4\n\n$ nano /etc/fstab\n\n  # &lt;file system&gt;  &lt;mount point&gt;  &lt;type&gt;  &lt;options&gt;         &lt;dump&gt;  &lt;pass&gt;\n  LABEL=DISK1      /mnt/disk1     xfs     defaults,noatime  0       2\n  LABEL=DISK2      /mnt/disk2     xfs     defaults,noatime  0       2\n  LABEL=DISK3      /mnt/disk3     xfs     defaults,noatime  0       2\n  LABEL=DISK4      /mnt/disk4     xfs     defaults,noatime  0       2\n</code></pre> <p>noatime is for optimizing system performance</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#trouble-shoot","title":"Trouble shoot","text":"<p>if you can't install <code>xfs</code>, you may need to use following command to install the required package</p> Distribution Command Debian apt-get install xfsprogs Ubuntu apt-get install xfsprogs Alpine apk add xfsprogs Arch Linux pacman -S xfsprogs Kali Linux apt-get install xfsprogs CentOS yum install xfsprogs Fedora dnf install xfsprogs Raspbian apt-get install xfsprogs"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#14-time-synchronization","title":"1.4 Time synchronization","text":"<p>Multi-node systems must maintain synchronized time and date to maintain stable internode operations and interactions.  Make sure all nodes sync to the same time-server regularly. Check the doc of time_sync.</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#redundancy","title":"Redundancy","text":"<p>MinIO erasure coding is a data redundancy and availability feature that allows MinIO deployments to automatically  reconstruct objects on-the-fly despite the loss of multiple drives or nodes in the cluster.  Erasure Coding provides object-level healing with less overhead than adjacent technologies such as RAID or replication.  Distributed deployments implicitly enable and rely on erasure coding for core functionality.</p> <p><code>Erasure Coding splits objects into data and parity blocks</code>, where parity blocks support reconstruction of  missing or corrupted data blocks. The number of parity blocks in a deployment controls the deployment\u2019s relative  data redundancy. Higher levels of parity allow for higher tolerance of drive loss at the cost of total available storage.</p> <p>The default erasure coding config of MinIO is set to EC:4 (4 parity blocks per erasure set). You can set a  custom parity level by setting the appropriate MinIO Storage Class environment variable. </p> <p>Consider using the MinIO Erasure Code Calculator for guidance in  selecting the appropriate erasure code parity level for your cluster.</p> <p>Less erasure code partiy increase storage efficiency but decrease the disk failure tolerance. Choose it well for your use case.</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#15-recommended-operating-systems","title":"1.5 Recommended Operating Systems","text":"<p>The official recommended OS is here: https://min.io/docs/minio/linux/operations/installation.html#minio-installation-platform-support</p> <p>But in this tutorial, we use debian 11.</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#16-clean-existing-data","title":"1.6 Clean existing Data","text":"<p>When starting a new MinIO server in a distributed environment, the storage devices must not have existing data.</p> <p>Once you start the MinIO server, all interactions with the data must be done through the <code>S3 API</code>.  Use the  - MinIO Client: https://min.io/docs/minio/linux/reference/minio-mc.html#minio-client - MinIO Console: https://min.io/docs/minio/linux/administration/minio-console.html#minio-console - one of the MinIO Software Development Kits to work with the buckets and objects.</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#17-create-user-account-and-group","title":"1.7 Create user account and group","text":"<p>By default, the <code>minio.service</code> runs as the <code>minio-user</code> User and Group. In this tutorial, we decide to use <code>minio</code>. You can create the user and group using the groupadd and useradd commands. The following example creates the user, group,  and sets permissions to access the folder paths intended for use by MinIO. These commands typically require root (sudo) permissions.</p> <pre><code>sudo groupadd -r minio\nsudo useradd -M -r -g minio minio\n\n# change the owner of the data folder\nsudo chown minio:minio /mnt/data\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#18-create","title":"1.8 Create","text":""},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#2-deploy-the-minio-service-on-mode-single-node","title":"2. Deploy the minio service on mode single node","text":""},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#21-install-the-binary","title":"2.1 Install the binary","text":"<pre><code># get the source\nwget https://dl.min.io/server/minio/release/linux-amd64/archive/minio_20231120224007.0.0_amd64.deb -O minio.deb\n\n# install the package\nsudo dpkg -i minio.deb\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#22-configure-the-systemd-service-file","title":"2.2 Configure the systemd service file","text":"<p>The .deb or .rpm packages install the following systemd service file to <code>/usr/lib/systemd/system/minio.service</code>.  For binary installations, create this file manually on all MinIO hosts.</p> <p>The below file is an example, as we changed the user and group to minio. So we need to modify the User and Group.</p> <pre><code>[Unit]\nDescription=MinIO\nDocumentation=https://min.io/docs/minio/linux/index.html\nWants=network-online.target\nAfter=network-online.target\nAssertFileIsExecutable=/usr/local/bin/minio\n\n[Service]\nWorkingDirectory=/usr/local\n\nUser=minio-user\nGroup=minio-user\nProtectProc=invisible\n\nEnvironmentFile=-/etc/default/minio\nExecStartPre=/bin/bash -c \"if [ -z \\\"${MINIO_VOLUMES}\\\" ]; then echo \\\"Variable MINIO_VOLUMES not set in /etc/default/minio\\\"; exit 1; fi\"\nExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES\n\n# MinIO RELEASE.2023-05-04T21-44-30Z adds support for Type=notify (https://www.freedesktop.org/software/systemd/man/systemd.service.html#Type=)\n# This may improve systemctl setups where other services use `After=minio.server`\n# Uncomment the line to enable the functionality\n# Type=notify\n\n# Let systemd restart this service always\nRestart=always\n\n# Specifies the maximum file descriptor number that can be opened by this process\nLimitNOFILE=65536\n\n# Specifies the maximum number of threads this process can create\nTasksMax=infinity\n\n# Disable timeout logic and wait until process is stopped\nTimeoutStopSec=infinity\nSendSIGKILL=no\n\n[Install]\nWantedBy=multi-user.target\n\n# Built for ${project.name}-${project.version} (${project.name})\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#23-configure-minios-environment-file","title":"2.3 Configure MinIO\u2019s environment file","text":"<p>Minio service uses env var to configure the runtime. By default, we put all <code>environment variable</code> that minio needs in the /etc/default/minio.</p> <p>For the single node deployment, we only have one server </p> <pre><code># hostnames\nminio.casd.local\n# all host has only one mount point\n/mnt/data\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#3-deploy-the-minio-cluster-on-mode-multi-node","title":"3. Deploy the minio cluster on mode multi node","text":"<p>As we described before, our cluster has a single server pool consisting 3 servers with the below config:</p> <pre><code># hostnames\nminio-1.casd.local\nminio-2.casd.local\nminio-3.casd.local\n\n# all host has only one mount point\n/mnt/data\n\n# The deployment has a load balancer running at https://datalake.casd.local that manages connections across all MinIO hosts.\n</code></pre> <p>Based on the above cluster information, we can write a file </p> <pre><code># Set the hosts and volumes MinIO uses at startup\n# The command uses MinIO expansion notation {x...y} to denote a\n# sequential series.\n#\n# The following example covers four MinIO hosts\n# with 4 drives each at the specified hostname and drive locations.\n# The command includes the port that each MinIO server listens on\n# (default 9000)\n\nMINIO_VOLUMES=\"http://minio-{1...3}.casd.local:9000/mnt/data\"\n\n# Set all MinIO server options\n#\n# The following explicitly sets the MinIO Console listen address to\n# port 9001 on all network interfaces. The default behavior is dynamic\n# port selection.\n\nMINIO_OPTS=\"--console-address :9001\"\n\n# Set the root username. This user has unrestricted permissions to\n# perform S3 and administrative API operations on any resource in the\n# deployment.\n#\n# Defer to your organizations requirements for superadmin user name.\n\nMINIO_ROOT_USER=minioadmin\n\n# Set the root password\n#\n# Use a long, random, unique string that meets your organizations\n# requirements for passwords.\n\nMINIO_ROOT_PASSWORD=minio-secret-key-CHANGE-ME\n\n# Set to the URL of the load balancer for the MinIO deployment\n# This value *must* match across all MinIO servers. If you do\n# not have a load balancer, set this value to to any *one* of the\n# MinIO hosts in the deployment as a temporary measure.\nMINIO_SERVER_URL=\"https://datalake.casd.local:9000\"\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#31-run-the-minio-server-process","title":"3.1 Run the minio server process","text":"<p>In cluster mode, you need to start the minio service on all nodes. The minio service detects that MINIO_VOLUMES has multiple nodes and multiple drives, it will then coordinate the communication between the nodes automatically.</p> <pre><code>sudo systemctl start minio.service\n\n# you can check the output of the minio systemd with below command\nsudo journalctl -f -u minio.service\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#32-trouble-shoot","title":"3.2 Trouble shoot","text":"<p>If the minio service exit with error, you can debug the cluster based on the output of the minio service.</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#check-user-and-file-access-permission","title":"Check user and file access permission","text":"<p>As we mentioned in the systemd config, the user and group of the minio service is <code>minio:minio</code>. We must make sure that the permission of the mount point of the drives is correct. For example, run the below command if the permission  and owner is not correct. </p> <pre><code>chown -R minio:minio /mnt/data\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#check-the-hostname","title":"Check the hostname","text":"<p>As the minio cluster communication is through http. The hostname and <code>/etc/hosts</code> must be correctly set, so the nodes can communicate with each other</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#empty-the-drive","title":"Empty the drive","text":"<p>If there are data inside the drive, the minio service can't format the disk. As a result, the minio service will not start</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#4-test-the-cluster","title":"4. Test the cluster","text":"<p>If the minio service runs correctly. You can test the cluster</p>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#41-access-via-minio-console-web-app","title":"4.1 Access via minio console (web app)","text":"<p>You will notice that all minio nodes provides a web GU interface. You can access them via any browser. There is no notion of master and slaves. These nodes are all have the same roles.</p> <pre><code>http://10.50.5.73:9001/\nhttp://10.50.5.74:9001/\nhttp://10.50.5.75:9001/\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#42-access-via-minio-client-cli","title":"4.2 Access via minio client (CLI)","text":"<p>You can follow this guide to install the  minio client (mc).</p> <p>After installation, you can use below command to create an alias</p> <pre><code># general form\nmc alias set &lt;ALIAS&gt; &lt;HOSTNAME&gt; &lt;ACCESS_KEY&gt; &lt;SECRET_KEY&gt;\n\n# example\nmc alias set myminio http://10.50.5.73:9000 admin changeMe\n\n# check the connection\nmc admin info myminio\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#5-setup-a-load-balancer","title":"5. Setup a load balancer","text":"<p>As minio provide an api access at port 9000 and a web access(minio-console) at port 9001. The nginx conf needs to two setup a load balancer for t</p> <pre><code>server {\n    listen 80;\n    server_name datalake-console.casd.local;\n\n    # redirect http request to https\n    return 301 https://$host$request_uri;\n\n}\n\n\nserver {\n    listen 443 ssl;\n\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA256:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA';\n\n    ssl_prefer_server_ciphers on;\n    charset utf-8;\n    # To allow special characters in headers\n    ignore_invalid_headers off;\n\n    # Allow any size file to be uploaded.\n    # Set to a value such as 1000m; to restrict file size to a specific value\n    client_max_body_size 0;\n    # To disable buffering\n    proxy_buffering off;\n\n    ssl_certificate /etc/nginx/ssl/wildcard_casd.crt;\n    ssl_certificate_key /etc/nginx/ssl/wildcard_casd.key;\n\n    server_name datalake-console.casd.local;\n\n    access_log /var/log/nginx/minio_acc.log;\n    error_log /var/log/nginx/minio_err.log;\n\n    location / {\n                  charset utf-8;\n                  proxy_set_header X-Real-IP $remote_addr;\n                  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                  proxy_set_header X-Forwarded-Proto $scheme;\n                  proxy_set_header Host $http_host;\n\n                  proxy_connect_timeout 300;\n                  # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n                  proxy_http_version 1.1;\n                  proxy_set_header Connection \"\";\n                  chunked_transfer_encoding off;\n\n                  proxy_pass http://minio1.casd.local:9001;\n}\n}\n</code></pre>"},{"location":"adminsys/fs/Minio/02.Install_minio_bare_metal/#setup-a-openldap","title":"setup a openldap","text":"<pre><code># general form\nmc idp ldap policy attach &lt;alias&gt; &lt;role&gt; --user='&lt;user-dn&gt;'\n\n# example\nmc idp ldap policy attach minio consoleAdmin --user='uid=pengfei,ou=people,dc=casd,dc=org'\n</code></pre>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/","title":"Install and config a minio client","text":"<p>Minio client (mc) is a command line tool that allows you to manage your s3 projects. The official documentation for its installation is here</p>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/#1-install-minio-client-binary","title":"1. Install minio client binary","text":"<pre><code># get the minio client bin\ncurl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n  --create-dirs \\\n  -o $HOME/minio-binaries/mc\n\n# put the bin file in a folder \nchmod +x $HOME/minio-binaries/mc\n\n# add bin file in the system path\nsudo vim /etc/profile.d/minio.sh\n\n# put the below line \nexport PATH=$PATH:$HOME/minio-binaries/\n\n# test the minio client\nmc --version\n</code></pre> <p>Or you can use minio_client_install.sh</p>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/#2-configure-minio-client","title":"2. Configure minio client","text":"<p>You need to configure the minio client to make it to connect to a minio server. You can use below command to set up a connection alias. </p> <pre><code># show existing alias list\nmc alias list\n\n# general form to create a new alias\nmc alias set &lt;ALIAS&gt; &lt;HOSTNAME&gt; &lt;ACCESS_KEY&gt; &lt;SECRET_KEY&gt;\n\n# you can add --insecure to accept self signed certificate\n# for example:\nmc --insecure alias set s3 https://minio.casd.local minio mypwd --api S3v4\n\n# example to create a new alias\nmc alias set myminio http://10.50.5.73:9000 admin changeMe\n\n# check the detail of a alias\nmc admin info myminio\n</code></pre>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/#3-basic-minio-client-commands","title":"3. Basic minio client commands","text":"<pre><code># list all objects in a given bucket\nmc ls s3/&lt;bucket-name&gt;\n\n# delete\n</code></pre> <p>You can find the complete minio client command list here https://min.io/docs/minio/linux/reference/minio-mc.html?ref=docs-redirect</p>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/#36-set-anonymous-connections-policy","title":"3.6 Set anonymous connections policy.","text":"<p>Allowed values are [private, public, download, upload]</p> <pre><code>mc anonymous -r set download minio/casd/tools \n</code></pre>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/#4-set-up-an-admin-connection","title":"4. Set up an admin connection","text":"<p>The <code>minio client (mc)</code> also provide commands for performing administrative tasks on your MinIO deployments. While mc supports any S3-compatible service, <code>mc admin only supports MinIO deployments</code>.</p> <p>You can find the official doc here</p> <p>Below command will add an admin host alias</p> <p>If the minio server certificate is self-signed, you can use option --insecure to ignore certificat related errors.</p> <pre><code># the general form is:\nmc config host add &lt;ALIAS&gt; &lt;ENDPOINT&gt; &lt;ACCESS_KEY&gt; &lt;SECRET_KEY&gt;\n\nmc config host add admin-s3 http://minio.casd.local minio mypwd --api S3v4\n</code></pre>"},{"location":"adminsys/fs/Minio/03.Install_and_config_minio_client/#mc-admin-common-commands","title":"MC Admin common commands","text":"<pre><code># displays the information of a minio server.\nmc admin info admin-s3\n</code></pre>"},{"location":"adminsys/fs/Minio/04.Minio_installation_details/","title":"12 Minio tenant deployment configuration","text":"<p>In this section, we will explain the configuration file of minion tenant deployment section by section.</p>"},{"location":"adminsys/fs/Minio/04.Minio_installation_details/#121-tenant-root-credential-configuration","title":"12.1 Tenant root credential configuration","text":"<p>Below contains three secrets</p> <ol> <li>surcharge variable d'env</li> </ol> <pre><code>## Fix pour la surcharge des variables d'environnements\n## Voir https://github.com/minio/operator/issues/790#issuecomment-917476406\napiVersion: v1\ndata:\n  config.env: changeMe\nkind: Secret\nmetadata:\n  name: override-configuration\ntype: Opaque\n---\n## Secret to be used as MinIO Root Credentials\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-creds-secret\ntype: Opaque\ndata:\n  ## Access Key for MinIO Tenant, base64 encoded (echo -n 'minio' | base64)\n  accesskey: changeMe\n  ## Secret Key for MinIO Tenant, base64 encoded (echo -n '95mWXbS5sGJRjhEEU7fJvb8aXjJDZv5xnxRRwRL6' | base64)\n  secretkey: changeMe\n---\n## Secret to be used for MinIO Console\napiVersion: v1\nkind: Secret\nmetadata:\n  name: console-secret\ntype: Opaque\ndata:\n  ## Passphrase to encrypt jwt payload, base64 encoded (echo -n 'cL3zpqAv' | base64)\n  CONSOLE_PBKDF_PASSPHRASE: changeMe\n  ## Salt to encrypt jwt payload, base64 encoded (echo -n '5wDfRn3D' | base64)\n  CONSOLE_PBKDF_SALT: changeMe\n  ## MinIO User Access Key (used for Console Login), base64 encoded (echo -n 'console' | base64)\n  CONSOLE_ACCESS_KEY: chaneMe\n  ## MinIO User Secret Key (used for Console Login), base64 encoded (echo -n 'ZqYGLa3Fjcj4xWNGLUx3xeYmtFbsCDcD7c7Ls7qJ' | base64)\n  CONSOLE_SECRET_KEY: changeMe\n---\n</code></pre>"},{"location":"adminsys/fs/Minio/04.Minio_installation_details/#122-tenant-general-configuration","title":"12.2 Tenant general configuration","text":"<ul> <li> <p>Namespace: The Kubernetes Namespace in which to deploy the tenant. <code>The Operator supports at most one MinIO Tenant per namespace.</code></p> </li> <li> <p>Storage Class: Specify the Kubernetes Storage Class the minio Operator uses when generating Persistent Volume Claims for the Tenant. The default value is directpv-min-io, if you were using the DirectPV to format disk and create storage class.</p> </li> <li> <p>Number of Servers: The total number of MinIO server pods to deploy in the Tenant. The Operator by default uses <code>pod anti-affinity</code>, such that the Kubernetes cluster must have at least one worker node per MinIO server pod. Use the Pod Placement pane to modify the pod scheduling settings for the Tenant.</p> </li> <li> <p>Number of Drives per Server: The number of storage volumes (Persistent Volume Claims) the Operator requests per Server. The Operator generates an equal number of PVC plus two for supporting Tenant services (Metrics and Log Search). The specified Storage Class must correspond to a set of Persistent Volumes sufficient in number to match each generated PVC.</p> </li> <li> <p>Total Size: The total raw storage size for the Tenant. Specify both the total storage size and the Unit of that storage. All storage units are in SI values, e.g.  bytes.</p> </li> <li> <p>Memory per Node [Gi]: Specify the total amount of memory (RAM) to allocate per MinIO server pod. See Memory guidance on setting this value. The <code>Kubernetes cluster must have worker nodes with sufficient free RAM</code> to match the pod request.</p> </li> </ul> <p>Erasure Code Parity</p> <p>The Erasure Code Parity to set for the deployment.</p> <p>The Operator displays the selected parity and its effect on the deployment under the Erasure Code Configuration section. Erasure Code parity defines the overall resiliency and availability of data on the cluster. Higher parity values increase tolerance to drive or node failure at the cost of total storage. See Erasure Coding for more complete documentation.</p> <pre><code>## Set the Tenant name\ntenant:\n  name: minionyxia\n\n## Set the object storage pool specifications and tolerations\n  pools:\n      ## Number of MinIO object storage servers, same as the number of storage nodes\n    - servers: 2\n      ## custom name for the pool\n      name: casd-minio-pool-0\n      ## volumesPerServer specifies the number of volumes attached per MinIO Tenant Pod / Server.\n      ## Must match the number of drives per node\n      volumesPerServer: 1\n      ## size specifies the capacity per volume\n      size: 512Gi\n      ## storageClass specifies the storage class name to be used for this pool\n      storageClassName: directpv-min-io\n      ## Tolerations to allow scheduling on storage nodes\n      tolerations:\n        - key: \"storage-node\"\n          operator: \"Equal\"\n          value: \"true\"\n          effect: \"NoSchedule\"\n      ## Run pods specifically on storage nodes\n      nodeSelector:\n        storage-node: \"true\"\n\n## Environment variables for Onyxia's Keycloak integration\n  env:\n     # keycloak url\n    - name: MINIO_IDENTITY_OPENID_CONFIG_URL\n      value: \"https://auth.casd.local/auth/realms/casd-onyxia/.well-known/openid-configuration\"\n      # keycloak minio client id\n    - name: MINIO_IDENTITY_OPENID_CLIENT_ID\n      value: \"minio\"\n    ## Actually not used but cannot be removed for some reason\n    - name: MINIO_IDENTITY_OPENID_CLIENT_SECRET\n      value: \"wjYxvjZLaVF9NZLNnxCLD9B6JpPAl1Vx\"\n    - name: MINIO_IDENTITY_OPENID_CLAIM_NAME\n      value: \"policy\"\n    - name: MINIO_IDENTITY_OPENID_REDIRECT_URI\n      value: \"https://minio-console.casd.local/oauth_callback\"\n    - name: MINIO_IDENTITY_OPENID_SCOPES\n      value: \"openid, profile, email, roles\"\n\n## Ingress configuration to expose the API and console services\ningress:\n  api:\n    enabled: true\n    ingressClassName: \"nginx\"\n    annotations:\n      # The following annotation is required to let MinIO communicate with the NGINX Ingress controller\n      # when using external certificates. See Knowledge base: 8dc2998d-5699-4be6-bed0-b2384a87fe9e\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    tls:\n      - hosts:\n          - minio.casd.local\n    host: minio.casd.local\n    path: /\n    pathType: Prefix\n  console:\n    enabled: true\n    ingressClassName: \"nginx\"\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    tls:\n      - hosts:\n          - minio-console.casd.local\n    host: minio-console.casd.local\n    path: /\n    pathType: Prefix\n</code></pre>"},{"location":"adminsys/fs/Minio/04.Minio_installation_details/#ssl-configuration","title":"SSL configuration","text":"<p>In the context of the Datalab cluster, it is required to:</p> <ul> <li>let MinIO trust the domains established by the wildcard domain <code>*.casd.local</code>;</li> <li>provide the associated Certificate Authority (CA).</li> </ul> <p>The secrets were created in the MinIO Operator steps.</p> <p>Those secrets then need to be indicated to the MinIO tenant. This is done in the following fields of the <code>tenant-values.yaml</code> file:</p> <pre><code>## Set the certificate configuration\ntenant:\n  certificate:\n    externalCaCertSecret:\n      - name: ca-cert\n    externalCertSecret:\n      - name: minio-tls\n        type: kubernetes.io/tls\n</code></pre> <p>The above certificate config does not pass the test. We don't know why. For now we use a work around to config the certificate</p>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/","title":"Set Up an NFS Server and client on Debian 11","text":"<p>NFS, or Network File System, is a distributed file system protocol that allows you to mount remote directories on  your server. This allows you to manage storage space in a different location and write to that space from multiple  clients. NFS provides a relatively standard and performant way to access remote systems over a network and works  well in situations where the shared resources must be accessed regularly.</p> <p>In this guide, we will see how to install the NFS server and client on Debian 11.</p>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#0-prerequisites","title":"0. Prerequisites","text":"<p>You need to have two servers: - server: nfs.casd.local(10.50.5.72) - client: client.casd.local(10.50..)</p>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#1-install-the-nfs-server","title":"1. Install the nfs server","text":"<pre><code>sudo apt update\nsudo apt install nfs-kernel-server\n</code></pre>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#11-create-sharing-folder","title":"1.1 Create sharing folder","text":"<p>We need to create share folders to host the shared files. In this tutorial, we choose one folder /nfs/share.</p> <pre><code>sudo mkdir -p /nfs/share\n</code></pre> <p>Since you\u2019re creating the directory with sudo, the directory is owned by the host\u2019s root user. It's recommended to not  use root user. So we create a custom account for the share folder</p> <pre><code># add a new group nfs \nsudo groupadd nfs\n\n# add a new user with group nfs\nsudo useradd nfs -g nfs\n\n# change folder owner\nsudo chown -R nfs:nfs /nfs/share\n</code></pre>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#12-configuring-the-nfs-exports-on-the-host-server","title":"1.2 Configuring the NFS Exports on the Host Server","text":"<p>The main configuration file of the nfs server is /etc/exports, it defines which folder will be shared, and which client can access it with which rights.</p> <p>Below is the explanation of the content of the /etc/exports</p> <pre><code># general form\ndirectory_to_share    client_ip(share_option1,...,share_optionN)\n\n# some example\n/nfs/share  10.50.5.108(rw,async)\n/nfs/share  10.50.0.0/16(rw,sync,no_root_squash,no_subtree_check)\n</code></pre> <p>If we only want one client which is able to connect to nfs server, we can use the simple client IP address.</p> <p>If we want many clients, we can use an ip address with subnet mask. For example 10.50.0.0/16 means all IP within  <code>10.50.*.*</code> is authorized to access the nfs server.</p> <p>share_options: - rw: This option gives the client computer both read and write access to the volume. - sync: This option forces NFS to write changes to disk before replying. This results in a more stable and               consistent environment since the reply reflects the actual state of the remote volume. However, it also                reduces the speed of file operations. The counter part option is async - no_subtree_check: This option prevents subtree checking, which is a process where the host must check whether                  the file is actually still available in the exported tree for every request. This can cause many                   problems when a file is renamed while the client has it opened. In almost all cases, <code>it is better to                   disable subtree checking</code>. - no_root_squash: By default, NFS translates requests from a root user remotely into a non-privileged user on                 the server. This was intended as security feature to prevent a root account on the client from using                  the file system of the host as root. no_root_squash disables this behavior for certain shares.</p> <p>After the configuration, you need to restart the service to activate the new configuration</p> <p>After install, you can manage the nfs-server service with below command(nfs-kernel-server, nfs-server work both)</p> <pre><code>sudo systemctl status nfs-server \nsudo systemctl restart nfs-server\nsudo systemctl stop nfs-server\n</code></pre>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#2-install-nfs-client-on-client-machine","title":"2. Install nfs client on client machine","text":"<pre><code># Install nfs client\nsudo apt install nfs-common \n</code></pre>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#3-create-the-mount-point-folder-and-mount-the-nfs-share-folder","title":"3. Create the mount point folder and mount the nfs share folder","text":"<p>Run the below command on the client machine </p> <pre><code># Create the mount point folder\nsudo mkidr -p /mnt/nfs\n\n# general form mount the nfs share folder\nsudo mount nfs_server_url:/path/to/share path/to/mount_pont\n\n# in our example\nsudo mount 10.50.5.72:/nfs/share /mnt/nfs\n\n# check the mounted point\ndf -h\n</code></pre>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#4-test-the-nfs-access","title":"4. Test the nfs access","text":"<pre><code># read the file in nfs mount\ncat /mnt/nfs/file1.txt \n\n# try to write the file\nvim /mnt/nfs/file1.txt\n\n# try to delete the file\n</code></pre> <p>if you set the no_root_squash option, it means the client with root privilege on the local server can edit the files   in nfs server. It's not recommended by default  </p>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#5-mount-nfs-at-boot","title":"5. Mount nfs at Boot","text":"<p>We can mount the remote NFS shares automatically at boot by adding the nfs config to the /etc/fstab file on the  client machine.</p> <p>Open the /etc/fstab file with root privileges in your text editor.</p> <pre><code># edit the fstab\nsudo vim /etc/fstab\n\n# General conf form, here file_system_type is nfs\nhost_ip:/path/to/host_share_folder    /path/local_mount_folder   file_system_type  nfs_mount_options\n\n# some example\n10.50.5.72:/nfs/share /mnt/nfs nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0\n\n# you can validate the fstab conf with the below command\nsudo mount -a\n</code></pre> <p>To get all possible nfs mount options, you can use <code>man nfs</code></p>"},{"location":"adminsys/fs/NFS/Install_nfs_server_client/#6-unmount-the-nfs-file-system","title":"6. Unmount the nfs file system","text":"<pre><code>sudo unmount /mnt/nfs\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/","title":"Toolbox","text":"<p>The <code>Rook Toolbox</code> is a tool that helps you get the current state of your Ceph deployment and troubleshoot problems when they arise. It also allows you to change your Ceph configurations like enabling certain modules, creating users, or pools.</p> <p>You can find the official doc here.</p> <p>The toolbox can be started by deploying the toolbox.yaml file, which is in the <code>rook/deploy/examples/</code> directory.</p> <pre><code># deploy the pod\nkubectl apply -f toolbox.yaml\n\n# check the status of the pod\nkubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\"\n\n# get a bash shell of the toolbox pod\nkubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') bash\n</code></pre> <p>Now you can run the <code>ceph</code> command inside this shell</p> <pre><code># get the staus of the cluster\nceph status\n\n# get the status of the osd\nceph osd status\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#dashboard-access","title":"Dashboard access","text":"<p>https://rook.io/docs/rook/v1.10/Storage-Configuration/Monitoring/ceph-dashboard/#ingress-controller</p> <p>Rook automatically enables the Ceph dashboard within the cluster when deployed. However, when hosting multiple VMs in a dev environment, it is difficult to directly access it.</p> <p>Thankfully, the Datalab cluster has an Ingress controller and load-balancer set up, easing the process.</p> <p>This tutorial walks us through the required steps to make the dashboard accessible in a web browser of your host machine running the Datalab cluster.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#ingress-creation","title":"Ingress creation","text":"<p>To create an Ingress resource associated with the dashboard service, create a manifest named <code>rook-dashboard-ingress.yaml</code>. Insert the following content in it:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rook-ceph-mgr-dashboard\n  namespace: rook-ceph # namespace:cluster\n  annotations:\n    kubernetes.io/tls-acme: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/server-snippet: |\n      proxy_ssl_verify off;\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - rook-ceph.casd.local\n  rules:\n    - host: rook-ceph.casd.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: rook-ceph-mgr-dashboard\n                port:\n                  name: https-dashboard\n</code></pre> <p>You may encounter this error</p> <pre><code>Error from server (BadRequest): error when creating \"rook-dashboard-ingress.yaml\": admission webhook \"validate.nginx.ingress.kubernetes.io\" denied the request: nginx.ingress.kubernetes.io/server-snippet annotation cannot be used. Snippet directives are disabled by the Ingress administrator\n</code></pre> <p>You can disable the nginx admission webrook as a work around (not recommended for production)</p> <pre><code>kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission\n</code></pre> <p>Apply this manifest with the following command:</p> <pre><code>kubectl apply -f dashboard-ingress-https.yaml\n</code></pre> <p>Then check the Ingresses to verify that it has been created successfully and that the load-balancer's IP has been attributed to it:</p> <pre><code>kubectl get ing -n rook-ceph\n</code></pre> <p>The host machine redirecting the <code>*.casd.local</code> wildcard domain to its own IP, it is now possible to access the dashboard from it.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#accessing-the-dashboard","title":"Accessing the dashboard","text":"<p>To access the dashboard on your host machine's web browser, the cluster's CA certificate must be trusted. Doing so is described in the Onyxia usage steps.</p> <p>Open up a web browser and access <code>https://rook-ceph.casd.local</code>. You are then brought to a log-in screen for the Ceph dashboard.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#dashboard-login","title":"Dashboard login","text":"<p>The default admin account is enough to access the dashboard in a dev environment, using the following credentials:</p> <ul> <li>username: <code>admin</code></li> </ul> <p>The automatically generated default password is obtained with the following command:</p> <pre><code>kubectl -n rook-ceph get secret rook-ceph-dashboard-password \\\n -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echo\n</code></pre> <p>The default credentials are then displayed. Copy and paste them, then log in. Access to the dashboard is then granted.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#in-depth-configuration-of-the-ceph-dashboard","title":"In-depth configuration of the Ceph dashboard","text":"<p>It is possible to enable additional dashboard configuration:</p> <ul> <li>To set Rook as Ceph's orchestrator in Ceph to let the dashboard display Kubernetes-specific info;</li> <li>To modify the admin dashboard password.</li> </ul> <p>Both of those steps make use of the toolbox pod, we deploy it as follows.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#toolbox-pod","title":"Toolbox pod","text":"<p>The toolbox pod is preconfigured to modify dashboard settings easily.</p> <p>Deploy the toolbox pod using its manifest. This is automatically done in the Datalab cluster. The manifest is available from the official Rook repository:</p> <pre><code>curl -fsSL -o toolbox.yaml https://raw.githubusercontent.com/rook/rook/master/deploy/examples/toolbox.yaml\nkubectl create -f toolbox.yaml\n</code></pre> <p>The toolbox pod allow us to use <code>ceph</code> commands, useful for troubleshooting (<code>ceph status</code>) or for dashboard configuration.</p> <p>Access the toolbox pod using the following command:</p> <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#common-ceph-commands","title":"Common Ceph commands","text":"<p>Once inside the toolbox pod, you can interact with Ceph clusters and run commands. Here is a list of useful commands to check the status of your cluster.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#check-the-status-of-the-cluster","title":"Check the status of the cluster","text":"<p>```shell  bash-4.4$ ceph status</p> <p>cluster:     id:     793aa423-c779-48d0-a784-efbae1199bd3     health: HEALTH_OK</p> <p>services:     mon: 1 daemons, quorum a (age 5d)     mgr: a(active, since 5h)     mds: 1/1 daemons up, 1 hot standby     osd: 1 osds: 1 up (since 5d), 1 in (since 5d)</p> <p>data:     volumes: 1/1 healthy     pools:   4 pools, 81 pgs     objects: 129 objects, 186 MiB     usage:   560 MiB used, 511 GiB / 512 GiB avail     pgs:     81 active+clean</p> <p>io:     client:   1.2 KiB/s rd, 2 op/s rd, 0 op/s wr</p> <p>```</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#detailed-health-status","title":"Detailed Health status","text":"<p>This is useful for identifying bad physical groups that need repairs.</p> <pre><code>bash-4.4$ ceph health detail\n\n\nHEALTH_OK\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#status-of-all-osds","title":"Status of all OSDs","text":"<p>```shell bash-4.4$ ceph osd status</p> <p>ID  HOST      USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE  0  worker1   559M   511G      0        0       2      106   exists,up</p> <p>```</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#ceph-pool-details","title":"Ceph Pool details","text":"<pre><code>bash-4.4$ ceph osd pool ls detail\n\n\npool 1 '.mgr' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr\npool 2 'myfs-metadata' replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 16 pgp_num 16 autoscale_mode on last_change 37 lfor 0/0/21 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs\npool 3 'myfs-replicated' replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 23 lfor 0/0/21 flags hashpspool stripe_width 0 application cephfs\npool 4 'replicapool' replicated size 1 min_size 1 crush_rule 4 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 49 lfor 0/0/47 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#show-pool-and-total-usage","title":"Show Pool and total usage","text":"<pre><code>bash-4.4$ rados df\n\n\nPOOL_NAME           USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS       WR  USED COMPR  UNDER COMPR\n.mgr             452 KiB        2       0       2                   0        0         0     672  1.2 MiB     297  2.9 MiB         0 B          0 B\nmyfs-metadata     40 KiB       22       0      22                   0        0         0  863979  422 MiB      34   30 KiB         0 B          0 B\nmyfs-replicated      0 B        0       0       0                   0        0         0       0      0 B       0      0 B         0 B          0 B\nreplicapool      146 MiB      105       0     105                   0        0         0    3578   19 MiB   44680  450 MiB         0 B          0 B\n\ntotal_objects    129\ntotal_used       560 MiB\ntotal_avail      511 GiB\ntotal_space      512 GiB\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#setting-the-ceph-orchestrator","title":"Setting the Ceph Orchestrator","text":"<p>It is possible to set Rook as the Ceph Orchestrator backend, allowing the dashboard to provide more complete Kubernetes cluster-related information.</p> <p>In the toolbox pod, simply run the following</p> <pre><code>ceph mgr module enable rook\nceph orch set backend rook\n</code></pre> <p>From the dashboard, it is now possible to access the <code>Services</code> tab, or to display the name of the OSD's node.</p>"},{"location":"adminsys/fs/Rook_Ceph/Rook_Ceph_toolbox_and_dashboard/#dashboard-password-modification","title":"Dashboard password modification","text":"<p>From the toolbox pod, begin by moving into the home folder and creating a file containing the password you wish to use:</p> <pre><code>cd\necho p@ssword123 &gt; psswd\nceph dashboard ac-user-set-password admin -i psswd\n</code></pre> <p>This set of operations stores the password you desire to use in a local file, as the <code>ac-user-set-password</code> command sets the new password from a file. Change the content of the <code>psswd</code> file to a password of your choosing.</p> <ul> <li>This password must be secure enough to be accepted by the dashboard tool, with default settings.</li> </ul> <p>It is now possible to access the dashboard using the updated credentials.</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/","title":"CephFS with Rook","text":""},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#what-is-ceph","title":"What is Ceph?","text":"<p>Ceph is a highly scalable distributed-storage solution offering object, block, and file storage. Ceph clusters are designed to run on any hardware using the so-called <code>CRUSH algorithm</code> (Controlled Replication Under Scalable Hashing). The Ceph architecture can be pretty neatly broken into two key layers: 1. Reliable Autonomic Distributed Object Stores (aka. RADOS): is a reliable autonomic distributed object store, which provides an extremely scalable storage service for variably sized objects.</p> <ol> <li>The Ceph file system: is built on top of RADOS. Files are striped over objects, and the <code>MDS (metadata server)</code> cluster provides distributed access to a POSIX file system namespace (directory hierarchy) that\u2019s ultimately backed by more objects.</li> </ol> <p>The <code>RADOS</code> contains many components, the following components are the most important:</p> <ul> <li> <p>Ceph Monitors (aka. MONs): are responsible for maintaining the maps of the cluster required for the Ceph daemons to coordinate with each other. There should always be more than one MON running to increase the reliability and availability of your storage service.</p> </li> <li> <p>Ceph Managers (aka. MGRs): are runtime daemons responsible for keeping track of runtime metrics and the current state of your Ceph cluster. They run alongside your monitoring daemons (MONs) to provide additional monitoring and an interface to external monitoring and management systems.</p> </li> <li> <p>Ceph Object Store Devices (aka. OSDs): are responsible for storing objects on a local file system and providing access to them over the network. These are usually tied to one physical disk of your cluster. Ceph clients interact with OSDs directly.</p> </li> </ul>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#what-is-rook","title":"What is rook?","text":"<p>Rook is a set of <code>storage Operators</code> for Kubernetes that turn distributed storage systems into <code>self-managing, self-scaling, self-healing storage services</code>. It automates tasks such as <code>deployment, configuration, scaling, upgrading, monitoring, resource management</code> for distributed storage like Ceph on top of Kubernetes. It has support for multiple storage providers like Ceph, EdgeFS, Cassandra, NFS, Yugabyte DB, and CockroachDB \u2013 via a Kubernetes Operator for each one.</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#51-prerequisites-for-rook","title":"5.1 Prerequisites for Rook","text":"<p>Rook can be installed on any existing Kubernetes cluster as long as it meets the minimum version and Rook is granted the required privileges</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#511-cpu-architecture","title":"5.1.1 CPU Architecture","text":"<p>Supported CPU Architectures are <code>amd64 / x86_64 and arm64</code>.</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#512-hard-drive-check","title":"5.1.2 Hard drive check","text":"<p>In order to configure the Ceph storage cluster, at least one of these local storage options are required:</p> <ul> <li>Raw devices (no partitions or formatted filesystems)</li> <li>Raw partitions (no formatted filesystem)</li> <li>LVM Logical Volumes (no formatted filesystem)</li> <li>Persistent Volumes available from a storage class in block mode</li> </ul> <p>Use below command to check your hard drive status</p> <pre><code>lsblk -f\nNAME   FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINT\nsda\n\u251c\u2500sda1 vfat   FAT32       B052-39EE                             507.6M     1% /boot/efi\n\u2514\u2500sda2 ext4   1.0         7bbbaa47-535d-4e32-968d-ea078a0dd460  110.4G     5% /\nsdb\n</code></pre> <p>If the FSTYPE field is not empty, there is a filesystem on top of the corresponding device. In the above example, you can only use the <code>sdb</code> device.</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#513-admission-controller","title":"5.1.3 Admission Controller","text":"<p>Enabling the <code>Rook admission controller</code> is recommended to provide an additional level of validation that Rook is configured correctly with the custom resource (CR) settings. An admission controller intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p> <p>To deploy the Rook admission controllers, install the <code>cert manager</code> before Rook is installed.</p> <p>Run below command to install cert manager</p> <pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.yaml\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#514-lvm-package","title":"5.1.4 LVM package","text":"<p>Base on your installation scenario you may need to install the lvm package</p> <pre><code># centos\nsudo yum install -y lvm2\n\n# debian\nsudo apt install -y lvm2\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#515-linux-kernel","title":"5.1.5 Linux Kernel","text":"<p>The recommended minimum kernel version is 4.17, and the kernel must be built with the RBD module</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#52-detailed-installation","title":"5.2 Detailed installation","text":""},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#521-step1-installing-rook-operator","title":"5.2.1 Step1 : Installing Rook operator","text":"<p>The base Rook operator and resources must first be installed in the cluster. Two install methods exist:</p> <ul> <li>applying manifests (kubectl apply -f *.yaml)</li> <li>applying Helm chart (helm install .)</li> </ul> <p>In this tutorial we use the manifest option.  The Onyxia repository can be found here</p> <p>There are four important files: - crds.yaml : resource for creating the rook CustomResourceDefinition - common.yaml # resource for creating prerequisites of rook operator (e.g. roles)  - operator.yaml # resource for deploying rook operator - cluster.yaml # create a ceph cluster by using rook operator</p> <p>You can download the latest ones from the git repository.</p> <p>Below command will get the v1.10.1</p> <pre><code>git clone --single-branch --branch v1.10.1 https://github.com/rook/rook.git\n\n# you can find all *.yaml in rook/deploy/examples\nls rook/deploy/examples\n</code></pre> <p>You need to edit these four files to adapt your own system. For example, the cluster-test.yaml will create a test rook cluster(1 node is enough). The cluster.yaml will create a production ready rook cluster(3 node minimun).</p> <pre><code># create the rook operator\nkubectl apply -f crds.yaml  -f common.yaml -f operator.yaml\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#522-step2-creating-a-ceph-cluster","title":"5.2.2 Step2 : Creating a Ceph cluster","text":"<p>You can find a complete example for production cluster.yaml (requires 3 node).  For test purporse, you can use cluster-test.yaml (requires 1 node only).</p> <p>To deploy a Ceph cluster with an appropriate manifest, run below command</p> <pre><code># deploy a ceph cluster by using rook operator\nkubectl apply -f cluster.yaml\n</code></pre> <p>As this manifest is the core of the ceph cluster, We will explain the manifest line by line.</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#5221-header","title":"5.2.2.1 Header","text":"<p>In k8s, every manifest must have a header which contains: - apiVersion: The version of k8s culster that the manifest is designed to run with - kind: the object type which the k8s mantifest will deploy - metadata: It specifies the name and namespace </p> <p>Below is an example</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#5222-spec","title":"5.2.2.2 Spec","text":""},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#image-version","title":"Image version","text":"<pre><code>dataDirHostPath: /var/lib/rook # specify the data directory where configuration files will be persisted\ncephVersion:\n  image: quay.io/ceph/ceph:v17 # the ceph version that will be deployed\n  allowUnsupported: true # allow unsupported Ceph version\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#basic-component","title":"basic Component","text":"<pre><code>mon:\n  count: 1 # configure the number of Ceph Monitors should be 3 for production\n  allowMultiplePerNode: true\nmgr:\n  count: 1 # configure the number of Ceph manager \n  allowMultiplePerNode: true\ndashboard: # enable the ceph dashboard\n  enabled: true\n  # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)\n  # urlPrefix: /ceph-dashboard\n  # serve the dashboard at the given port.\n  # port: 8443\n  # serve the dashboard using SSL\n  ssl: false \n</code></pre> <p>The <code>storage</code> key lets you define the cluster level storage options; for example, which node and devices to use, the database size, and how many OSDs to create per device:</p> <pre><code>storage:\n  useAllNodes: true\n  useAllDevices: true\n  config:\n    # metadataDevice: \"md0\" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.\n    # databaseSizeMB: \"1024\" # uncomment if the disks are smaller than 100 GB\n    # journalSizeMB: \"1024\"  # uncomment if the disks are 20 GB or smaller\n</code></pre> <p>The <code>disruptionManagement</code> key to manage daemon disruptions during upgrade or fencing:</p> <pre><code>disruptionManagement:\n  managePodBudgets: false\n  osdMaintenanceTimeout: 30\n  manageMachineDisruptionBudgets: false\n  machineDisruptionBudgetNamespace: openshift-machine-api\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#advance-component","title":"Advance component","text":"<p><code>RDB</code> stands for RADOS (Reliable Autonomic Distributed Object Store) block device, which are thin-provisioned and resizable Ceph block devices that store data on multiple nodes.</p> <p><code>RBD images</code> can be asynchronously shared between two Ceph clusters by enabling rbdMirroring. Since we\u2019re working with one cluster in this tutorial, this isn\u2019t necessary. The number of workers is therefore set to 0</p> <pre><code>rbdMirroring:\n  workers: 0\ncrashCollector: # enable the crash collector for the Ceph daemons\n  disable: false\ncleanupPolicy: # The cleanup policy is only important if you want to delete your cluster. That is why this option has to be left empty:\n  deleteDataDirOnHosts: \"\"\nremoveOSDsIfOutAndSafeToRemove: false\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#523-installing-storage-on-top-of-the-rook-ceph-cluster","title":"5.2.3 Installing Storage on top of the rook-ceph cluster","text":"<p>Ceph provide three types fo storage:</p> <ul> <li>Block: Create block storage to be consumed by a pod (RWO)</li> <li>Shared Filesystem (CephFs): Create a filesystem to be shared across multiple pods (RWX)</li> <li>Object: Create an object store that is accessible inside or outside the Kubernetes cluster</li> </ul>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#5231-block-default-for-onyxia","title":"5.2.3.1 Block (default for onyxia)","text":"<p>To deploy a Ceph block storage, you can use the two example manifest: - for proudction: cephblock_storageclass.yaml - for test: cephblock_test_storageclass.yaml</p> <p>The version that Onyxia uses can be found here</p> <p>These two manifest will create a <code>CephBlockPool</code> and a rook-ceph-block StorageClass. </p> <p>Must set the ceph-block StorageClass as default for onyxia to use it</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\" # this line is essential for Onyxia to work\n  name: rook-ceph-block\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#5232-cephfs","title":"5.2.3.2 CephFs","text":"<p>To deploy a CephFs storage, you can use the two example manifest: - filesystem.yaml will create a production level cephFS (requires 3 Ceph Object Storage Daemons (OSDs)). - filesystem-test.yaml will create a test cephFS( 1 osd is enough).</p> <p>Apply this file's content with the following command:</p> <pre><code>kubectl apply -f filesystem-test.yaml\n</code></pre> <p>You can check if the pods are created correctly with below command</p> <pre><code># To confirm the filesystem is configured, wait for the mds pods to start\nkubectl -n rook-ceph get pod -l app=rook-ceph-mds\n\n# You should see below output if everything is ok\nNAME                                    READY   STATUS    RESTARTS   AGE\nrook-ceph-mds-myfs-a-7bdbb9c64f-78pnj   1/1     Running   0          5m26s\nrook-ceph-mds-myfs-b-6c7b84897c-rn9kx   1/1     Running   0          5m24s\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#creat-a-storageclass-based-on-cephfs","title":"Creat a StorageClass based on cephfs","text":"<p>Now, we have a CephFS, but pods can't use it directly. We need to create a StorageClass, which is needed for Kubernetes to interoperate with the CSI driver to create persistent volumes.</p> <p>You can use cephfs_storageclass.yaml as example to creat a storage class for cephfs.</p> <pre><code>kubectl apply -f cephfs_storageclass.yaml\n</code></pre>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#53-test-your-installation","title":"5.3 Test your installation.","text":"<p>You can use the ceph toolbox to test your ceph cluster.</p> <p>For more details, please visit 06.Rook_Ceph_toolbox_and_dashboard.md.</p>"},{"location":"adminsys/fs/Rook_Ceph/rook_ceph_Installation/#54-add-new-node","title":"5.4 Add new node","text":"<p>To add a new osd node inside the ceph cluster: 1. Prepare the physical disk on the host as we described in section 5.1.2 2. Restart the rook-ceph-operator pod. </p> <p>I was expecting <code>rook-ceph operator</code> to automatically add/remove ceph osds if we have set <code>useAllNodes: true</code> in <code>cluster.yaml</code>. But no, view this issue. So you need to restart the pod to add new nodes.</p> <p>Below command is an example on how to restart the rook-ceph operator</p> <pre><code>kubectl rollout restart deployment rook-ceph-operator -n rook-ceph\n</code></pre>"},{"location":"adminsys/os_setup/01.Lang_support/","title":"Debian server language support management","text":""},{"location":"adminsys/os_setup/01.Lang_support/#1-add-new-language-support","title":"1. Add new language support","text":"<pre><code># check installed language\nsudo locale -a\n\n# install a new language support\nsudo locale-gen en_US.UTF-8\n</code></pre>"},{"location":"adminsys/os_setup/01.Lang_support/#2-change-the-default-language-support","title":"2. Change the default language support","text":"<pre><code>sudo vim /etc/default/locale\n\n# add the below lines\nLANG=en_US.UTF-8\nLANGUAGE=\"en_US:en\"\nLC_ADDRESS=en_US.UTF-8\nLC_NAME=en_US.UTF-8\nLC_MONETARY=en_US.UTF-8\nLC_PAPER=en_US.UTF-8\nLC_IDENTIFICATION=en_US.UTF-8\nLC_TELEPHONE=en_US.UTF-8\nLC_MEASUREMENT=en_US.UTF-8\nLC_TIME=en_US.UTF-8\nLC_NUMERIC=en_US.UTF-8\n\n\n# load the new config\nsource /etc/default/locale\n\n# update the language support by using the default config\nsudo update-locale LANG=en_US.UTF-8\n</code></pre>"},{"location":"adminsys/os_setup/02.Add_debian_backports_repo/","title":"Add debian backports repo","text":"<p>The </p> <pre><code># add the backports repo to the source.list.d dir\necho \"deb http://deb.debian.org/debian bullseye-backports main\" | sudo tee /etc/apt/sources.list.d/bullseye-backports.list\n\n# update the repo index\nsudo apt update\n\n# install a package by using the backports repo\nsudo apt -t bullseye-backports install openssl\n</code></pre>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/","title":"Remote desktop access","text":"<p>If your linux runs on cloud, and you wish access this server's desktop interface, the below tutorial can help you. I have tested three solutions: - X2go: https://wiki.x2go.org/doku.php - Nomachine: https://www.nomachine.com/ - xrdp:</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#x2go-and-nomachine","title":"X2go and nomachine","text":"<p>If you have no other choice, these two solutions can be your last hope. They both have many bugs. The screen resolution for example is a pain in the ass to setup correctly. Nomachine also requires admin rights on both client and server.</p> <p>You can follow their official doc to install them.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#xrdp","title":"Xrdp","text":"<p>xrdp is a free and open-source implementation of <code>Microsoft RDP (Remote Desktop Protocol) server</code> that enables operating  systems other than Microsoft Windows (such as Linux and BSD-style operating systems) to provide a fully functional  <code>RDP-compatible remote desktop</code> experience. It works by bridging graphics from the X Window System to the  client and relaying controls from the client back to X Window Server.</p> <p>The initial versions of the XRDP project relied on a <code>local VNC server installation</code>. Due to the <code>slow performance</code> of  forwarding to a VNC server, the developers introduced the X11rdp mode,  resulting in improved draw times and  an overall better user experience. In 2019, the XRDP developers announced the xorgxrdp project as the replacement  to the <code>X11rdp mode</code>, which is the default mode that XRDP uses in new installations.</p> <p>You can visit their github page, if you are interested in how it works.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#xrdp-server-side-installation","title":"Xrdp server side installation","text":"<p>This tutorial is tested under ubuntu 24.</p> <pre><code># Step 1 \u2013 Update Ubuntu\nsudo apt-get update -y\n\nsudo apt-get upgrade #optional\n\n# Step 2 \u2013 Install XRDP\nsudo apt install xrdp -y\nsudo systemctl status \n\n# Step 3 \u2013 Configure SSL\n# xrdp daemon is lanuched by the service account xrdp, it requires certain privilege to access SSL/TLS certificates stored on the system.\n# The below command add user xrdp to the group ssl-cert\nsudo adduser xrdp ssl-cert\n\n# restart the deamon\nsudo systemctl restart xrdp\n\n# Add a Firewall Rule to allow inbound and outbound traffic on port 3389\nsudo ufw allow from 192.168.0.0/24 to any port 3389\nsudo ufw allow 3389\nsudo ufw reload \n\n# Step 4 \u2013 Test the XRDP connection\n</code></pre>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#xrdp-client-side-installation-and-configuration","title":"Xrdp client side installation and configuration","text":"<p>The client side installation happens on the pc which you want to use to connect to the server.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#for-windows","title":"For Windows:","text":"<ol> <li>Search for \u201cRemote Desktop Connection\u201d: You can do this by typing \u201cRemote Desktop Connection\u201d in the Windows search bar.</li> <li>Open the Remote Desktop Connection application: Click on the application in the search results. You should see below GUI. </li> </ol> <p> 3. Enter the Computer\u2019s IP Address or Hostname: In the Remote Desktop Connection window, you\u2019ll need to enter the  IP address or hostname of the computer you want to connect to. 4. Click \u201cConnect\u201d: Once you\u2019ve entered the required information, click the \u201cConnect\u201d button. When you get the certificate warning, click YES.</p> <p> 5. Enter Credentials: You\u2019ll be prompted to enter the username and password for the computer you are connecting to.  Ensure you have the correct credentials. The session value should be Xorg. If you choose <code>xvnc</code>, you need to install the required packages on the server side. 6. Connect: After entering the credentials, click \u201cOK\u201d or \u201cConnect\u201d to establish the RDP connection.</p> <p></p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#for-mac","title":"For Mac","text":"<ol> <li>Download Microsoft Remote Desktop from the App Store: If you don\u2019t have it already, you can download the Microsoft  Remote Desktop application from the Mac App Store.</li> <li>Open Microsoft Remote Desktop: Once installed, open the application.</li> <li>Click on \u201cNew\u201d: To create a new connection, click on the \u201c+\u201d icon or select \u201cNew\u201d from the File menu.</li> <li>Enter Connection Details: Enter the PC name or IP address, and configure other settings as needed.</li> <li>Save the Connection: Click \u201cAdd\u201d to save the connection.</li> <li>Connect: Select the newly created connection and click \u201cStart\u201d to initiate the RDP session.</li> </ol>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#for-linux","title":"For linux","text":"<p>You can download various rdp clients: - FreeRDP - rdesktop - KRDC - NeutrinoRDP</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#troubleshoot","title":"Troubleshoot","text":"<p>To debug <code>xrdp</code>, you can get the log from <code>/var/log/xrdp.log</code>.</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#wrong-credential","title":"Wrong credential","text":"<p>If you get this message <code>login failed</code>, you have mistyped your details or logged in to an active session  (someone else is logged on with the same login). Linux allows multiple console connections, but only one GUI desktop connection. You need to close other GUI desktop connection</p>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#black-screen","title":"Black screen","text":"<p>For recent linux version(ubuntu 22, 24), you may get a BLACK screen. Because xrdp uses the Xorg(implementation of the X11 protocol of the X window System) to do the remote display. But ubuntu 22, 24 use Wayland, so it misses some important packages. </p> <p>Below command will install dbus-x11 package, which provides a D-Bus session bus for each X11 display.</p> <pre><code>sudo apt-get install dbus-x11\n\n# a script will be generated, normally you don't need to modify it. \nsudo nano /etc/xrdp/startwm.sh\n\n# just check if it matches with the below content\nif test -r /etc/profile; then\n        . /etc/profile\nfi\n\nif test -r ~/.profile; then\n        . ~/.profile\nfi\n\ntest -x /etc/X11/Xsession &amp;&amp; exec /etc/X11/Xsession\nexec /bin/sh /etc/X11/Xsession\n\n\n# if everything matches, just reboot the server\nsudo reboot\n</code></pre>"},{"location":"adminsys/os_setup/03.Remote_Desktop_access/#custom-xrdp-port","title":"custom xrdp port","text":"<p>The xrdp server listens for incoming RDP connections on port number 3389 by default. </p> <p>To instruct xrdp to listen on a different port, you need to edit the <code>xrdp.ini</code></p> <pre><code>sudo vim /etc/xrdp/xrdp.ini\n\n# Locate the port directive in the [Globals] section and set the desired value. In this example, the RDP port is 49952:\nport=49952\n\n# save the file and restart the daemon\nsudo systemctl restart xrdp\n\n# don't forget change the firewall if you have one\nsudo ufw status\nsudo ufw allow 49952/tcp\nsudo ufw reload\n</code></pre> <p>For the client side, you need to specify the port number inside the <code>computer ip input</code> with the form IP_address:port_number.</p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/","title":"Debian security updates automation","text":""},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#1-apply-updates-manually","title":"1. Apply updates manually","text":"<pre><code># fetch repo updates\nsudo apt-get update\n\n#  list all available upgrades\nsudo apt list --upgradable\n\n# install updates\nsudo apt-get upgrade\n\n# clean outdated package cache:\nsudo apt-get autoclean\n\n# clean unnecessary dependencies:\nsudo apt autoremove -y\n\n# check the integrity of the apt-get, this the advance feature which is not implemented in apt. So you need to type apt-get\nsudo apt-get check\n\n# try to fix \nsudo apt --fix-broken install\n</code></pre> <p>If your linux kernel is updated, we recommend you to reboot your OS to check if everything is ok</p> <pre><code># restart \nsudo shutdown -r now\n\n# show the kernel version\nuname -mrs\n</code></pre> <p>A script which can automate the process via cron job</p> <pre><code>#!/bin/bash\nexport NEEDRESTART_MODE=a\nexport DEBIAN_FRONTEND=noninteractive\n## Questions that you really, really need to see (or else). ##\nexport DEBIAN_PRIORITY=critical\napt-get -qy clean\napt-get -qy update\napt-get -qy -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\" upgrade\n</code></pre> <p>You can notice, we set special shell variable named DEBIAN_FRONTEND, NEEDRESTART_MODE, and DEBIAN_PRIORITY to  avoid issues when running task in the backround via cron job.</p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#2-use-the-unattended-upgrades-package","title":"2. Use the unattended-upgrades package","text":"<p>There is a package called unattended-upgrades, which can install the security updates automatically in the background. We also recommend two more packages: - apt-listchanges: can compare a new package version with the one currently installed and show what has been                         changed by extracting the relevant entries from the Debian changelog and NEWS files. - bsd-mailx: traditional simple command-line-mode mail user agent</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n\n# install the packages\nsudo apt install unattended-upgrades apt-listchanges bsd-mailx\n\n# remove old conf and generate default conf\nsudo dpkg-reconfigure unattended-upgrades\n\n# Select \"Yes\" when prompted to enable automatic updates.\n</code></pre> <p>The objective of the three tools, <code>unattended-upgrades</code> install the updates, <code>apt-listchanges</code> log the changes  during the update, <code>bsd-mailx</code> send the log to user mail box.</p> <p>You can control the <code>unattended-upgrades</code> daemons with the below command.</p> <pre><code>systemctl start unattended-upgrades # start the service\nsystemctl stop unattended-upgrades # stop the service\nsystemctl restart unattended-upgrades # restart the service\nsystemctl enable unattended-upgrades # enable at boot time\nsystemctl disable unattended-upgrades # disable at boot time\nsystemctl status unattended-upgrades # get the status\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#21-configure-the-unattended-upgrades-daemon","title":"2.1 Configure the unattended-upgrades daemon","text":"<p>There are two important conf files for <code>unattended-upgrades</code> daemon: - /etc/apt/apt.conf.d/50unattended-upgrades: it's auto generated after the installation of <code>unattended-upgrades</code> - /etc/apt/apt.conf.d/20auto-upgrades: You need to add it manually or call <code>sudo dpkg-reconfigure -plow unattended-upgrades</code>                                       to generate this config file</p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#211-50unattended-upgrades","title":"2.1.1 50unattended-upgrades","text":"<p>This conf file set up the package repo origin. Below is an example</p> <pre><code># open the conf file\nsudo vim /etc/apt/apt.conf.d/50unattended-upgrades\n\n    \"origin=Debian,codename=${distro_codename},label=Debian\";\n    \"origin=Debian,codename=${distro_codename},label=Debian-Security\";\n    \"origin=Debian,codename=${distro_codename}-security,label=Debian-Security\";\n</code></pre> <p>You can skip packages by using blacklist</p> <pre><code>// Use python regular expression\n// \nUnattended-Upgrade::Package-Blacklist {\n    \"nginx\";\n        \"linux-image*\";\n};\n</code></pre> <p>You need to configure an email address to get email when there is a problem or package upgrades:</p> <pre><code>Unattended-Upgrade::Mail \"notify@server1.cyberciti.biz\";\n# Or at least send it to root user on the same system:\n# You can access root mail from /var/mails via root account\nUnattended-Upgrade::Mail \"root\";\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#212-enable-auto-cleanup-of-old-packages","title":"2.1.2 Enable Auto-Cleanup of Old Packages","text":"<p>After auto upgrades, we can also remove old unused packages</p> <pre><code>sudo vim /etc/apt/apt.conf.d/50unattended-upgrades\n\n# enable this line\nUnattended-Upgrade::Remove-Unused-Kernel-Packages \"true\";\nUnattended-Upgrade::Remove-Unused-Dependencies \"true\";\n\n# we don't recommend auto reboot at all\nUnattended-Upgrade::Automatic-Reboot \"false\";  # Reboots automatically if required\n# if you set auto reboot to true, you need also set the reboot time\nUnattended-Upgrade::Automatic-Reboot-Time \"03:00\";  # Set the reboot time (change as needed)\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#213-enable-periodic-updates","title":"2.1.3 Enable Periodic Updates","text":"<p>/etc/apt/apt.conf.d/20auto-upgrades</p> <p>This config file activates the <code>unattended-upgrades</code> daemon. It also sets how often the apt clean the unnecessary packages.</p> <p>We recommend you add at least the below three lines in this config file.</p> <pre><code># Update-Package-Lists is like apt update, you can choose 0, 1, 2, etc\n# \"0\" : Disable automatic updates.\n# \"1\" : Update package lists daily.\n# \"2\" : Update every 2 days, etc.\n# in our case, it runs every 7 days\nAPT::Periodic::Update-Package-Lists \"7\";\n\n# like apt upgrade\nAPT::Periodic::Unattended-Upgrade \"7\";\n\n# set how often the clean will be done\nAPT::Periodic::AutocleanInterval \"15\";\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#22-configure-the-apt-listchanges","title":"2.2. Configure the apt-listchanges","text":"<p>The main config file of this daemon is  <code>/etc/apt/listchanges.conf</code>. Below is an example</p> <pre><code>[apt]\nfrontend=pager\nwhich=news\nemail_address=root\nemail_format=text\nconfirm=false\nheaders=false\nreverse=false\nsave_seen=/var/lib/apt/listchanges.db\n</code></pre> <p>change the mail_address if you want to redirect the mail to another mail box. </p>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#23-test-your-installation","title":"2.3 Test your installation","text":"<pre><code>sudo unattended-upgrades --dry-run --debug\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#3-view-and-config-the-upgrade-schedules","title":"3. View and config the upgrade schedules","text":"<p>In debian <code>Debian 11/10</code> Unattended Upgrades daemon uses <code>systemd timer</code> to schedules the updates.  To view schedule value, use the below command</p> <pre><code># schedules used for download packages\nsystemctl cat apt-daily.timer \n\n# output example\n# /lib/systemd/system/apt-daily.timer\n[Unit]\nDescription=Daily apt download activities\n\n[Timer]\nOnCalendar=*-*-* 6,18:00\nRandomizedDelaySec=12h\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n\n\n# schedules used for upgrade packages\nsystemctl cat apt-daily-upgrade.timer\n\n# output example\n# /lib/systemd/system/apt-daily-upgrade.timer\n[Unit]\nDescription=Daily apt upgrade and clean activities\nAfter=apt-daily.timer\n\n[Timer]\nOnCalendar=*-*-* 6:00\nRandomizedDelaySec=60m\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#31-modify-the-default-schedules","title":"3.1 Modify the default schedules","text":"<p>Edit the schedules used for download packages</p> <pre><code>systemctl edit apt-daily.timer \n# restart the service\nsudo systemctl restart apt-daily.timer \n# check the status\nsystemctl status apt-daily.timer \n</code></pre> <p>Edit the schedules used for upgrade packages</p> <pre><code>systemctl edit apt-daily-upgrade.timer\nsudo systemctl restart apt-daily-upgrade.timerr\nsystemctl status apt-daily-upgrade.timer\n</code></pre>"},{"location":"adminsys/os_setup/05.Debian_security_update_automation/#4-trouble-shoot","title":"4. Trouble shoot","text":"<p>If you encounter problems, you can check the log of the <code>unattended-upgrades</code> daemon. </p> <pre><code>tail -f /var/log/unattended-upgrades/unattended-upgrades-shutdown.log\n</code></pre>"},{"location":"adminsys/os_setup/06.Accept_private_ca_certificate/","title":"Accept private ca certificate","text":"<p>If you want your server to accept custom certificates, you can follow the below steps. There are two scenarios:  1. You have a self-signed certificate  2. You have a certificate which signed by a CA, but the CA is not recognized by the server by default.</p> <p>For <code>scenario 1</code>, you just copy the self-signed certificate. For <code>scenario 2</code>, you should copy the root CA certificate, so all the certificate signed by this CA will be accepted in the future.</p>"},{"location":"adminsys/os_setup/06.Accept_private_ca_certificate/#1-convert-certificate-to-accepted-format","title":"1. Convert certificate to accepted format","text":"<p>Debian only accepts certificate of format .crt or .pem. If your certificate is in other formats, you need to convert them into the accepted format</p> <pre><code># convert der to crt\nopenssl x509 \\\n       -inform der -in domain.der \\\n       -out domain.crt\n\n# convert pcks7 to crt\nopenssl pkcs7 \\\n       -in domain.p7b \\\n       -print_certs -out domain.crt\n\n# convert pkcs12 to crt\nopenssl pkcs12 \\\n       -in domain.pfx \\\n       -nodes -out domain.combined.crt\n</code></pre> <p>Certain certificate format contains also the private key, so pay attention on the output file, don't leak the private key.</p>"},{"location":"adminsys/os_setup/06.Accept_private_ca_certificate/#2-add-certificate-as-trusted","title":"2. Add certificate as trusted","text":"<pre><code># to keep track off the custom ca we create a sub-folder\nsudo mkdir /usr/local/share/ca-certificates/casd-ca\n\n# copy the certificate\nsudo cp your-ca.crt /usr/local/share/ca-certificates/casd-ca/.\n\n# ask debian to load the new certificate\nsudo update-ca-certificates\n\n# test it with a site which uses the certificate or signed by the certificate\ncurl https://target-url\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/","title":"Configure a shared folder in linux","text":"<p>The goal of this tutorial is to show how to set up a shared folder for all users. Users must be able to access data inside this folder (read write and execute by default) without the owner of the data changing the acl manually.</p> <p>The command such as <code>cp, mv</code> conserve the origin ACL of the data, so even the default ACL of the shared folder allows all users to access the data, but if the data is created in another folder and copied in the shared folder, by default the data conserves the origin ACL. As a result, the data may not be accessible </p> <p>The idea is : 1. create a shared folder called <code>/home/common</code> 2. set default ACL to o::rwx (give others read, write rights.) 3. set up a systemd to auto change ACL, when copy or move data to the shared folder</p>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#1-create-the-shared-folder","title":"1. Create the shared folder","text":"<pre><code># the owner and group will be root:root\nsudo mkdir /home/common\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#2-setup-default-acl-for-the-shared-folder","title":"2. Setup default ACL for the shared folder","text":"<p>Run the below command to install the required packages</p> <pre><code># install required packages\nsudo apt update\n\nsudo apt install inotify-tools acl -y\n</code></pre> <ul> <li>acl: offers more options than basic chmod</li> <li>inotify-tools: overwatch a folder, when a waiting event happens, it can trigger target actions</li> </ul> <p>Configure default ACL </p> <pre><code># by default we grant full access for others. For the owner and group, the origin ACL will be conserved.\nsudo setfacl -d -m o::rwx /home/common\n</code></pre> <p>After this step, all files and folders created in the shared folder will inherit the default ACL </p>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#3-configure-a-systemd-daemon-to-auto-update-acl","title":"3. Configure a systemd daemon to auto update ACL","text":""},{"location":"adminsys/os_setup/07.Setup_shared_folder/#31-create-the-daemon-script","title":"3.1 Create the daemon script","text":"<p>Create the daemon script in <code>/usr/local/bin</code></p> <pre><code># choose your favorite editor\nsudo vim /usr/local/bin/update_acl.sh\n</code></pre> <p>Copy the below script in the file</p> <pre><code>#!/bin/bash\n\n# the dir which the daemon will watch\nWATCH_DIR=\"/home/common\"\n# the ACL will be enforced by the daemon\nACL_PERMISSIONS=\"o::rwx\"\n\ninotifywait -m -r -e close_write,moved_to,create \"$WATCH_DIR\" --format \"%w%f\" |\nwhile read NEWITEM; do\n  # check if the new coming item is a directory or a file\n    if [ -d \"$NEWITEM\" ]; then\n        echo \"Fixing ACL for new directory: $NEWITEM\"\n        # -R means recursively update the ACL of the new directory.\n        setfacl -R -m \"$ACL_PERMISSIONS\" \"$NEWITEM\"\n        # -d sets default ACL so future files in the new directory inherit correct permissions.\n        setfacl -d -m \"$ACL_PERMISSIONS\" \"$NEWITEM\"\n    else\n        echo \"Fixing ACL for new file: $NEWITEM\"\n        setfacl -m \"$ACL_PERMISSIONS\" \"$NEWITEM\"\n    fi\ndone\n</code></pre> <p>make the script executable</p> <pre><code>sudo chmod +x /usr/local/bin/update_acl.sh\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#32-create-the-systemd-daemon-launcher-for-update_aclsh","title":"3.2 Create the systemd daemon launcher for update_acl.sh","text":"<p>The systemd daemon launcher must be located at <code>/etc/systemd/system/</code>. By convention, we name it as <code>update_acl.service</code></p> <p>Open the file with your favorite editor</p> <pre><code>sudo vim /etc/systemd/system/update_acl.service\n</code></pre> <p>Copy the below lines in the file</p> <pre><code>[Unit]\nDescription=Update ACLs for date copied to shared directory\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/update_acl.sh\nRestart=always\nUser=root\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#33-enable-the-systemd-daemon","title":"3.3 Enable the systemd daemon","text":"<pre><code># reload the daemon list from the repository\nsudo systemctl daemon-reload\n\n# enable the service for startup\nsudo systemctl enable update_acl.service\n\n# start the service \nsudo systemctl start update_acl.service\n\n# check the satus\nsudo systemctl status update_acl.service\n\n# stop the service\nsudo systemctl stop update_acl.service\n</code></pre>"},{"location":"adminsys/os_setup/07.Setup_shared_folder/#4-test-the-solution","title":"4. Test the solution","text":"<p>After the above steps, you need to login to the server with two different users: - user1 - user2</p> <p>user1 actions</p> <pre><code>#  create a file in his home\ntouch ~/test1.txt\n\n# set the acl to owner only, \nchmod 0700 ~/test.txt\n\n# copy the file to the /home/common\ncp ~/test.txt /home/common\n\n# create a file directly in the shared folder\ncd /home/common\n\n# create a file\ntouch test2.txt\n</code></pre> <p>user2 actions</p> <pre><code># go to the share folder\ncd /home/common\n\n# list the existing files\nls -lah\n\n# show the content of test1 and test2\ncat test1.txt\ncat test2.txt\n</code></pre> <p>If user2 can show the content, it means the daemon works well. If user2 see <code>permission deny</code>, it means something went wrong. Call admin linux</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/","title":"Upgrade Debian from 11 to 13","text":"<p>If you are under debian 11, and you want to upgrade to 13. You can follow the below steps.</p> <p>Debian only supports upgrades one major version at a time, so you must pass through Bookworm (12) before reaching Trixie (13).</p> <p>So we need to upgrade debian 11 to 12, then to 13.</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#1-preparation-of-debian-11","title":"1. Preparation of debian 11.","text":"<p>To make sure your debian 11 is ready. Let's run the below commands</p> <pre><code># make sure you have the latest debian 11 packages\nsudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n\n# check your architecture\ndpkg --print-architecture\n\n# check your release version\nlsb_release -a\n\n# if you don't have lsb_release commands, you can install it\n# lsb-release is the package name. The command is lsb_release\nsudo apt install lsb-release\n\nsudo reboot\n</code></pre>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#2-upgrade-to-debian-12bookworm","title":"2. Upgrade to Debian 12(Bookworm)","text":"<p>Edit the sources list by replacing <code>bullseye</code> source with <code>bookworm</code> source.</p> <pre><code># open the source.list file and comment the old sources\nsudo vim /etc/apt/sources.list\n\n# add the new debian 12 sources\ndeb http://deb.debian.org/debian bookworm main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free non-free-firmware\n</code></pre> <p>Start the upgrade process</p> <pre><code>sudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n\n# check your release version\nlsb_release -a\n\n# you should see debian 12 as output\n\n# restart the server\nsudo reboot\n</code></pre> <p>During the upgrade, you may be asked to confirm if you want to use the new default conf or your old conf for <code>sudoer</code> or <code>sshd</code> I recommend you to keep your version. Because the new conf may remove your sudo rights or ssh access.</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#3-upgrade-to-debian-13trixie","title":"3. Upgrade to Debian 13(Trixie)","text":"<p>Edit the sources list by replacing <code>bookworm</code> source with <code>Trixie</code> source.</p> <pre><code># open the source.list file and comment the old sources\nsudo vim /etc/apt/sources.list\n\n# add the new debian 13 sources\ndeb http://deb.debian.org/debian trixie main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian-security trixie-security main contrib non-free non-free-firmware\ndeb http://deb.debian.org/debian trixie-updates main contrib non-free non-free-firmware\n</code></pre> <p>Start the upgrade process</p> <pre><code>sudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n\n# check your release version\nlsb_release -a\n\n# you should see debian 13 as output\n\n# restart the server\nsudo reboot\n</code></pre> <p>During the upgrade, you may be asked to confirm if you want to use the new default conf or your old conf for <code>sudoer</code> or <code>sshd</code> I recommend you to keep your version. Because the new conf may remove your sudo rights or ssh access.</p>"},{"location":"adminsys/os_setup/08.Debian_upgrade/#3-post-upgrade-cleanup","title":"3. Post-upgrade cleanup","text":"<pre><code># Check services that were replaced or modified:\nsystemctl --failed\n\n# verify the kernel\nuname -a\n\n# check your release version\nlsb_release -a\n\n# verify the firmware\ndpkg -l | grep firmware\n</code></pre>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/","title":"Configure Postfix MTA as Send-Only on Debian 11","text":""},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#1-setup-server-hostname","title":"1 Setup server hostname","text":"<p>The hostname of the server will be used as the name of the sender of the emails. So you should keep it nice and clean</p> <pre><code># get the current hostname\nhostname\n\n# if the name does not fit you, you can set up a new hostname\nsudo hostnamectl set-hostname smtp.casd.local --static\n</code></pre>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#2install-the-packages","title":"2Install the packages","text":"<pre><code># Install mailutils package\nsudo apt install mailutils\n\n# install postfix\nsudo apt install postfix\n</code></pre> <p>As the <code>postfix</code> package installs, you\u2019ll be asked to select an option on screen for your mail server.  For General type of email configuration window, select Internet site and click OK button. Here we suppose your server has internet connexion.</p> <p>The next page will ask you to set your Mail server name, this can be domain or server hostname with an A record. In this tutorial, we choose the host name of the server <code>smtp.casd.local</code>.</p>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#3-configure-postfix-mta-server","title":"3. Configure Postfix MTA Server","text":"<p>Edit Postfix configuration file /etc/postfix/main.cf to ensure it is configured as send only ( Only relaying emails from the local server).</p> <p>Set Postfix to listen on the 127.0.0.1loopback interface. <code>The default setting is to listen on all interfaces</code></p> <pre><code># open the conf file\nsudo vim /etc/postfix/main.cf\n\n# edit the below line\ninet_interfaces=loopback-only\nmyhostname=smtp.casd.local\n\n# restart the postfix service\nsudo systemctl restart postfix\n</code></pre>"},{"location":"adminsys/os_setup/Install_configure_Postfix_to_sendmail/#4-test-the-postfix-service","title":"4. Test the postfix service","text":"<p>To test email delivery, use the mail command like below.</p> <pre><code># send a mail to userx@example.com with title `Postfix Testing` and content `Postfix Send-Only Server`\necho \"Postfix Send-Only Server\" | mail -s \"Postfix Testing\" userx@example.com\n</code></pre> <p>Check the junk mails, you may find it there.</p>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/","title":"Install ansible on debian","text":"<p>Ansible can manage complex deployments, and scale deployments on thousands of servers. The official site is here</p> <p>You can find the official ansible installation guide here.</p>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#ansible-python-compatibility","title":"Ansible Python Compatibility","text":"<p>Ansible requires python to run, so before you install ansible, make sure you have python installed.</p> <p>Based on the table below and the available python version for your ansible host you should choose the appropriate ansible version to use with kubespray.</p> Ansible Version Python Version 2.11 2.7,3.5-3.9 2.12 3.8-3.10"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#install-virtualenv","title":"Install virtualenv","text":"<p>It is recommended to deploy the ansible version used by kubespray into a python virtual environment. So we need to install virtualenv too.</p> <pre><code># if you don't have pip, install pip first\nsudo apt install python3-pip\n\n# install virtual env\npython3 -m pip install virtualenv\n\n# if you see this warning The script virtualenv is installed in '/home/pliu/.local/bin' which is not on PATH, you need to add /home/pliu/.local/bin into your path\n\n# For example you can update path by adding below line to .bashrc\nexport PATH=\"/home/pliu/.local/bin:$PATH\"\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#install-ansible","title":"Install ansible","text":"<p>This doc will install a specific ansible to run kubespary, so it'n not the standard way to install ansible. For more information on ansible installation, you can visit this page</p> <p>Now follow the below steps to install ansible:</p>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#step-1-creat-an-virtual-env","title":"Step 1: creat an virtual env","text":"<p>The full shell script can be found here</p> <pre><code># change it if you want\nROOTDIR=~/opt\nmkdir -p $ROOTDIR\n\nVENVDIR=$ROOTDIR/kubespray-venv\n\nKUBESPRAYDIR=$ROOTDIR/kubespray\n\n# change it based on your python version\nANSIBLE_VERSION=2.12\n\n# create the virtual env for ansible\nvirtualenv  --python=$(which python3) $VENVDIR\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#step-2-install-ansible-via-kubespary-installation-script","title":"Step 2: Install ansible via kubespary installation script","text":"<pre><code># activate the virtual env\nsource $VENVDIR/bin/activate\n\n# clone the kubespray source from git repo\ncd $ROOTDIR\n\n# The tag number should be one of the release (latest release is recommended)\nTAG=v2.19.1\ngit clone -b $TAG https://github.com/kubernetes-sigs/kubespray.git\n\n# go to the kubespray dir\ncd $KUBESPRAYDIR\n\n# install the dependenices and ansible\npip install -U -r requirements-$ANSIBLE_VERSION.txt\n\n# test the dependencies and install ansible\ntest -f requirements-$ANSIBLE_VERSION.txt &amp;&amp; \n\n# Below two command does not work, because the file does not exist, and the ansible-galaxy command\n# does not take .txt file. \nansible-galaxy role install -r requirements-$ANSIBLE_VERSION.yml\n\nansible-galaxy collection -r requirements-$ANSIBLE_VERSION.yml\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#test-your-ansible","title":"Test your ansible","text":"<p>First you need to build an inventory which is a list of server name and ip address.</p> <p>Below example shows that we can divide server in groups, and each server is represented with a name and its ip address</p> <pre><code>[k8s]\nk8s-02 ansible_host=10.0.2.5\nk8s-03 ansible_host=10.0.2.4\n\n[others]\nk8s-01 ansible_host=10.0.2.6\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#ad-hoc-commands","title":"Ad-hoc commands","text":"<p>The simplest way to use ansible is to call ad-hoc commands. Below example will call ping command on the servers that are in the inventory</p> <pre><code>ansible -i hosts all -m ping\n\n# You should see below results\nk8s-02 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\nk8s-03 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\nk8s-01 | UNREACHABLE! =&gt; {\n    \"changed\": false,\n    \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 10.0.2.6 port 22: No route to host\",\n    \"unreachable\": true\n}\n</code></pre> <p>You can notice for k8s-02 and 03, it shows success, which means the command worked. But for k8s-01, it shows unreachable, which means ansible can't connect to this server.</p> <p>You can also limit the server that you want to run commands. For example, below command will only run command on group k8s. </p> <pre><code>ansible -i hosts k8s -m ping\n\n# we can even limit to the server name (e.g. k8s-02)\nansible -i hosts all -m ping --limit k8s-02\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#ansible-playbook","title":"ansible playbook","text":"<p>Ansible Playbooks are the way of sending commands to remote systems through scripts. Ansible playbooks are used to configure complex system environments to increase flexibility by executing a script to one or more systems. Ansible playbooks tend to be more of a configuration language than a programming language.</p> <p>Below is an example:</p> <pre><code>---\n\n# defines the target host of the task\n- hosts: group1\n  tasks:\n  - name: Enable SELinux\n    selinux:\n      state: enabled\n    # The when clause is the activation condition of the task. The ansible_os_family variable is gathered via gather_facts functionality.\n    when: ansible_os_family == 'Debian'\n    # Register can save output of the task to a variable, this variable can be used in the future task\n    register: enable_selinux\n\n  # a message will be displayed for the host user if the SELinux was indeed enabled before.\n  - debug:\n      Imsg: \"Selinux Enabled. Please restart the server to apply changes.\"\n    when: enable_selinux.changed == true\n\n- hosts: group2\n  tasks:\n  - name: Install apache\n    yum:\n      name: httpd\n      state: present\n     # we can use logic operator in the when close \n    when: ansible_system_vendor == 'HP' and ansible_os_family == 'RedHat'\n</code></pre> <p>You can also handler task. Below example shows that we changed the config file of sshd, and we need to restart the service to make the change take effect.</p> <pre><code>- hosts: group2\n  tasks:\n  # this task will go to the target file and find the target line by using\n  # regexp, and replace the target line with value which we specified. \n  - name: sshd config file modify port\n    lineinfile:\n     path: /etc/ssh/sshd_config\n     regexp: 'Port 28675'\n     line: '#Port 22'\n     # notify cluase can call the handler task \n    notify:\n       - restart sshd\n\nhandlers\n    # a handler task will not be executed if not notified\n    - name: restart sshd\n      service: sshd\n        name: sshd\n        state: restarted\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/Install_ansible_debian/#ansible-roles","title":"Ansible Roles","text":"<p>In a playbook, we defines all the task in one file. This makes the sub-module not reusable</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/","title":"Ansible roles","text":"<p>Read this doc  for more details on how to write an ansible role</p> <p>An Ansible Role is a <code>self-contained, portable unit</code> of Ansible automation that serves as the preferred method for  grouping related tasks and associated variables, files, handlers, and other assets in a known file structure.  While automation tasks can be written exclusively in an Ansible Playbook, Ansible Roles allow you to create bundles  of automation content that can be  - run in 1 or more plays,  - reused across playbooks,  - shared with other users in collections.</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#role-organization","title":"role organization","text":"<p><code>Ansible Roles</code> are expressed in YAML files. When a role is included in a task or a play, Ansible looks for  a <code>main.yml</code> file in at least 1 of 8 standard role directories such as : - tasks,  - handlers,  - modules,  - defaults,  - variables,  - files,  - templates, - meta.</p> <pre><code>roles/\n    my_role1/               # this hierarchy represents a \"role\"\n        tasks/            #\n            main.yml      #  &lt;-- tasks file can include smaller files if warranted\n        handlers/         #\n            main.yml      #  &lt;-- handlers file\n        templates/        #  &lt;-- files for use with the template resource\n            ntp.conf.j2   #  &lt;------- templates end in .j2\n        files/            #\n            bar.txt       #  &lt;-- files for use with the copy resource\n            foo.sh        #  &lt;-- script files for use with the script resource\n        vars/             #\n            main.yml      #  &lt;-- variables associated with this role\n        defaults/         #\n            main.yml      #  &lt;-- default lower priority variables for this role\n        meta/             #\n            main.yml      #  &lt;-- role dependencies\n        library/          # roles can also include custom modules\n        module_utils/     # roles can also include custom module_utils\n        lookup_plugins/   # or other types of plugins, like lookup in this case\n\n    my_role2/              # same kind of structure as \"my_role1\" was above, but for another purpose\n    my_role3/              # \"\"\n    my_role4/              # \"\"\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#role-vs-playbook","title":"Role vs Playbook","text":"<p>Why use an Ansible Role instead of an Ansible Playbook? Ansible Roles and Ansible Playbooks are both tools for organizing and executing automation tasks, but each serves a different purpose. Whether you choose to create Ansible Roles or write all of your tasks in an Ansible Playbook depends on your specific use case and your experience with Ansible.</p> <p>Most automation developers and system administrators begin creating automation content with individual playbooks. A playbook is a list of automation tasks that execute for a defined inventory. Tasks can be organized into a play\u2014a grouping of 1 or more tasks mapped to a specific host and executed in order. A playbook can contain 1 or more plays, offering a flexible mechanism for executing Ansible automation in a single file.</p> <p>While playbooks are a powerful method for automating with Ansible, writing all of your tasks in a playbook isn\u2019t always the best approach. In instances where scope and variables are complex and reusability is helpful, creating most of your automation content in Ansible Roles and calling them within a playbook may be the more appropriate choice.</p> <p>The following example illustrates the use of a role, linux-systemr-roles.timesync, within a playbook. In this instance, over 4 tasks would be required to achieve what the single role accomplishes. </p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#creating-a-role","title":"Creating a role","text":"<p>You can create a new role skeleton by using <code>ansible-galaxy</code></p> <pre><code> ansible-galaxy role init role_name\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#sharing-a-role","title":"Sharing a role","text":"<p>There are few ways to share your ansible roles:</p> <ul> <li>Ansible Galaxy: A free repository for sharing roles and other Ansible content with the larger Ansible community.             Roles can be uploaded to Ansible Galaxy via the command-line (CLI), whereas collections can be shared                 from the web interface. Since Ansible Galaxy is a community site, content is not vetted, certified.</li> <li>Ansible automation hub: repo for <code>Red Hat Ansible Automation Platform</code>, which is a central repository for                         finding, downloading, and sharing <code>Ansible Content Collections</code>.</li> <li>Private automation hub: An on-premise repository. You can share roles and other automation content within your                             enterprise, allowing teams to simplify workflows and speed up automation. </li> </ul>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-roles-in-an-ansible-playbook","title":"Use roles in an ansible playbook","text":"<p>There are three ways to integre an <code>ansible role</code> in an <code>ansible playbook</code>. - Use the <code>roles</code> option in playbook - Use the <code>include_role</code> in a task  - Use the <code>import_role</code> in a task</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-roles-option-in-playbook","title":"Use the <code>roles</code> option in playbook","text":"<p>Below is an example of a playbook which calls the role <code>configure_sshd_pam_sssd_openldap</code> and <code>intall_nginx</code> before tasks. </p> <p>If you have multiple roles, the order is not guarantied with this approach. The roles are executed before tasks. If you want to order the task and roles, use the <code>include_role</code> or <code>import_role</code></p> <pre><code>---\n- hosts: linux_servers\n  roles:\n    - configure_sshd_pam_sssd_openldap\n    - install_nginx\n  tasks:\n    - name: task1\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-include_role-in-a-task","title":"Use the <code>include_role</code> in a task","text":"<p>The content of the role is parsed during the execution of the task. </p> <pre><code>---\n- hosts: linux_servers\n  tasks:\n    - name: Print a message\n      ansible.builtin.debug:\n        msg: \"this task runs before the role1\"\n\n    - name: Include the role with name role1\n      ansible.builtin.include_role:\n        name: role1\n      vars:\n        dir: '/opt/a'\n        app_port: 5000\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-import_role-in-a-task","title":"Use the <code>import_role</code> in a task","text":"<p>The content of the role is parsed at the start of the playbook. </p>"},{"location":"adminsys/os_setup/package_management/01.Install_deb_files/","title":"Install Deb files","text":"<p>There are many ways to install packages in debian based linux (debian/ubuntu/Mint). There are two main ways to install packages:  - use the package manager such apt-get/apt to connect to package repo servers.  - use the standalone .deb file</p> <p>In this tutorial, we will focus on how to install packages via deb files. There are mainly four ways: - Use the GUI (available in ubuntu desktop) - Use apt (e.g. sudo apt install ./filename.deb) - Use dpkg (e.g. sudo dpkg -i ./filename.deb) - Use gdebi ()</p>"},{"location":"adminsys/os_setup/package_management/01.Install_deb_files/#1-use-apt","title":"1. Use apt","text":"<pre><code># be sure to \ncd path/to/deb\n\n# don't miss ./, without it apt will search them in the package repo instead of local file system.\nsudo apt install ./filename.deb\n\n# list all installed \nsudo apt list --installed\n\n# remove package name\nsudo apt remove package-name\n</code></pre>"},{"location":"adminsys/os_setup/package_management/01.Install_deb_files/#2-use-dpkg","title":"2. Use dpkg","text":"<pre><code># check the metadata of .deb file\ndpkg --info package-name.deb\n\n# install a deb file via dpkg, the -i option means install, it's case sensitive\nsudo dpkg -i ./filename.deb\n\n# if all required packages is already installed on the system, then we can stop here, if not\n# we need to run the below command, it fixes all missing dependencies\nsudo apt-get install -f\n\n# To see a list of all installed packages with Dpkg, use the command \nsudo dpkg-query -l\n\n# remove packages with Dpkg using \ndpkg -r packagename\n</code></pre>"},{"location":"adminsys/os_setup/package_management/01.Install_deb_files/#3-use-gdebi","title":"3. Use gdebi","text":"<p>gdebi is a tool specially developed for installing <code>.deb</code> files. It has a core and GUI. Actually <code>gdebi</code> is just a  front-end to the dpkg with added functionality that it can check for <code>dependency packages in the repositories</code>  and can install them in one-operation, while <code>dpkg -i</code> requires two operations manually (later being <code>apt-get -f install</code>).</p> <p>If the dependency packages exists in a repository which is not in the system source list, the gdebi installation will fail. You need to enable all required package repo before running the command.</p> <pre><code># the gdebi-core allows you to install .deb file via command line\nsudo apt install gdebi-core\n\n# install a .deb via gdebi\nsudo gdebi ./filename.deb\n\n# if you want to have GUI integration, you need to install main gdebi package too\nsudo apt install gdebi\n\n# this will add more options in the filesytem GUI, when right click on a .deb file in the filesytem ui, you will get\n# options to install it via gdebi\n\n# to remove a package installed by gdebi,\nsudo apt remove packagename\nsudo apt autoclean\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/","title":"Install aptly","text":""},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#1-system-package-repository-introduction","title":"1. System Package repository Introduction","text":"<p>Before we start, we need to understand what is a linux system package repository. In Linux, the <code>system package repository</code> is a storage location hosted on remote servers from which the system retrieves and installs software and updates. </p> <p>Each linux distribution has its own repository and package manager. For example, for <code>Red Hat</code> distribution, they uses <code>yum/dnf</code> as package manager,  the default repo url is shown below</p> <pre><code>[base]\nname=CentOS-7 - Base\nmirrorlist=http://mirrorlist.centos.org/?release=7&amp;arch=$basearch&amp;repo=os&amp;infra=$infra\n#baseurl=http://mirror.centos.org/centos/7/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#released updates\n[updates]\nname=CentOS-7 - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=7&amp;arch=$basearch&amp;repo=updates&amp;infra=$infra\n#baseurl=http://mirror.centos.org/centos/7/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-7 - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=7&amp;arch=$basearch&amp;repo=extras&amp;infra=$infra\n#baseurl=http://mirror.centos.org/centos/7/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-7 - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=7&amp;arch=$basearch&amp;repo=centosplus&amp;infra=$infra\n#baseurl=http://mirror.centos.org/centos/7/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n</code></pre> <p>For debian distribution, it uses <code>apt-get/apt</code> as package manager, and the default/standard repo url are:</p> <pre><code>deb http://deb.debian.org/debian bullseye main\ndeb-src http://deb.debian.org/debian bullseye main\ndeb http://security.debian.org/debian-security bullseye-security main contrib\ndeb-src http://security.debian.org/debian-security bullseye-security main contrib\ndeb http://deb.debian.org/debian/ bullseye-updates main contrib\ndeb-src http://deb.debian.org/debian/ bullseye-updates main contrib\n</code></pre> <p>You can find the repo list are in <code>/etc/apt/sources.lits</code> file and files under the <code>/etc/apt/sources.list.d</code> directory.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#11-repo-list-format","title":"1.1 Repo list format","text":"<p>Below figure shows the format of each row in the source list.</p> <p></p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#type","title":"Type","text":"<p>The term <code>deb</code> indicates that it is the repository of binaries, which are pre-compiled fiels. The term <code>deb-src</code> indicates that it is the repository of packages in source file format, which requires <code>compilation</code> in order to use it in the system.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#repository-url","title":"Repository URL","text":"<p>The URL (HTTP, HTTPS, or FTP) represents the location of the repository from which you can download the packages.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#distribution","title":"Distribution","text":"<p>The next term is the short codename (i.e. Buster, Wheezy, Lenny, Jessie, Bullseye, etc.) of the release of your current system. The repo server may support multiple release, so we need to specify which release we want.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#component","title":"Component","text":"<p>The final term represents the <code>categories of the Debian package</code>. The available categories of the Debian distribution are : - main: This category contains packages that are released under a free license (BSD, GPL, etc.) and that meet the DFSG (Debian Free Software Guidelines). These packages also contain the source code within them, which can be modified and redistributed. - contrib: This category contains the packages that meet the DFSG (Debian Free Software Guidelines. The packages in the Contrib category are open-source packages, but depend on non-free packages to work. - non-free: This category contains the packages that do not meet the DFSG (Debian Free Software Guidelines). These packages have some strict license conditions that restrict the usage and redistribution of the software.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#12-add-a-repo-by-using-the-sourceslist-file","title":"1.2 Add a repo by using the sources.list file","text":"<p>In below example, we add the virtualbox repo to the repo list. You can follow below steps to add a custom repository:</p> <ol> <li>Download and import GPG keys of vbox. You can use below command. You may need to install gpg first</li> </ol> <pre><code>wget -O- -q https://www.virtualbox.org/download/oracle_vbox_2016.asc | sudo gpg --dearmour -o /usr/share/keyrings/oracle_vbox_2016.gpg\n</code></pre> <ol> <li>Open the <code>/etc/apt/sources.list</code> file and add below line at the end of the file. <code>deb [arch=amd64 signed-by=/usr/share/keyrings/oracle_vbox_2016.gpg] http://download.virtualbox.org/virtualbox/debian bullseye contrib</code>. Or you can run below command</li> </ol> <pre><code>echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/oracle_vbox_2016.gpg] http://download.virtualbox.org/virtualbox/debian bullseye contrib\" | sudo tee /etc/apt/sources.list.d/virtualbox.list\n</code></pre> <ol> <li>Save and close</li> <li>Update apt (sudo apt update)</li> <li>Search the virtual box package with <code>apt search virtualbox</code>. You should see below output</li> </ol> <pre><code>Sorting... Done\nFull Text Search... Done\nlibvirt-daemon-driver-vbox/stable 7.0.0-3 amd64\n  Virtualization daemon VirtualBox connection driver\n\nvirtualbox-6.1/unknown 6.1.42-155177~Debian~bullseye amd64\n  Oracle VM VirtualBox\n\nvirtualbox-7.0/unknown 7.0.6-155176~Debian~bullseye amd64\n  Oracle VM VirtualBox\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#2-installing-aptly","title":"2. Installing Aptly","text":"<p>You must not run below command with <code>root</code> user. </p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#21-install-aptly-dependencies","title":"2.1 Install Aptly dependencies","text":"<p>Atply requrires below package: - bzip2: compression - gnupg: the gnu privacy guard is a complete and free implementation of the OpenPGP standard - gpgv: gpgv is actually a stripped-down version of gpg which is only able to check signatures. - xz-utils: is a general-purpose data compression software with a high compression ratio</p> <pre><code>sudo apt install bzip2 gnupg gpgv\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#22-install-aptly","title":"2.2 Install Aptly","text":"<p>The package repo for aptly can be found here</p> <p>Step1 : Download the public gpg key of Aptly </p> <pre><code># New key per 2022-03-15 !\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys A0546A43624A8331\n</code></pre> <p>Step2 : Add aptly repo to sources.list</p> <pre><code># -a means append which is important, without it the original content will be removed\necho \"deb http://repo.aptly.info/ squeeze main\" | sudo tee -a /etc/apt/sources.list\n</code></pre> <p>Don't worry about squeeze part in the repo name. I have tested it, it works for the debian 11 (bullseye).</p> <p>Step3 : Update repo and install</p> <pre><code>sudo apt update\nsudo apt install aptly\n</code></pre> <p>Step4 : Check your aptly install</p> <pre><code>aptly version\n\n# you should see below output\naptly version: '1.5.0'\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#23-create-gpg-pair","title":"2.3 Create GPG pair","text":"<p>We will use a <code>GPG key pair to sign the published repositories</code>. If you don't have one, use below command to generate one.</p> <pre><code># call the gpg key gen client\ngpg --full-generate-key\n\n# it will prompt below lines, just fill it with name, email\nReal name: casd-debian\nEmail address: service@casd.eu\nYou selected this USER-ID:\n    \"casd-onyxia &lt;casd-support@casd.eu&gt;\"\n\nChange (N)ame, (E)mail, or (O)kay/(Q)uit? O\n\n# then it will ask you to enter a passphrase to secure the private key\n</code></pre> <p>After you entered all the information, a folder <code>.gnupg</code> will be generated in the current folder. For more information about the GPG key management, you can check this page. Below are some commonly used command</p> <pre><code># list secret keys\ngpg --list-secret-keys\n\n# list public keys\ngpg --list-keys\n\n# delet keys, to remove keys you need to delete the secret key first, then the public key\ngpg --delete-secret-key\ngpg --delete-key\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#troubleshooting","title":"Troubleshooting","text":"<p>The gpg agent is very bugy. And you may receive many error message. The most common solution is to restart the gpg agent by using below command</p> <pre><code># kill the gpg-agent process will trigger the daemon to create a new one\ngpgconf --kill gpg-agent\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#verify-the-passphrase-of-the-secret-key","title":"Verify the passphrase of the secret key","text":"<p>The simplest way is to use the change password feature of the secret key. It asks</p> <pre><code>gpg --passwd &lt;secret-key-id&gt;\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#3-creating-repo-mirrors","title":"3. Creating repo mirrors","text":"<p>Now we have all the necessary parts to create a local mirror of an official repo. Suppose we need to  mirror <code>bullseye</code> (current stable 2023) for architecture <code>amd64</code>. Only <code>main</code> component is required and this repository is targeted for servers, not desktops. You can find local debian repo mirror per Country in this page. In below command, we choose the server in France.</p> <pre><code># command 1\naptly mirror create -architectures=amd64 -filter='Priority (required) | Priority (important) | Priority (standard)' bullseye-main http://ftp.fr.debian.org/debian/ bullseye main\n</code></pre> <p>You should see this error <code>ERROR: unable to fetch mirror: verification of detached signature failed: exit status 2</code> when you run above command. So we need to add the default debian keyring as trusted.</p> <pre><code># command 2\ngpg --no-default-keyring --keyring /usr/share/keyrings/debian-archive-keyring.gpg --export | gpg --no-default-keyring --keyring trustedkeys.gpg --import\n</code></pre> <p>Now rerun the <code>command 1</code>, you should see below output</p> <pre><code>Downloading http://ftp.fr.debian.org/debian/dists/bullseye/InRelease...\nSuccess downloading http://ftp.fr.debian.org/debian/dists/bullseye/InRelease\ngpgv: can't allocate lock for '/home/coder/.gnupg/trustedkeys.gpg'\ngpgv: Signature made Sat 17 Dec 2022 10:15:20 AM UTC\ngpgv:                using RSA key 0146DC6D4A0B2914BDED34DB648ACFD622F3D138\ngpgv: Good signature from \"Debian Archive Automatic Signing Key (10/buster) &lt;ftpmaster@debian.org&gt;\"\ngpgv: Signature made Sat 17 Dec 2022 10:15:21 AM UTC\ngpgv:                using RSA key A7236886F3CCCAAD148A27F80E98404D386FA1D9\ngpgv: Good signature from \"Debian Archive Automatic Signing Key (11/bullseye) &lt;ftpmaster@debian.org&gt;\"\ngpgv: Signature made Sat 17 Dec 2022 10:20:04 AM UTC\ngpgv:                using RSA key A4285295FC7B1A81600062A9605C66F00D6C9793\ngpgv:                issuer \"debian-release@lists.debian.org\"\ngpgv: Good signature from \"Debian Stable Release Key (11/bullseye) &lt;debian-release@lists.debian.org&gt;\"\n\nMirror [bullseye-main]: http://ftp.fr.debian.org/debian/ bullseye successfully added.\nYou can run 'aptly mirror update bullseye-main' to download repository contents.\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#31-the-filter-flag","title":"3.1 The filter flag","text":"<p>The flag -filter= allows us to cut down number of packages to download. First part, <code>Priority (required) | Priority (important) | Priority (standard)</code> is essential \u201cbase\u201d Debian system. </p> <p>We can also specify individual packages explicitly. For example, with below command, the mirror will  add package such as: nginx, postgresql, etc. </p> <pre><code>aptly mirror create -architectures=amd64 -filter='Priority (required) | Priority (important) | Priority (standard) | nginx | postgresql' -filter-with-deps bullseye-main http://ftp.fr.debian.org/debian/ bullseye main\n\n# we create also mirror for bullseye-updates and bullseye-security \n# you can notice the repo url is the same, but the distribution is `bullseye-updates`\n# and the component is `main` \n\naptly mirror create -architectures=amd64 -filter='Priority (required) | Priority (important) | Priority (standard) | nginx | postgresql' -filter-with-deps bullseye-updates http://ftp.fr.debian.org/debian/ bullseye-updates main\n</code></pre> <p>Flag -filter-with-deps instructs <code>aptly</code> to include dependencies of matching packages as well. </p> <p>If filter is not specified, all packages would be included in the mirror and that would require more space and download size would be bigger.</p> <p>In below example, we will mirror all the package of the debian security repo, because we have 0 filter in it.</p> <pre><code># without filter, the mirror will take 20GiB \naptly mirror create -architectures=amd64 bullseye-security http://security.debian.org/debian-security bullseye-security/updates main contrib non-free\n\n# with filter \naptly mirror create -architectures=amd64 -filter='Priority (required) | Priority (important) | Priority (standard) | nginx | postgresql' -filter-with-deps bullseye-security http://security.debian.org/debian-security bullseye-security/updates main contrib non-free\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#32-check-the-created-mirror","title":"3.2 Check the created mirror","text":"<pre><code># bullseye-update mirror\naptly mirror create -architectures=amd64 bullseye-updates http://ftp.fr.debian.org/debian/ bullseye-updates main\n\n# bullseye-security mirror\naptly mirror create -architectures=amd64 bullseye-security http://security.debian.org/debian-security bullseye-security/updates main contrib non-free\n\n\n# get the full list of existing mirrors\naptly mirror list\n\n# you should see below output\nList of mirrors:\n * [bullseye-main]: http://ftp.fr.debian.org/debian/ bullseye\n\nTo get more information about mirror, run `aptly mirror show &lt;name&gt;`.\n\n# get the detail of bullseye-main mirror\n aptly mirror show bullseye-main\n\n# you should see below output\nName: bullseye-main\nArchive Root URL: http://ftp.fr.debian.org/debian/\nDistribution: bullseye\nComponents: main\nArchitectures: amd64\nDownload Sources: no\nDownload .udebs: no\nFilter: Priority (required) | Priority (important) | Priority (standard)\nFilter With Deps: no\nLast update: never\n\nInformation from release file:\nAcquire-By-Hash: yes\nArchitectures: all amd64 arm64 armel armhf i386 mips64el mipsel ppc64el s390x\nChangelogs: https://metadata.ftp-master.debian.org/changelogs/@CHANGEPATH@_changelog\nCodename: bullseye\nComponents: main contrib non-free\nDate: Sat, 17 Dec 2022 10:14:37 UTC\nDescription:  Debian 11.6 Released 17 December 2022\n\nLabel: Debian\nNo-Support-For-Architecture-All: Packages\nOrigin: Debian\nSuite: stable\nVersion: 11.6\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#33-edit-existing-mirror","title":"3.3 Edit existing mirror","text":"<p>When you want to modify an existing mirror configuraiton, you need to run below command. You can find more information here</p> <pre><code># note with below command, the old filter will be replaced, if you want to append, you need to do it\n# manually \naptly mirror edit -filter='nginx | postgresql' -filter-with-deps bullseye-main\n\n# append example\naptly mirror edit -filter='Priority (required) | Priority (important) | Priority (standard) | nginx | postgresql' -filter-with-deps bullseye-main\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#34-updatesynchronize-the-mirror","title":"3.4 Update(Synchronize) the mirror","text":"<p>Now we are ready to synchronize the local mirror with the official repo. </p> <p>The doc and command aptly called it update, I found this is ambigue, so I use the word synchronize.</p> <pre><code># this command will download all the filtered package to local server\n# in ~/.aptly/pool/\naptly mirror update bullseye-main\n\n# you should see below output\nDownloading http://ftp.fr.debian.org/debian/dists/bullseye/InRelease...\nSuccess downloading http://ftp.fr.debian.org/debian/dists/bullseye/InRelease\ngpgv: can't allocate lock for '/home/coder/.gnupg/trustedkeys.gpg'\ngpgv: Signature made Sat 17 Dec 2022 10:15:20 AM UTC\ngpgv:                using RSA key 0146DC6D4A0B2914BDED34DB648ACFD622F3D138\ngpgv: Good signature from \"Debian Archive Automatic Signing Key (10/buster) &lt;ftpmaster@debian.org&gt;\"\ngpgv: Signature made Sat 17 Dec 2022 10:15:21 AM UTC\ngpgv:                using RSA key A7236886F3CCCAAD148A27F80E98404D386FA1D9\ngpgv: Good signature from \"Debian Archive Automatic Signing Key (11/bullseye) &lt;ftpmaster@debian.org&gt;\"\ngpgv: Signature made Sat 17 Dec 2022 10:20:04 AM UTC\ngpgv:                using RSA key A4285295FC7B1A81600062A9605C66F00D6C9793\ngpgv:                issuer \"debian-release@lists.debian.org\"\ngpgv: Good signature from \"Debian Stable Release Key (11/bullseye) &lt;debian-release@lists.debian.org&gt;\"\nDownloading &amp; parsing package files...\nDownloading http://ftp.fr.debian.org/debian/dists/bullseye/main/binary-amd64/Packages.gz...\nSuccess downloading http://ftp.fr.debian.org/debian/dists/bullseye/main/binary-amd64/Packages.gz\nApplying filter...\nPackages filtered: 58640 -&gt; 325.\nBuilding download queue...\nDownload queue: 325 items (168.48 MiB)\n</code></pre> <p>Current mirror contents are stored in the package database. You can update mirror at any moment as required.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#4-use-snapshots","title":"4 Use snapshots","text":"<p>As you know, the package in the official repo can be updated anytime, but our mirror can't synchronize each modification in real time. We can use snapshot to release packages in a state  </p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#41-create-snapshots","title":"4.1 Create snapshots","text":"<p>It\u2019s time take snapshots of the mirrors to preserve <code>exact current mirror state</code>. I will label snapshots after mirror name, applying version for the main bullseye mirror (i.e. Debian 11.6 -&gt; bullseye ) and current date for frequently updated <code>security</code> and <code>updates</code> mirrors:</p> <pre><code># the general form\naptly snapshot create &lt;snapshot-name&gt; from mirror &lt;mirror-name&gt;\n\n# if you don't know the existing mirror name, use below command to get all available mirrors\naptly mirror list\n\n# create a snapshot for mirror bullseye-main\naptly snapshot create bullseye-main-11.6 from mirror bullseye-main\n\n# create a snapshot for mirror bullseye-updates\naptly snapshot create bullseye-updates-20230206 from mirror bullseye-updates\n\n# create a snapshot for bullseye-security\naptly snapshot create bullseye-security-20230206 from mirror bullseye-security\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#42-merging-snapshots","title":"4.2 Merging snapshots","text":"<p>Releasing the snapshop one by one is not very productive. We can merge the three snapshoot into one by using below command.</p> <pre><code># general form\n# -latest option chooses merge strategy: package with latest version \u201cwins\u201d.\n# it can take as many sub snapshot as possible\naptly snapshot merge -latest &lt;snapshot-name&gt; &lt;sub-snapshot-name1&gt; &lt;sub-snapshot-name2&gt; ...\n\n# if you don't know your snapshot name, you can use below command to get them\naptly snapshot list\n\n# our example\naptly snapshot merge -latest bullseye-stable-20230206 bullseye-main-11.6 bullseye-security-20230206 bullseye-updates-20230206\n</code></pre> <p>Let's check the merged snapshot, for example the latest version of package nginx should came from the security mirror, because the there is a security update latly. </p> <p>We can use below command to track the package origin</p> <pre><code>aptly package show -with-references 'Name (nginx)'\n\n# you should see below output\nPackage: nginx\nPriority: optional\nSection: httpd\nInstalled-Size: 102\nMaintainer: Debian Nginx Maintainers &lt;pkg-nginx-maintainers@alioth-lists.debian.net&gt;\nArchitecture: all\nVersion: 1.18.0-6.1+deb11u3\nDepends: nginx-core (&lt;&lt; 1.18.0-6.1+deb11u3.1~) | nginx-full (&lt;&lt; 1.18.0-6.1+deb11u3.1~) | nginx-light (&lt;&lt; 1.18.0-6.1+deb11u3.1~) | nginx-extras (&lt;&lt; 1.18.0-6.1+deb11u3.1~), nginx-core (&gt;= 1.18.0-6.1+deb11u3) | nginx-full (&gt;= 1.18.0-6.1+deb11u3) | nginx-light (&gt;= 1.18.0-6.1+deb11u3) | nginx-extras (&gt;= 1.18.0-6.1+deb11u3)\nFilename: nginx_1.18.0-6.1+deb11u3_all.deb\nSize: 92936\nMD5sum: 47deabf24cd33a9440782b61f30bc2e9\nSHA1: 320d46db1c47ef6f7ccc1c7d3685403615127b33\nSHA256: 795f27bbd556a60e132e6f779b78105969009d129e727ab60f826e1bf4320365\nSHA512: 47e69866c8e829f60b05d7074c28c89a21c09a3d3e6527192882f2c1f63abfafd8a3ee2f45ab1bb381508eb7f17e5663656b05da4efdfdb3ed129c467aa112ca\nDescription: small, powerful, scalable web/proxy server\nDescription-Md5: 902443ddbee17249123a068e7ca7c6d8\nHomepage: https://nginx.net\n\nReferences to package:\n  mirror [bullseye-security]: http://security.debian.org/debian-security/ bullseye-security/updates\n  mirror [bullseye-main]: http://ftp.fr.debian.org/debian/ bullseye\n  snapshot [bullseye-main-11.6]: Snapshot from mirror [bullseye-main]: http://ftp.fr.debian.org/debian/ bullseye\n  snapshot [bullseye-stable-20230206]: Merged from sources: 'bullseye-main-11.6', 'bullseye-security-20230206', 'bullseye-updates-20230206'\n  snapshot [bullseye-security-20230206]: Snapshot from mirror [bullseye-security]: http://security.debian.org/debian-security/ bullseye-security/updates\n</code></pre> <p>in the <code>references to package</code> block, you can notice the package origin is from snapshot <code>bullseye-security-20230206</code> (last row). </p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#5-publish-repository","title":"5 Publish Repository","text":"<p>Now we can publish our snapshot <code>bullseye-stable-20230206</code> as Debian repository, which will be consumed by apt-get client.</p> <p>Use below command to publish the snapshot</p> <pre><code># general form\n# the -distribution option: specifies the distribution name of the repo, which is mandatory\naptly publish snapshot -distribution=&lt;distribution-name&gt; &lt;snapshot-name&gt;\n\n\n# our example\naptly publish snapshot -distribution=bullseye bullseye-stable-20230206\n</code></pre> <p>The above command will publish the repository to <code>~/.aptly/public/</code> directory. You can start any HTTP server to serve this directory as static files, or use aptly built-in webserver for testing.</p> <pre><code># run aptly built-in webserver (for testing only, not recommended for prod)\naptly serve\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#51-check-published-snapshot-dependencies","title":"5.1 Check published snapshot dependencies","text":"<p>Below command will generate a graph which shows the dependencies of the published snapshot</p> <pre><code># this command requires graphviz\n# -output=\"\": specify output filename, default is to open result in viewer\n# -format=\"png\": graph output format, could be anything graphviz supports, e.g. png, pdf, svg, \u2026\naptly graph -output=\"/tmp/package_build_history.png\" -format=\"png\"\n\n# install graphviz\nsudo apt install graphviz\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#52-serve-the-published-via-a-web-server","title":"5.2 Serve the published via a web server","text":"<p>For testing purpose, the built-in webserver is enough. But For production, we need to install a real webserver. In this tutorial, we recommend nginx, because it has better performence for service static content and consum less resources compare to Apache2.</p> <p>You can check this tutorial to install nginx</p> <p>After the installation of nginx, you need to create a virtual host. To do so, create a <code>conf file</code> under /etc/nginx/sites-available/. In our tutorial, I named the conf file as deb-repo.conf</p> <pre><code>sudo vim /etc/nginx/sites-available/deb-repo.conf\n\n# add below content in it\n# you need to modify the server name and the root path \nserver {\n    server_name deb.casd.local;\n\n    root /package-repo/aptly/.aptly/public;\n\n    location / {\n        autoindex on;\n        try_files $uri $uri/ =404;\n    }\n}\n\n# check the correctness of your conf syntax\nsudo nginx -t\n</code></pre> <p>To enable this conf, you need to create a sym link in /etc/nginx/sites-enabled/ </p> <pre><code>sudo ln -s /etc/nginx/sites-available/deb-repo.conf /etc/nginx/sites-enabled/\n\n# you may need to restart the nginx server\n\nsudo systemctl restart nginx\n</code></pre> <p>We do not recommend to add ssl to transfer http to https. Because the server identity and package integrity is ensured by the apt GPG protocol. And the package is public data, no need to ensure confidentiality. The ssl will only add some performence overhead without any utility.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#6-using-the-private-repository","title":"6. Using the private Repository","text":""},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#61-prepare-the-gpg-gnu-privacy-guard-key","title":"6.1 Prepare the GPG (GNU privacy guard) key","text":"<p>First I need to import public part of the GPG key that was used to sign the repository into trusted apt keyring on target machine.</p> <p>The file extenion .asc indicates this public key is ASCII armored key. The extension .gpg indicates the key is in <code>binary OpenPGP format</code>. It is very important for the clients when they import the key as trusted gpg key into their key store. </p> <pre><code># run from the server which has aptly \ngpg --export --armor &gt; repo_gpg_key.asc\n\n# the generated file should look like this\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nAvQWBa75d4ySlyqQHh9wHJ0V5Ba7X/kePzM=\n=s9av\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#62-copy-the-public-key-to-target-machine","title":"6.2 Copy the public key to target machine","text":"<p>There are two solutions for adding the the gpg key of the private repo: Solution 1 : Use <code>apt-key</code> tool Solution 2 : Without tool</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#solution-1","title":"Solution 1:","text":"<p>This solution is <code>deprecated</code>, so we don't recommend this solution.</p> <pre><code># down the public key from the deb server\nwget http://deb.casd.local/casd_gpg_key.asc\n\n# apt-key requires gnupg package, if you don't have it you need to install it\nsudo apt install gnupg \n\n# run from the target server which consume apt repository\n# add our GPG as trusted key\nsudo apt-key add casd_gpg_key.asc\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#solution-2","title":"Solution 2:","text":"<p>We recommand this solution, and the client does not need to install any extra package.</p> <pre><code># download the key and add it to the key store\n# -q activate quiet mode to hide output\n# -O- means output to std out.\nwget -qO- http://deb.casd.local/casd_gpg_key.asc | sudo tee /etc/apt/trusted.gpg.d/casd_gpg_key.asc\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#change-apt-source-repo-list","title":"Change apt source repo list","text":"<p>Now we need to edit the <code>/etc/apt/sources.list</code> file and add our repo as a source repo. Here suppose our repo server url is <code>http://deb.casd.local</code>, your source.list should look like below file </p> <pre><code># comment the default sources\n# deb http://deb.debian.org/debian bullseye main\n# deb http://security.debian.org/debian-security bullseye-security main\n# deb http://deb.debian.org/debian bullseye-updates main\n\n# add our repo\ndeb http://deb.casd.local/ bullseye main\n</code></pre> <p>Save and exit, then you need to update the <code>apt-get</code> repo cache</p> <pre><code># update repo cache\nsudo apt-get update\n\n# update the installed packages\nsudo apt-get upgrade\n</code></pre> <p>As this repository has been published from snapshot, <code>it would never change until it is update to new snapshot</code>. Good thing is that I can setup all my machine to use this repo and get identical set of packages installed. Bad thing is that I need to maintain and update my repo as updates are coming, but if I have many machines, the advantage of predictable upgrades outweighs the maintenance costs.</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#7-upgrading-repository","title":"7. Upgrading repository","text":"<p>Several days after you published your repository, when you update your mirrors again. And you discover there are new security updates. You need to follow below steps to build a new release </p> <pre><code># update mirror\naptly mirror update bullseye-main\naptly mirror update bullseye-updates\naptly mirror update bullseye-security \n\n# create new snapshot\naptly snapshot create bullseye-main-11.6 from mirror bullseye-main\naptly snapshot create bullseye-updates-&lt;date&gt; from mirror bullseye-updates\naptly snapshot create bullseye-security-&lt;date&gt; from mirror bullseye-security\n\n# merge the snapshot\naptly snapshot merge -latest bullseye-stable-&lt;date&gt; bullseye-main-11.6 bullseye-updates-&lt;date&gt; bullseye-security-&lt;date&gt; \n\n# publish the new release\n# when there is already a release in use, you need to add switch to replace the old release\naptly publish switch bullseye bullseye-stable-&lt;date&gt;\n\n# generate the build graph\naptly graph -output=\"/pathToAptly/package_build_history_&lt;date&gt;.png\" -format=\"png\"\n\n# if you don't need the old release anymore, you can drop them\naptly db cleanup\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#8-serve-the-packages","title":"8. Serve the packages","text":"<p>Although you can run <code>aptly serve</code> to publish the packages via port <code>8080</code> for test purpose. It's recommended to set up  a web server such as Nginx or apache2</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#9-cron","title":"9. Cron","text":"<p>There is a cron job script which try to update the mirror and provide release. https://github.com/gvogets/aptly-scripts/blob/master/cronjob</p> <p>We find the cron job is not adapted for our need. So we right our own cron job scripts: - A release cron job:  publish aptly snapshots automatically - A clean cron job :</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#91-aptly-release-cron-job","title":"9.1 Aptly release cron job","text":"<p>The below cron job <code>aptly_build_release.bash</code> is designed to publish aptly snapshots automatically</p> <pre><code>#!/bin/bash\n\n# if we put this script in cron.weekly, this script will be executed with root privilege. And the aptly db data is user specific.\n# So run as root can not get the information about the mirror and snapshoot. So for the aptly command, we need to use sudo -u aptly\n# to run the command as user `aptly`.\n# script config\nuid=changeMe\ndist_name=bullseye\nroot_path=/path/to/.aptly\nlog_file_path=\"${root_path}/log/aptly_log.out\"\ngpg_pwd_file_path=\"${root_path}/creds/pwd\"\nrepo_path=\"${root_path}/public\"\nexport_repo_path=\"${root_path}/export\"\nrelease_date=\"$(date '+%Y%m%d')\"\nsnapshot_merge_list=()\n\n# set bash script abort on the first command fails, if \nset -eo pipefail\n\n# Saves file descriptors so they can be restored to whatever they were before \n#redirection or used themselves to output to whatever they were before the following redirect.\nexec 3&gt;&amp;1 4&gt;&amp;2\n# Restore file descriptors for particular signals. Not generally necessary since they \n# should be restored when the sub-shell exits.\ntrap 'exec 2&gt;&amp;4 1&gt;&amp;3' 0 1 2 3\n# Redirect stdout to file log.out then redirect stderr to stdout. Note that the order is \n# important when you want them going to the same file. stdout must be redirected \n# before stderr is redirected to stdout.\n# By default, the log will be appended to the log file. If you only want to keep the log\n# of the last run, replace &gt;&gt; by &gt;\nexec 1&gt;&gt;\"${log_file_path}\" 2&gt;&amp;1\n# Everything below will go to the file 'aptly_log.out':\n\n# clean the old graph\nrm -f ${repo_path}/*.png\n\n# clean last release zip symoblic link\nrm -f ${repo_path}/*.zip\n\n\n# read the gpg private key password from a file\ngpg_pwd=$(&lt;\"${gpg_pwd_file_path}\")\n\n# update all existing mirror\nsudo -u $uid aptly mirror list --raw | xargs -n1 sudo -u $uid aptly mirror update\n\n\n# create a list of mirror\nREPOS=$(sudo -u $uid aptly mirror list --raw | cut -d '#' -f1 | sort | uniq | xargs)\n\n# loop through the mirror list, create new snapshot for each mirror\n# build a snapshot merge list\nfor REPO in $REPOS; do\n    sudo -u $uid aptly snapshot create \"${REPO}-${release_date}\" from mirror \"${REPO}\"\n    snapshot_merge_list+=(\"${REPO}-${release_date}\")\ndone\n\necho \"${snapshot_merge_list}\"\n\n\n# merge the snapshot\nsudo -u $uid aptly snapshot merge -latest \"${dist_name}-stable-${release_date}\" \"${snapshot_merge_list[@]}\"\n\n# publish the new release\n# when there is already a release in use, you need to add switch to replace the old release\n# we need to put -batch=true when run this script in cron, otherwise, aptly and gpg will use the same /dev/tty. And this will cause \n# the -passphrase option fail, and a promt will show up. \nsudo -u $uid aptly publish switch -batch=true -passphrase=\"${gpg_pwd}\" \"${dist_name}\" \"${dist_name}-stable-${release_date}\"\n\n# generate the build graph\nsudo -u $uid aptly graph -output=\"${repo_path}/package_build_history_${release_date}.png\" -format=\"png\"\n\n# if you don't need the old release anymore, you can drop them\nsudo -u $uid aptly db cleanup\n\n# zip the release and put it to the export repo\nsudo -u $uid zip -r \"${export_repo_path}/${dist_name}-${release_date}.zip\" \"${repo_path}\"\n\n# create a symbolic link\nsudo -u $uid ln -s \"${export_repo_path}/${dist_name}-${release_date}.zip\" \"${repo_path}/${dist_name}-${release_date}.zip\"\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#911-script-configuration","title":"9.1.1 Script configuration","text":"<p>The first section of this script is the configuration. </p> <ul> <li>uid: The uid of the user which you want to run the aptly script</li> <li>dist_name: The name of the Debian distribution which you want to publish</li> <li>root_path: The root folder path of the aptly app. This path will be different if you follow an other installation guide. If you follow my guide. It should be located at <code>~/.aptly</code>. By default, it should contain three folders <code>db</code>, <code>pool</code> and <code>public</code>. To make the corn script work, you must create three new folders(i.e. <code>log</code>, <code>creds</code>, and <code>public</code>) under the root_path.</li> <li>log_file_path: The path of the log file of the cron job</li> <li>gpg_pwd_file_path: The path of the file which contains the password of the GPG private key</li> <li>repo_path: This folder will host the source of the deb package, and it will be used by the webserver to publish the package</li> <li>export_repo_path: This folder contains the zip file of all the publish deb packages. Each zip file can be used as an export of a release.  </li> </ul> <p>The aptly root path folder should have the following shape:</p> <pre><code>.aptly/\n\u251c\u2500\u2500 creds\n\u251c\u2500\u2500 db\n\u251c\u2500\u2500 export\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 pool\n\u2514\u2500\u2500 public\n</code></pre> <ul> <li>In creds folder, it contains the passphrase of the GPG private key. This key is used to sign all the packages that are published by you. The default file name is <code>pwd</code>. If you change the file name, you need to change the config too in the cron script.</li> <li>In db folder, it contains the database of the aptly which stores the state of the aptly (e.g. mirror list, snapshot list, published package list, etc.)</li> <li>In export folder, it contains a zip file of all published packages. This zip file can be copied and deployed on another web server. The web server can act as a deb repo afterward.</li> <li>In log folder, it contains a log file of the cron script execution</li> <li>In pool folder, it contains all the packages of the available mirrors</li> <li>In public folder, it contains all the publish packages of the current deb repo.</li> </ul>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#912-handling-new-emerging-mirrors","title":"9.1.2 Handling New emerging mirrors","text":"<p>By default, the script will consider all mirrors available in the aptly, update it, create a snapshot, build a merge  of all snapshots and publish the merged snapshot.</p> <p>If you want to add a new mirror and enable the mirror update via the corn job,  you only need to create a mirror and  update the mirror. You don't need to modify the above script</p> <p>For example, we want to add two more mirrors:   - docker:    - k8s</p>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#create-mirror-for-k8s","title":"Create Mirror for k8s","text":"<pre><code># add gpg key of k8s repo as trusted key\ngpg --no-default-keyring --keyring trustedkeys.gpg --keyserver keyserver.ubuntu.com --recv-keys B53DC80D13EDEF05\n\n# create mirror\naptly mirror create -architectures=amd64 k8s-main  http://apt.kubernetes.io/ kubernetes-xenial main\n\n# update mirror\naptly mirror update k8s-main\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#create-mirror-for-docker","title":"Create Mirror for docker","text":"<pre><code># add gpg key for the containerd.io repo\ngpg --no-default-keyring --keyring trustedkeys.gpg --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8\n\n# create mirror\naptly mirror create -architectures=amd64 docker-main http://download.docker.com/linux/debian/ bullseye stable\n\n# update mirror\naptly mirror update docker-main\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#92-cron-job-for-cleaning-old-release","title":"9.2 Cron job for cleaning old release","text":"<p>The snapshot and the generated zip files will not be cleaned automatically. As a result, every week we will have at least 4 more snapshot in the db and a zip file which cost 100GB. So we need to clean them at a point to make the db clean and save some disk spaces. Two solution is possible:</p> <ul> <li>Add a task in the current cron job, and clean the old snapshots after publishing a new snapshot. Pros: The aptly db only contains currently published snapshots, so db is very clean. Cons: All history snapshots are removed. So unable to rollback in case of  problems.</li> <li>Create a new cronjob that cleans the snapshot. Pros: Can have a different run policy. So we can run it monthly, and we keep 4 weekly snapshots for rollback when needed.</li> </ul> <p>For now, I choose to use solution 2. The below cron job <code>clean_snapshot.bash</code> which cleans the snapshot</p> <pre><code># script config, you can change the days to set the snapshot interval\ndays=30\n# you need to change this value to the uid who install aptly\nuid=changeMe\nroot_path=/path/to/.aptly\nlog_file_path=\"${root_path}/log/aptly_log.out\"\nexport_repo_path=\"${root_path}/export\"\n\n\nday_in_sec=86400\ninterval=$((days * day_in_sec))\n\n# setup log \n\nexec 3&gt;&amp;1 4&gt;&amp;2\ntrap 'exec 2&gt;&amp;4 1&gt;&amp;3' 0 1 2 3\n# By default, the log will be appended to the log file. If you only want to keep the log\n# of the last run, replace &gt;&gt; by &gt;\nexec 1&gt;&gt;\"${log_file_path}\" 2&gt;&amp;1\n\n# get the list of the snapshot\nSNAP_SHOTS=$(sudo -u $uid aptly snapshot list --raw | cut -d '#' -f1 | sort | uniq | xargs)\n\n# for each snapshot check if it is older than the given day or not. If yes, then delete\nfor SNAP in $SNAP_SHOTS; do\n   echo \"Examing the snapshot: $SNAP\"\n   # delete the longest match of pattern \"-\" from the beginning\n   release_date=${SNAP##*-}\n   # convert the string date to the numeric timestamp, here we choose %s (seconde) as prcision. \n   # It can be %m (month), %d (day)\n   # current date timestamp\n   cd_timestamp=$(date --date \"$date\" +'%s')\n   # release date timestamp\n   rd_timestamp=$(date --date \"$release_date\" +'%s')\n   # get the expected timestamp\n   expected_timestamp=$((cd_timestamp - interval))\n   # compare it\n   if [ $rd_timestamp -lt $expected_timestamp ]\n   then\n      echo \"INFO: snapshot $SNAP is older than $days days\" &gt;&amp;2\n      sudo -u $uid aptly snapshot drop -force $SNAP\n   fi\ndone\n\n# clean the zip file older than the given day\nfind \"${export_repo_path}\" -name \"*.zip\" -type f -mtime +\"${days}\" -delete\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#extra-repo-mirror-for-k8s","title":"Extra repo mirror for k8s","text":""},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#k8s-main","title":"k8s-main","text":"<pre><code># add gpg key of k8s repo as trusted key\ngpg --no-default-keyring --keyring trustedkeys.gpg --keyserver keyserver.ubuntu.com --recv-keys B53DC80D13EDEF05\n\n# create mirror\naptly mirror create -architectures=amd64 k8s-main  http://apt.kubernetes.io/ kubernetes-xenial main\n\n# update mirror\naptly mirror update k8s-main\n</code></pre>"},{"location":"adminsys/os_setup/package_management/02.Install_aptly_to_build_debian_repo_server/#containerdio","title":"containerd.io","text":"<p>This mirror contains the packages of containerd (container runtime). </p> <pre><code># add gpg key for the containerd.io repo\ngpg --no-default-keyring --keyring trustedkeys.gpg --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8\n\n# create mirror\naptly mirror create -architectures=amd64 docker-main http://download.docker.com/linux/debian/ bullseye stable\n\n# update mirror\naptly mirror update docker-main\n</code></pre>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/","title":"Configure Aptly","text":"<p>Normally you don't need to modify the Aptly default configuration to use it. Because the default values are OK for  most cases. Especially, if you created an <code>additional aptly user</code> and its home directory resides on  the <code>desired big (caching) partition</code> because mirroring of repositories may require 200-400 GB.</p> <p>If your default root folder does not have enough space to host the mirrored packages, you need to change the conf to put them elsewhere.</p> <p>The official doc can be found here</p>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#default-conf","title":"Default conf","text":"<p>The default conf file is located at ~/.aptly.conf. It's in json format. </p> <p>aptly looks for configuration file first in <code>~/.aptly.conf</code> then in <code>/etc/aptly.conf</code> and, if no config file found, new one is created in home directory. If <code>-config=</code> flag is specified, aptly would use config file at specified location. Also aptly needs root directory for database, package and published repository storage. If not specified, directory defaults to <code>~/.aptly</code>, it will be created if missing.</p> <p>Below is an example of the generated <code>.aptly.conf</code> file.</p> <pre><code>aptly@debian:~$ cat .aptly.conf\n{\n  \"rootDir\": \"/home/aptly/.aptly\",\n  \"downloadConcurrency\": 4,\n  \"downloadSpeedLimit\": 0,\n  \"downloadRetries\": 0,\n  \"downloader\": \"default\",\n  \"databaseOpenAttempts\": -1,\n  \"architectures\": [],\n  \"dependencyFollowSuggests\": false,\n  \"dependencyFollowRecommends\": false,\n  \"dependencyFollowAllVariants\": false,\n  \"dependencyFollowSource\": false,\n  \"dependencyVerboseResolve\": false,\n  \"gpgDisableSign\": false,\n  \"gpgDisableVerify\": false,\n  \"gpgProvider\": \"gpg\",\n  \"downloadSourcePackages\": false,\n  \"skipLegacyPool\": true,\n  \"ppaDistributorID\": \"ubuntu\",\n  \"ppaCodename\": \"\",\n  \"skipContentsPublishing\": false,\n  \"skipBz2Publishing\": false,\n  \"FileSystemPublishEndpoints\": {},\n  \"S3PublishEndpoints\": {},\n  \"SwiftPublishEndpoints\": {},\n  \"AzurePublishEndpoints\": {},\n  \"AsyncAPI\": false,\n  \"enableMetricsEndpoint\": false\n</code></pre>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#change-the-root-dir","title":"Change the root dir","text":"<p>As we mentioned above, by default, eveything is stored in the root dir <code>~/.aptly</code>.  - The database will be sotred in <code>rootDir/db</code> - The downloaded packages will be stored in <code>rootDir/pool</code>  - The published repositories will be sotred in <code>rootDir/public</code></p> <p>So if your ~/ folder does not have enough space, you need to modify the default root dir to a bigger hard drive.</p>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#download-concurrency","title":"Download concurrency","text":"<p>We can also configure the number of parallel download threads to use when downloading packages Below conf means that the thread number will be 4</p> <pre><code>\"downloadConcurrency\": 4,\n</code></pre>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#speed-limit","title":"Speed limit","text":"<p>To avoid the aptly consume all network bandwidth, we can set a speed limits.  Below are some example conf </p> <pre><code># The limit on download bandwidth is set to 100 `kbytes per second`\n\"downloadSpeedLimit\": 100,\n\n# 0 means unlimited\n\"downloadSpeedLimit\": 0,\n</code></pre>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#architectures","title":"Architectures","text":"<p>Linux release packages for different CPU architecutres (e.g. <code>amd64 arm64 armel armhf i386 mips64el mipsel ppc64el s390x</code>). We often need to only mirror packages for one CPU architecture. This can save many disk spaces, if we don't download the package for all architectures.</p> <p>Below are some examples</p> <pre><code># we only download packages for amd64\n\"architectures\": [\"amd64\"],\n\n# if empty, by default it will download all available architectures\n\"architectures\": [],\n</code></pre>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#filesystempublishendpoints","title":"FileSystemPublishEndpoints","text":"<p>aptly defaults to publish to a single publish directory under <code>~/.aptly/public</code>. For a more advanced publishing strategy, you can define one or more <code>filesystem endpoints</code> in the FileSystemPublishEndpoints list of the aptly configuration file.</p> <p>Below are some examples</p> <pre><code>...\n\"FileSystemPublishEndpoints\": {\n  \"test1\": {\n    \"rootDir\": \"/opt/srv1/aptly_public\",\n    \"linkMethod\": \"symlink\"\n  },\n  \"test2\": {\n    \"rootDir\": \"/opt/srv2/aptly_public\",\n    \"linkMethod\": \"copy\",\n    \"verifyMethod\": \"md5\"\n  },\n  \"test3\": {\n    \"rootDir\": \"/opt/srv3/aptly_public\",\n    \"linkMethod\": \"hardlink\"\n  }\n},\n...\n</code></pre> <ul> <li>rootDir: defines the publishing directory</li> <li>linkMethod: defines how aptly links the files from the <code>internal pool</code> to the <code>published directory</code>. Three possible values: <code>hardlink</code>, <code>symlink</code> or <code>copy</code>. If not specified, empty or wrong, the default value is hardlink.</li> <li>verifyMethod: This is used only when setting the linkMethod to copy. Possible values are <code>md5</code> and <code>size</code>. It specifies how aptly compares existing links from the internal pool to the published directory. The size method compares only the file sizes, whereas the md5 method calculates the md5 checksum of the found file and compares it to the desired one. If not specified, empty or wrong, this defaults to md5.</li> </ul>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#hardlink-vs-symlink-vs-copy","title":"hardlink vs symlink vs copy","text":"<p>Underneath the file system, files are represented by inodes. (Or is it multiple inodes? Not sure.)</p> <p>A file in the file system is basically a link to an inode.</p> <p>A hard link, then, just creates another file with a link to the same underlying inode.</p> <p>When you delete a file, it removes one link to the underlying inode. The inode is only deleted (or deletable/over-writable) when all links to the inode have been deleted.</p> <p>A symbolic link is a link to another name in the file system.</p> <p>Once a hard link has been made the link is to the inode. Deleting, renaming, or moving the original file will not affect the hard link as it links to the underlying inode. Any changes to the data on the inode are reflected in all files that refer to that inode.</p> <p>Note: Hard links are only valid within the same File System. Symbolic links can span file systems as they are simply the name of other files.</p> <p>Copy will duplicate the inodes and then create a new link. This will consume hard drive space unlike the <code>symlink</code> and <code>hardlink</code></p>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#use-endpoint-when-publishing","title":"Use endpoint when publishing","text":"<p>In order to publish to a predefined endpoint, specify the endpoint as filesystem:endpoint-name with endpoint-name as the name given in the aptly configuration file. For example:</p> <pre><code># default general syntax\naptly publish snapshot -distribution=&lt;distribution-name&gt; &lt;snapshot-name&gt;\n\n# To use add file system option before snapshot-name\naptly publish snapshot -distribution=&lt;distribution-name&gt; filesystem:&lt;endpoint-name&gt;:&lt;snapshot-name&gt;\n\n# for example we want to publish snapshot bullseye-stable-20230206 to endpoint test1\naptly publish snapshot -distribution=bullseye filesystem:test1:bullseye-stable-20230206\n</code></pre>"},{"location":"adminsys/os_setup/package_management/03.Configure_aptly/#manage-gpg-keys","title":"Manage GPG keys","text":"<p>All the packages which are published by Aptly are signed with a GPG key. The client can then use the public key to verify the authenticity and integrity of the package. So when you publish the packages on the server side, aptly will try to use the GPG private key to sign them. In general, this private key is protected by a password.</p> <pre><code># change the password of the private key aptly\n# here aptly is the id of the private key\ngpg --passwd --change-passphrase aptly\n</code></pre>"},{"location":"adminsys/os_setup/package_management/04.Use_private_repo_in_debian/","title":"Use private repo","text":""},{"location":"adminsys/os_setup/package_management/04.Use_private_repo_in_debian/#add-private-repo-in-your-apt","title":"add private repo in your apt","text":"<p>suppose the url of the repo is <code>https://repolin.casd.fr</code></p> <pre><code>sudo vim /etc/apt/sources.list\n\n# add below line for a repo\ndeb [trusted=yes] https://repolin.casd.fr/ bullseye main\n</code></pre> <p>If the certificate is not signed by a known CA. You can install the certificate </p> <pre><code># get the certificate of the private repo\nopenssl s_client -connect {HOSTNAME}:{PORT} -showcerts\n\n# in our case\nopenssl s_client -connect repolin.casd.fr:443 -showcerts\n\n# copy the output certificate in a file and put it in below folder\nsudo mv casd-root.crt /usr/local/share/ca-certificates/\n# update the certificate list\nrun sudo update-ca-certificates\n\n# update the apt repo list\nsudo apt update\n\n# update packages\nsudo apt upgrade\n</code></pre>"},{"location":"adminsys/os_setup/security/","title":"Debian server security docs","text":"<p>In this folder, we store all documentation about debian server security.</p>"},{"location":"adminsys/os_setup/security/#1-sshd-authentication","title":"1. SSHD Authentication","text":"<p>The most common way to remote access a debian server is via ssh protocol. On the sever side, a sshd daemon runs as  ssh server that listens to port 22 (default port). It supports many authentication mechanisms such as:</p> <ul> <li>Password Authentication (Using /etc/shadow) : This method only works for local users (not LDAP, SSSD, or Kerberos users). In <code>/etc/ssh/sshd_config</code>, put <code>UsePAM no \\n PasswordAuthentication yes</code></li> <li>Public Key Authentication (No Password Required): SSHD checks if the user private key matches a valid public key in ~/.ssh/authorized_keys.</li> <li>GSSAPI/Kerberos Authentication: SSHD can authenticate users using GSSAPI (Kerberos-based authentication) without PAM</li> <li>PAM (Pluggable Authentication Modules) Recommended: It supports multiple authentication methods (LDAP, Kerberos, SSSD, MFA).</li> </ul>"},{"location":"adminsys/os_setup/security/#11-terms","title":"1.1 Terms","text":"<p>On linux server, to allow user remote access, we use many daemons:</p> <ul> <li>sshd</li> <li>pam(Pluggable Authentication Modules):</li> <li>sssd(System Security Services Daemon)Recommended: Provides access to remote identity and authentication providers, such as LDAP, Active Directory (AD), FreeIPA, or Kerberos.</li> <li>nslcd(Name Service LDAP Daemon) deprecated: Connects the Name Service Switch (NSS) and PAM to an LDAP directory for user authentication and identity lookup.</li> <li>nscd(Name Service Cache Daemon): Caches results from services like DNS and LDAP to reduce query load. sssd has its own caching mechanism, do not recommend when using sssd.</li> </ul>"},{"location":"adminsys/os_setup/security/#12-ssh-client-server-authentication-workflow","title":"1.2 SSH client server authentication workflow","text":"<p>In the below section, we describe the authentication  workflow of a ssh server configured with <code>sshd -&gt; pam -&gt; sssd -&gt; krb/openldap</code></p>"},{"location":"adminsys/os_setup/security/#step-1-ssh-client-initiates-connection","title":"Step 1. SSH Client Initiates Connection","text":"<p>A user runs:</p> <pre><code>ssh user@debian-server\n</code></pre> <p>The SSH daemon (sshd) on the Debian server receives the connection request and begins the authentication process.</p>"},{"location":"adminsys/os_setup/security/#step2-sshd-hands-authentication-to-pam","title":"Step2. SSHD Hands Authentication to PAM","text":"<p>SSHD is configured to use PAM (Pluggable Authentication Modules) for user authentication. It checks /etc/pam.d/sshd, which includes configurations for authentication backends. PAM invokes the relevant authentication module, in this case, SSSD.</p>"},{"location":"adminsys/os_setup/security/#step3-pam-calls-sssd-for-authentication","title":"Step3. PAM Calls SSSD for Authentication","text":"<p>PAM is configured to use SSSD via the module: /etc/pam.d/common-auth</p> <p>You should see the below line which tells pam to query sssd for authentication</p> <pre><code>auth    [success=1 default=ignore]    pam_sss.so\n</code></pre>"},{"location":"adminsys/os_setup/security/#step-4-sssd-queries-openldapkerberos-for-user-authentication","title":"Step 4. SSSD Queries OpenLDAP/kerberos for User Authentication","text":"<p>SSSD Configuration (/etc/sssd/sssd.conf) specifies OpenLDAP as the backend. SSSD checks if the credentials are cached: If cached, it allows authentication without querying OpenLDAP (useful for offline authentication). If not cached, SSSD sends the authentication request to OpenLDAP.</p>"},{"location":"adminsys/os_setup/security/#step-5-openldap-validates-user-credentials","title":"Step 5. OpenLDAP Validates User Credentials","text":"<p>OpenLDAP checks: If the user exists in the LDAP directory (uid=user). The password stored in LDAP. If using LDAP bind authentication, OpenLDAP attempts to bind as the user with the provided password. If using Kerberos (via LDAP), OpenLDAP defers authentication to a Kerberos KDC.</p>"},{"location":"adminsys/os_setup/security/#step-6-authentication-result-passed-back","title":"Step 6. Authentication Result Passed Back","text":"<p>If authentication is successful, OpenLDAP responds to SSSD. SSSD caches the credentials (if caching is enabled). SSSD informs PAM of the successful authentication. PAM notifies SSHD that the user is authenticated.</p>"},{"location":"adminsys/os_setup/security/#step-7-sshd-grants-access","title":"Step 7. SSHD Grants Access","text":"<p>If the user has the correct authorization (i.e., shell access, SSH keys, group policies), SSHD grants access. The user gets a shell on the Debian server. Authentication Flow Summary SSH Client \u2192 Requests login from SSHD. SSHD \u2192 Delegates authentication to PAM. PAM \u2192 Calls pam_sss.so to use SSSD. SSSD \u2192 Queries OpenLDAP (or checks cache). OpenLDAP \u2192 Verifies user credentials. SSSD \u2192 Returns authentication result to PAM. PAM \u2192 Informs SSHD. SSHD \u2192 Grants or denies access.</p>"},{"location":"adminsys/os_setup/security/#additional-notes","title":"Additional Notes","text":"<p>If 2FA (Two-Factor Authentication) is enabled, PAM may prompt for additional verification. If public key authentication is used, SSHD may bypass PAM and authenticate using the user's SSH key. If the LDAP server is down, authentication fails unless SSSD has cached credentials. Authorization (e.g., checking if a user belongs to a specific group) is usually done via sssd's access_provider settings.</p> <p><code>/etc/pam.d/sshd</code></p> <pre><code># PAM configuration for the Secure Shell service\n\n# Standard Un*x authentication.\n@include common-auth\n\n# Disallow non-root logins when /etc/nologin exists.\nauth       required     pam_env.so\nauth       sufficient   pam_unix.so nullok \nauth       sufficient   pam_sss.so use_first_pass\nauth       required     pam_deny.so\n\naccount    required     pam_unix.so\naccount    sufficient   pam_sss.so\n\npassword   required    pam_sss.so\nsession    required    pam_sss.so\n\n# Uncomment and edit /etc/security/access.conf if you need to set complex\n# access limits that are hard to express in sshd_config.\n# account  required     pam_access.so\n\n# Standard Un*x authorization.\n@include common-account\n\n# SELinux needs to be the first session rule.  This ensures that any\n# lingering context has been cleared.  Without this it is possible that a\n# module could execute code in the wrong domain.\nsession [success=ok ignore=ignore module_unknown=ignore default=bad]        pam_selinux.so close\n\n# Set the loginuid process attribute.\nsession    required     pam_loginuid.so\n\n# Create a new session keyring.\nsession    optional     pam_keyinit.so force revoke\n\n# Standard Un*x session setup and teardown.\n@include common-session\n\n# Print the message of the day upon successful login.\n# This includes a dynamically generated part from /run/motd.dynamic\n# and a static (admin-editable) part from /etc/motd.\nsession    optional     pam_motd.so  motd=/run/motd.dynamic\nsession    optional     pam_motd.so noupdate\n\n# Print the status of the user's mailbox upon successful login.\nsession    optional     pam_mail.so standard noenv # [1]\n\n# Set up user limits from /etc/security/limits.conf.\nsession    required     pam_limits.so\n\n# Read environment variables from /etc/environment and\n# /etc/security/pam_env.conf.\nsession    required     pam_env.so # [1]\n# In Debian 4.0 (etch), locale-related environment variables were moved to\n# /etc/default/locale, so read that as well.\nsession    required     pam_env.so user_readenv=1 envfile=/etc/default/locale\n\n# SELinux needs to intervene at login time to ensure that the process starts\n# in the proper default security context.  Only sessions which are intended\n# to run in the user's context should be run after this.\nsession [success=ok ignore=ignore module_unknown=ignore default=bad]        pam_selinux.so open\n\n# Standard Un*x password updating.\n@include common-password\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_local_account/","title":"Configure ssh server on debian 11","text":"<p>An <code>SSH Server</code> is a service that runs as a <code>daemon background process (systemd service)</code> on a computer (server) and  allows <code>secure remote access</code> using the <code>Secure Shell (SSH) protocol</code>. </p> <p>It listens for incoming connections on <code>port 22 (by default)</code>. It then authenticates users, creates a  secure encrypted session, and allows remote execution of commands.</p> <p>It enables users to:   - Log in remotely by using <code>passwords, SSH keys, or Kerberos authentication</code>.   - Execute commands on a remote system.   - Transfer files securely (using scp or sftp).   - Forward network connections (port forwarding, tunneling).</p>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_local_account/#general-workflow","title":"General workflow","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/","title":"Configure debian server ssh to use pam ldap","text":"<p>We will use libpam-ldapd as the ldap server client and server authenticator to check user login and password via ldap server. It is a newer alternative to the original <code>libpam-ldap</code>. libpam-ldapd uses the same backend <code>(nslcd)</code> as <code>libnss-ldapd</code>, and thus also shares the same configuration file <code>(/etc/nslcd.conf)</code> for LDAP connection parameters. If you're already using libnss-ldapd for NSS, it may be more convenient to use libpam-ldapd's pam_ldap implementation.</p> <p>The /etc/pam.d/common-* files are managed by pam-auth-update (from libpam-runtime).</p> <p>The libpam-ldapd package includes <code>/usr/share/pam-configs/ldap</code>, and running <code>dpkg-reconfigure libpam-runtime</code> will let you configure the <code>pam_unix/pam_ldap</code> module(s) to use in /etc/pam.d/common-*.</p> <p>The nslcd is the name service LDAP connection daemon.</p> <p>Installing the libpam-ldapd package will automatically select the pam_ldap module for use in /etc/pam.d/common-*.</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#61-install-the-required-packages","title":"6.1 Install the required packages","text":"<pre><code>sudo apt-get install libnss-ldapd libpam-ldapd\n</code></pre> <p>After the installation, a pop-up window will require you to enter the <code>ldap uri</code> and the <code>base dn</code> of the ldap server</p> <p>For example</p> <pre><code>ldap_uri: ldap://10.50.5.57/ or ldap://ldap.casd.local/\n\nldap_base_dn: dc=casd,dc=local\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#62-edit-the-config","title":"6.2 Edit the config","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#621-the-first-config-is-etcnslcdconf","title":"6.2.1 The first config is <code>/etc/nslcd.conf</code>","text":"<p>As you already enter some information during installation. This file is filled with some info.</p> <p>Below is a working example.</p> <pre><code># /etc/nslcd.conf\n# nslcd configuration file. See nslcd.conf(5)\n# for details.\n\n# The user and group nslcd should run as.\nuid nslcd\ngid nslcd\n\n# The location at which the LDAP server(s) should be reachable.\nuri ldap://10.50.5.57/\n\n# The search base that will be used for all queries.\nbase dc=casd,dc=local\n\n# The LDAP protocol version to use.\n#ldap_version 3\n\n# The DN to bind with for normal lookups.\n#binddn cn=annonymous,dc=example,dc=net\n#bindpw secret\n\n# The DN used for password modifications by root.\n#rootpwmoddn cn=admin,dc=example,dc=com\n\n# SSL options\n#ssl off\n#tls_reqcert never\n# tls_cacertfile /etc/ssl/certs/ca-certificates.crt\n\n# The search scope.\n#scope sub\n</code></pre> <p>The good practice is not write the <code>binddn</code> and <code>bindpw</code> with admin privilege. If you leave it empty, <code>pam-ldapd</code> will use the current user login and pwd to bind to the ldap. So it's safer.</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#622-etcnsswitchconf","title":"6.2.2 /etc/nsswitch.conf","text":"<p>Change the old version to below version</p> <pre><code># /etc/nsswitch.conf\n#\n# Example configuration of GNU Name Service Switch functionality.\n# If you have the `glibc-doc-reference' and `info' packages installed, try:\n# `info libc \"Name Service Switch\"' for information about this file.\n\npasswd:         files ldap\ngroup:          files ldap\nshadow:         files ldap\ngshadow:        files\n\nhosts:          files dns\nnetworks:       files\n\nprotocols:      db files\nservices:       db files\nethers:         db files\nrpc:            db files\n\nnetgroup:       nis\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#623-etcpamdcommon-","title":"6.2.3  /etc/pam.d/common-*","text":"<p>There are a list of config files for pam which are located at  /etc/pam.d/. In our case, we need to modify: - /etc/pam.d/common-auth - /etc/pam.d/common-account - /etc/pam.d/common-session - /etc/pam.d/common-password</p> <pre><code>sudo vim /etc/pam.d/common-auth\n\n# comment the old content, and add below line\nauth      sufficient  pam_unix.so\nauth      sufficient  pam_ldap.so minimum_uid=1000 use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-account\n# comment the old content, and add below line\naccount   required    pam_unix.so\naccount   sufficient  pam_ldap.so minimum_uid=1000\naccount   required    pam_permit.so\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-session\n# comment the old content, and add below line\nsession   required    pam_unix.so\nsession   optional    pam_ldap.so minimum_uid=1000\n# this line will create the user home for first login\nsession    required   pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-password\n# comment the old content, and add below line\npassword  sufficient  pam_unix.so nullok md5 shadow use_authtok\npassword  sufficient  pam_ldap.so minimum_uid=1000 try_first_pass\npassword  required    pam_deny.so\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#624-etcsshsshd_config","title":"6.2.4 /etc/ssh/sshd_config","text":"<p>Normally, you don't need to modify the  /etc/ssh/sshd_config. Because the <code>libpam-ldapd</code> will set UsePAM yes automatically for sshd to use PAM authentication.</p> <p>If you have troubles, don't forget to check </p> <p>The above conf is the minimun for the pam-ldapd works. You need to enrich it if you have special requirements</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#63-restart-the-service","title":"6.3 Restart the service","text":"<p>As we metioned before, the</p> <pre><code># check the status of the daemon\nsudo systemctl status nscd\nsudo systemctl status nslcd\n\n# restart the service\nsudo systemctl restart nscd\nsudo systemctl restart nslcd\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#64-test-and-troubleshoot","title":"6.4 Test and troubleshoot","text":"<p>To ensure that everything is working correctly you can run </p> <pre><code># this command prints all user account of the server which also includes the users from LDAP\ngetent passwd\n\n# below is an example of user passwd from ldap\ntrigaud:x:3000:3000:Titouan:/home/trigaud:/bin/bash\n\n# below can show the user shadow form ldap too\ngetent shadow \n</code></pre> <p>To test authentication log in with an LDAP user, you can run below command</p> <pre><code># general form to local login\nsu - &lt;UID&gt;\n\n# for example, run below command and enter the pwd. if it's correct, \nsu - trigaud\n</code></pre> <p>To troubleshoot problems you can run <code>nslcd in debug mode</code> (remember to stop nscd when debugging). Debug mode should return a lot of information about the LDAP queries that are performed and errors that may arise.</p> <pre><code>/etc/init.d/nscd stop\n/etc/init.d/nslcd stop\nnslcd -d\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_ldap/#for-ad-compatibility","title":"For AD compatibility","text":"<p>To use AD as authentication server, we can't use <code>nslcd</code> anymore. We need to test the <code>sssd</code> and <code>AD</code> connexion.</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_sssd_ldap/","title":"Configure debian server ssh to use sssd","text":"<p>In </p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/","title":"Configure debian server to use AD/Krb for sshd authentication","text":"<p>In this tutorial, we show how to configure sshd, pam, sssd, to allow a <code>debian 11</code> server to use AD/Krb as  authentication server. We will follow the below steps:</p> <p>We suppose we have : - <code>AD/Krb</code> : The ip address is <code>10.50.5.64</code>, ad domain name <code>casdds.casd</code>, krb realm name <code>CASDDS.CASD</code>, hostname <code>auth</code>, fqdn is <code>auth.casdds.casd</code> - <code>debian 11</code>: ip address is <code>10.50.5.199</code>, hostname is <code>hadoop-client</code>, fqdn is <code>hadoop-client.casdds.casd</code></p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-1-prerequisite","title":"Step 1: Prerequisite","text":""},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#11-reset-hostname-of-hadoop-client","title":"1.1 Reset hostname of hadoop-client","text":"<p>The hostname is essential for the server to have a valid FQDN in the domain, so we need to make sure the hostname is set correctly. Follow the below steps: - set system hostname - update /etc/hosts</p> <pre><code># general form\nsudo hostnamectl set-hostname &lt;custom-hostname&gt;\n\n# for example \nsudo hostnamectl set-hostname hadoop-client\n\n# check the new hostname with below command\nhostname\n\n# expected output\nhadoop-client\n</code></pre> <p>you can also directly edit the hostname config file(not recommended) by using <code>sudo vim /etc/hostname</code></p> <p>Update <code>/etc/hosts</code>:</p> <pre><code>sudo vim /etc/hosts \n\n127.0.1.1 hadoop-client.casdds.casd hadoop-client\n10.50.5.199 hadoop-client.casdds.casd   hadoop-client\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#12-update-system-packages-in-hadoop-client","title":"1.2 Update system packages in hadoop-client","text":"<pre><code>sudo apt update \nsudo apt upgrade\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#13-change-dns-server-settings-in-hadoop-client","title":"1.3 Change dns server settings in hadoop-client","text":"<p>To join the server into an AD domain, you must use the AD as dns server.</p> <p>Edit the <code>/etc/resolv.conf</code> :</p> <pre><code>search casdds.casd\nnameserver 10.50.5.64\nnameserver 8.8.8.8\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#14-install-the-required-packages-in-hadoop-client","title":"1.4 Install the required packages in hadoop-client","text":"<pre><code>sudo apt install realmd sssd sssd-tools libnss-sss libpam-sss adcli samba-common-bin krb5-user oddjob oddjob-mkhomedir packagekit -y\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-2-join-the-debian-serverhadoop-client-to-the-ad-domain","title":"Step 2 : Join the debian server(hadoop-client) to the AD domain","text":""},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#21-check-if-the-domain-can-be-reached-or-not","title":"2.1. Check if the domain can be reached or not","text":"<pre><code>realm discover CASDDS.CASD\n</code></pre> <ul> <li>If the error message is realm command is unknown, open a new shell.</li> <li>If the error message is CASDDS.CASD is unknown, check the dns server ip is reachable, and dns server name setup is correct.</li> </ul>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#22-join-the-serverhadoop-client-to-the-ad-domain","title":"2.2. Join the server(hadoop-client) to the AD domain","text":"<p>To execute the below command, you must have an account with <code>domain administrator</code> privilege :</p> <pre><code>sudo realm join --user=Administrateur CASDDS.CASD\n</code></pre> <p>If there is no error message, it means your server has joined the domain.</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#23-configure-the-linux-serverhadoop-client-account-in-ad","title":"2.3 Configure the linux server(hadoop-client) account in AD","text":"<p>If the <code>hadoop-client</code> has success joined the AD domain, you should see the server appears in the <code>Computer</code> section in the AD manager GUI. Check the below figure</p> <p></p> <p>To check, you need to connect to the <code>Windows Server</code> -&gt; Open <code>AD manager</code> -&gt; In <code>Users and Computers</code> subfolder of  Active Directory. You should find a line of <code>HADOOP-CLIENT</code>. Right Click on it, and select <code>properties</code>, you should see the below pop-up window</p> <p></p> <p>Select the <code>Trust this computer for delegation to any service</code> option in <code>Delegation</code>. </p> <p>Click on the <code>Static IP address</code> option in <code>Dial-in</code>, then put the address ip of the <code>hadoop-client</code>.  </p> <p>You can add a new computer in AD manually, but we don't recommend that. Try to use the <code>realm join</code></p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-3-config-adkrb-dns-server-to-well-integrate-hadoop-client","title":"Step 3: Config AD/Krb, DNS server to well integrate hadoop-client","text":"<p>To make the debian server (hadoop-client) fqdn <code>recognizable</code> and <code>reachable</code> by the other servers in the domain, we need to configure the dns server </p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#31-check-the-dns-entries-in-windows-server","title":"3.1. Check the dns entries in windows server","text":"<p>Open the <code>dns manager</code> in the Windows server (<code>auth.casdds.casd</code>). Check the forward lookup and reverse lookup. You need to make sure the <code>hostname, fqdn and ip address</code> are correct. The two below figures are examples of the <code>hadoop-client</code> config.</p> <p></p> <p></p> <p>Normally, these entries are created automatically by the <code>realm join</code> command. If they are not created correcly, you  need to create them manually.</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#32-check-the-spn-service-principal-name-in-windows-server","title":"3.2. Check the SPN (Service Principal Name) in Windows server","text":"<p>Every registered computer in the domain should have a <code>valid SPN (Service Principal Name)</code>. You can check the name by  using the below command. You can open a <code>powershell prompt</code> in the <code>AD/krb</code> server.</p> <pre><code>setspn -L hadoop-client\n\n# expected output\nRegistered ServicePrincipalNames for CN=HADOOP-CLIENT,CN=Computers,DC=casdds,DC=casd:\n        RestrictedKrbHost/hadoop-client.casdds.casd\n        RestrictedKrbHost/HADOOP-CLIENT\n        host/hadoop-client.casdds.casd\n        host/HADOOP-CLIENT\n</code></pre> <p>If you don't see any outputs, you can create a <code>SPN (Service Principal Name)</code> manually. Below is the command to do so.</p> <pre><code># create a new spn and link it to the hadoop-client(AD account)\n# The -S option adds an SPN only if it does not already exist (avoids duplicates).\nsetspn -S host/hadoop-client.casdds.casd hadoop-client\n\n# generate a keytab for principal  host/hadoop-client.casdds.casd@CASDDS.CASD\nktpass -princ host/hadoop-client.casdds.casd@CASDDS.CASD -mapuser HADOOP-CLIENT$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 -out hadoop-client.keytab\n</code></pre> <p>Below lines are the explanation of the commands: - The <code>ktpass</code> command can generate a keytab file for Kerberos authentication.  - <code>-princ host/hadoop-client.casdds.casd@CASDDS.CASD</code> defines the Kerberos principal name. - <code>-mapuser HADOOP-CLIENT$</code> maps the Kerberos principal to the account (HADOOP-CLIENT) in Active Directory (AD). The <code>$</code> indicates it's a computer account (not a user). - <code>-crypto AES256-SHA1</code> specifies that only the <code>AES256-SHA1</code> crypto algo is supported. You can replace it with <code>ALL</code> to specify all available cryptographic algorithms should be supported for encryption. - <code>-ptype KRB5_NT_PRINCIPAL</code> specifies the principal type as KRB5_NT_PRINCIPAL, which is used for standard Kerberos authentication(for services, use KRB5_NT_SRV_HST). - <code>-pass *</code> prompts the user to enter the password manually. Typically, computer accounts in AD have auto-generated passwords. - <code>-out hadoop-client.keytab</code> saves the keytab file, which will be used by the linux server(hadoop-client) for authentication.</p> <p>The <code>hadoop-client.keytab</code> file is copied to the hadoop-client, so it can use this keytab for Kerberos authentication. <code>hadoop-client</code> can use the kerberos ticket to prove the identity of <code>hadoop-client</code>.</p> <p>After coping the <code>hadoop-client.keytab</code> file to <code>hadoop-client</code>, you can use the below command to check keytab contents:</p> <pre><code>klist -k /tmp/hadoop-client.keytab  \n\n# you should see outputs like\n\"host/hadoop-client.casdds.casd\"\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#33-rename-the-keytab-file-in-linuxhadoop-client","title":"3.3 Rename the keytab file in linux(hadoop-client)","text":"<p>In linux, many Kerberos-aware applications (e.g. kinit, Hadoop, etc.) expect the keytab file to be named <code>krb5.keytab</code>  and located in <code>/etc/</code> by default. We can use the below commands</p> <pre><code>sudo mv hadoop-client.keytab /etc/krb5.keytab\n\n# Ensures only root can read it (for security).\nsudo chmod 600 /etc/krb5.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#34-leave-and-rejoin-the-realm","title":"3.4 Leave and rejoin the realm","text":"<p>If there are errors that you can't resolve, you can always leave the realm and rejoin</p> <pre><code>sudo realm leave CASDDS.CASD\n\nsudo realm join --user=Administrateur CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#35-create-a-service-account-in-ad-for-sssd-daemon","title":"3.5. Create a service account in AD for sssd daemon.","text":"<p>As we explained before, the linux server relies on <code>sssd</code> daemon to get the <code>user id and groups</code> from the AD server. This requires sssd to have an account that allows him to access AD.</p> <p>You need to create a service account <code>sssd</code> in <code>Active Directory manager</code>. Then use the below command to create a <code>principal</code> and the <code>keytab</code> file.</p> <pre><code>ktpass -princ sssd@CASDDS.CASD -mapuser sssd -crypto AES256-SHA1 -ptype KRB5_NT_PRINCIPAL -pass * -out sssd.keytab\n</code></pre> <p>Copie the keytab file to the debian server(hadoop-client):</p> <pre><code>scp sssd.keytab sssd@debian.casdds.casd:/tmp/\n</code></pre> <p>Put the keytab file in /etc</p> <pre><code>sudo cp /tmp/sssd.keytab /etc/\n# need to check the acl of the file, 644 is too open for me\nsudo chmod 644 /etc/sssd.keytab\nsudo chown root:root /etc/sssd.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-4-configuration-of-sssd-pam-and-kerberos","title":"Step 4 : Configuration of SSSD, PAM and Kerberos","text":"<p>We will follow the below order to configure each component: - kerberos client: configure krb client to connect to the target krb Realm - sshd/pam: configure sshd server to use pam as authentication backend - pam/sssd: configure pam to use sssd as backend - sssd/krb: configure sssd to use krb plugin</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#41-configure-kerberos-client-in-debianhadoop-client-server","title":"4.1 Configure kerberos client in debian(hadoop-client) server","text":"<pre><code># install the required package\nsudo apt install krb5-user\n\n# edit the config file `/etc/krb5.conf`  \nsudo vim /etc/krb5.conf\n</code></pre> <p>Put the below content in the file <code>/etc/krb5.conf</code> </p> <pre><code> [libdefaults]\n        default_realm = CASDDS.CASD\n\n        default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        permitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        # Fadoua said this must be removed from, otherwise the ticket will not be forwad to the target host \n        # proxiable = true\n        ticket_lifetime = 24h\n        dns_lookup_realm = true\n        dns_lookup_kdc = true\n        dns_canonicalize_hostname = false\n        rdns = false\n         allow_weak_crypto = true\n\n\n[realms]\n        CASDDS.CASD = {\n                kdc = 10.50.5.64\n                admin_server = 10.50.5.64\n        }\n\u2026..\n[domain_realm]\n\u2026.\n        .casdds.casd = CASDDS.CASD\n        casdds.casd = CASDDS.CASD\n</code></pre> <p>To check the krb client, use the below command</p> <pre><code># ask a ticket kerberos\nkinit host/hadoop-client.casdds.casd\n\n# the short version should work if the keytab is in place, if not you can specify the path of keytab\nkinit -kt /etc/krb5.keytab host/hadoop-client.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#42-configure-sshd-to-use-pam","title":"4.2. Configure sshd to use pam","text":"<p>We need to edit two files: - <code>/etc/ssh/sshd_config</code> (configuration for the ssh server) - <code>/etc/ssh/ssh_config</code> (configuration for the ssh client)</p> <p>In <code>/etc/ssh/sshd_config</code>, enable the below lines</p> <pre><code># disable other authentication methods\nChallengeResponseAuthentication no\nPasswordAuthentication no\n\n# use pam as authentication backend\nUsePAM yes\n\n# GSSAPI options for sshd server to accept GSSAPI, it's required for the server to accept krb ticket as \n# credentials\n# \nGSSAPIAuthentication yes\n# Cleans up the Kerberos credentials after the session.\nGSSAPICleanupCredentials yes\n# Ensures that the SSH client does not strictly check for a valid acceptor name in the Kerberos tickets.\nGSSAPIStrictAcceptorCheck no\n# Allows the exchange of Kerberos keys for stronger encryption.\nGSSAPIKeyExchange yes\n\n\nX11Forwarding yes\n\nPrintMotd no\n\n\n# Allow client to pass locale environment variables\nAcceptEnv LANG LC_*\n\n# override default of no subsystems\nSubsystem       sftp    /usr/lib/openssh/sftp-server\n</code></pre> <p>You need to restart the sshd service to enable the new config </p> <pre><code>sudo systemctl restart sshd\n</code></pre> <p>In the <code>/etc/ssh/ssh_config</code>, you need to add the below line </p> <pre><code>   Host *\n       GSSAPIAuthentication yes\n       GSSAPIDelegateCredentials yes\n       PasswordAuthentication no\n</code></pre> <p>For hadoop-client, the <code>ssh_config</code> is not required, because it defines the behaviour of the ssh client. It needs to be configured in the ssh client which wants to connect to the hadoop-client ssh server. </p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#43-configure-pam","title":"4.3 Configure pam","text":"<p>All the configuration files for pam are located in <code>/etc/pam.d/</code>. The below is the minimum config for the pam to use sssd daemon as authentication backend.</p> <pre><code>### /etc/pam.d/common-auth\nsudo: unable to resolve host debian118: Name or service not known\nauth      sufficient  pam_unix.so try_first_pass\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-account\nsudo: unable to resolve host debian118: Name or service not known\naccount   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <pre><code>### /etc/pam.d/common-password\nsudo: unable to resolve host debian118: Name or service not known\npassword  sufficient  pam_unix.so\npassword  sufficient  pam_sss.so\npassword  required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-session\nsudo: unable to resolve host debian118: Name or service not known\nsession   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#44-configure-sssd","title":"4.4 Configure sssd","text":"<p>Now we need to configure the sssd daemon. The main config file is in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[sssd]\nservices = nss, pam\ndomains = casdds.casd\nconfig_file_version = 2\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n\n[domain/casdds.casd]\nldap_sasl_authid = sssd@CASDDS.CASD\nkrb5_keytab = /etc/sssd.keytab\ndefault_shell = /bin/bash\nkrb5_store_password_if_offline = True\ncache_credentials = True\nkrb5_realm = CASDDS.CASD\nrealmd_tags = manages-system joined-with-adcli\nid_provider = ad\nfallback_homedir = /home/%u@%d\nad_domain = casdds.casd\nuse_fully_qualified_names = False\nldap_id_mapping = True\naccess_provider = ad\nldap_group_nesting_level = 2\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#5configure-ssh-client-on-windows","title":"5.configure ssh client on Windows","text":"<p>In windows, there are many ssh clients: - MobaXterm: - tabby: https://tabby.sh/ - powershell+openssh - PuTTY</p> <p>Below is the instruction on how to install and configure openssh via PowerShell</p> <pre><code>Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\nStart-Service sshd\nSet-Service -Name sshd -StartupType 'Automatic'\n</code></pre> <p>In windows, all the configuration file for openssh is located in <code>C:\\ProgramData\\ssh</code> If you want to setup the config for ssh server, you can edit the file in <code>C:\\ProgramData\\ssh\\sshd_config</code>.</p> <p>To restart ssh service in windows</p> <pre><code># start sshd service\nStart-Service sshd\n\n# restart sshd service \nRestart-Service sshd\n</code></pre> <p>Configure ssh client </p> <pre><code># open a notepad\nnotepad $env:USERPROFILE\\.ssh\\config\n\n# add the below lines\n# * means for all hosts\nHost *\n    GSSAPIAuthentication yes\n    GSSAPIDelegateCredentials yes \n</code></pre> <p>You can also define the behaviors host by host, below is an example</p> <pre><code>Host hadoop-client\n    HostName hadoop-client.casdds.casd\n    User pengfei@casdds.casd\n    Port 22\n    GSSAPIAuthentication yes\n    GSSAPIDelegateCredentials yes \n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#step-6-test-the-solution","title":"Step 6 : Test the solution","text":"<p>In our scenario, the user follow the below steps: 1. first login to a Windows server, the first ticket kerberos is generated in the Windows server. 2. user ssh to hadoop-client with the ticket kerberos with option forward ticket 3. user try to access hdfs cluster with the forward kerberos ticket</p> <p>Suppose you have an account <code>user</code> in AD with the privilege to connect to <code>hadoop client</code>  \\</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#61-understand-the-ticket","title":"6.1. Understand the ticket","text":"<p>In linux, you can ask a ticket and check the ticket with the below command</p> <pre><code># ask a new ticket, you need to provide a password associated with the provided principal\nkinit user@CASDDS.CASD  \n\n# check the ticket contents\nklist -5fea   \n</code></pre> <p>The option: - 5: Show only Kerberos 5 tickets (modern Kerberos version). - f: Show ticket flags (like FORWARDABLE, RENEWABLE, etc.). - e: Display encryption type used for the ticket. - a Show addresses associated with the ticket (if address-restriction of the ticket is activated).</p> <p>You should see the below output as the ticket content</p> <pre><code>Ticket cache: FILE:/tmp/krb5cc_1000\nDefault principal: user@CASDDS.CASD\n\nValid starting       Expires              Service principal\n03/31/25 10:00:00  03/31/25 20:00:00  krbtgt/CASDDS.CASD@CASDDS.CASD\n        Flags: FRI\n        Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96\n        Addresses: 192.168.1.100\n</code></pre> <p>A kerberos ticket has the below properites:</p> <ul> <li>Ticket cache: Location of the ticket. </li> <li>Default principal: Your Kerberos identity (user@EXAMPLE.COM).</li> <li>Valid starting / Expires: Time range for which the ticket is valid.</li> <li>Service principal: The Kerberos service this ticket is for. (krbtgt/CASDDS.CASD@CASDDS.CASD is a tgt issued by CASDDS.CASD the kdc server)</li> <li>Flags (-f option): F = Forwardable (Can be forwarded to another machine). R = Renewable (Can be extended before expiration). I = Initial (Freshly obtained).</li> <li>Encryption type (-e option): aes256-cts-hmac-sha1-96, means AES-256 encryption with SHA-1 HMAC.</li> <li>Addresses (-a option): Shows the IP addresses associated with the ticket (if address-restricted).</li> </ul> <p>You can ask ticket with special options:</p> <pre><code># below command ask a Forwardable, Renewable for a 7 day validity\nkinit -f -r 7d\n</code></pre> <p>Based on the kdc configuration, it may or may not generate the ticket.</p>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#62-connexion-ssh","title":"6.2. Connexion SSH","text":"<p>From windows, if the server has joined the domain, windows will generate a kerberos ticket after user logon:</p> <pre><code># check the user ticket\nklist -5fea\n\n# for windows ssh client\n# -K active la d\u00e9l\u00e9gation Kerberos\nssh -K user@debian.casdds.casd  \n\n# for linux ssh client\nssh -o GSSAPIDelegateCredentials=yes user@debian.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#appendix","title":"Appendix :","text":""},{"location":"adminsys/os_setup/security/04.Configure_ssh_pam_sssd_ad_en/#acl-for-etcsssdsssdconf","title":"ACL for /etc/sssd/sssd.conf","text":"<p>The Permissions for <code>/etc/sssd/sssd.conf</code> must be <code>600</code> :</p> <pre><code>sudo chmod 600 /etc/sssd/sssd.conf\n</code></pre>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/","title":"Configure sssd to use static uid, gid","text":"<p>POSIX (Portable Operating System Interface) is <code>UNIX/Linux standards for identity and access control</code>. A <code>POSIX</code>  account use specific attributes such as - uidNumber \u2013 Unique User ID (UID) - gidNumber \u2013 Primary Group ID (GID) - homeDirectory \u2013 User's home directory path  - loginShell \u2013 The default shell (e.g., /bin/bash)</p> <p>These attributes allow UNIX/Linux systems to recognize, authenticate users, and create user workspace.</p> <p>By default, AD does not use POSIX attributes for user and group.  Instead, AD relies on:</p> <ul> <li>Security Identifiers (SIDs): Every user and group has a <code>SID</code>, which is a unique identifier in Windows.</li> <li><code>sAMAccountName</code>: pliu (This is legacy login, )</li> <li>UserPrincipalName (UPN): pliu@casd.eu (authentication in modern windows server)</li> </ul>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#1-default-behavior-when-sssd-uses-ad-as-authentication-backend","title":"1. Default behavior when sssd uses AD as authentication backend","text":"<p>The default behavior in <code>SSSD</code> and <code>Winbind</code> is to use <code>auto id mapping</code>. SSSD will dynamically generate UIDs and GIDs from the <code>AD objects's ObjectSID</code>. This will lead to inconsistent UIDs and GIDs across machines.</p> <p>In certain scenarios, it will create conflicting ACL in user home. For example, if a user with AD account with name <code>test</code> login to a linux server, a user home will be created <code>/home/test</code>. If the user account is deleted, and a new account <code>test</code> is created, when the new <code>test</code> user login to the linux server, it will user /home/test as home dir too. But the new and old <code>test</code> will have different uid. So the old files in /home/test will have old uid as owner, the new  <code>test</code> user can't access it. </p> <p>To avoid inconsistent UIDs and GIDs, we recommend you to use static uidNumber and gidNumber.</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#2-configure-sssd-to-user-static-uidnumber-and-gidnumber","title":"2. Configure sssd to user static uidNumber and gidNumber.","text":"<p>To configure sssd to user static uidNumber and gidNumber, follow the below steps 1. Add posix attributes in AD 2. Configure sssd to read posix attributes</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#21-adds-posix-attributes-in-ad","title":"2.1 Adds posix attributes in AD","text":"<p>If SSSD wants to use static uidNumber and gidNumber, the AD server must have those attributes. Before Windows server 2016. The AD server can use <code>rfc2307</code> schema, which allows us to create posix compatible user  accounts and groups. This feature has been removed since <code>Windows server 2016</code>. But you can still add attributes such as <code>uidNumber</code>, <code>gidNumber</code> to a user account or group. </p> <p>Open <code>AD users and groups gui</code>-&gt; on the toolbar, click on <code>view</code>-&gt; Select <code>advance features</code> -&gt; now when you double-click on  a user account, you will see a tab called </p> <p></p> <p></p> <p>The official doc can be found here.</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#22-configure-sssd-to-read-posix-attributes","title":"2.2 Configure sssd to read posix attributes","text":"<p>Before changing your sssd configuration, make sure <code>AD Objects have gidNumber and uidNumber Attributes</code>.</p>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#221-for-ad-windows-server-2016","title":"2.2.1 For AD &lt; Windows server 2016.","text":"<p>Configure the AD server to use <code>rfc2307</code> schema, and create posix compatible accounts and groups. Then add the below  conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False\nldap_schema = rfc2307\n</code></pre>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#221-for-ad-windows-server-2016_1","title":"2.2.1 For AD &gt;= Windows server 2016.","text":"<p>Add the below conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False  # Important: Forces usage of uidNumber/gidNumber\nldap_user_uid_number = uidNumber\nldap_user_gid_number = gidNumber\nldap_group_gid_number = gidNumber\nenumerate = True  # Optional: Lists all users and groups\n</code></pre>"},{"location":"adminsys/os_setup/security/05.Static_uid_gid_integration_in_sssd/#23-restart-sssd-and-check-uid-gid","title":"2.3 Restart sssd and check uid, gid","text":"<pre><code># restart sssd\nsystemctl restart sssd\n# clear sssd cache\nsss_cache -E\n\n# check user id and groups\nid &lt;uid&gt;\n\n# you should see the output id value matches the value which you deined in AD\n\n# check group id\ngetent group &lt;groupname&gt;\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/","title":"Configure debian server sshd auth with pam, sssd, kerberos","text":"<p>There is a complete tutorial on how to setup openldap, kerberos for unified authentication. You can visit this website.</p> <p>In this tutorial, we show a scenario light compares to the architecture which shows in the above tutorial.  We don't use the <code>SASL/GSSAPI</code> to delegate user password check to kerberos.</p> <p>It means, in the below tutorial, user has a password in openldap, and a password in kerberos, which are not synchronized automatically. The user id does the link of user between openldap and kerberos.</p> <p>Suppose we have a ldap and kerberos server running on <code>10.50.5.200</code> with url as <code>krb.casd.local</code>.</p> <p>suppose we have three servers: - krb.casd.local: server hosts openldap and kerberos (can be replaced by AD/kerberos) - ssh-server.casd.local: server runs sshd server which uses pam, sssd, sssd-krb5 to check user authentication - ssh-client.casd.local: a vm runs an ssh client and krb5-client, user can get a kerberos ticket, and use this ticket to ssh                            into the ssh-server.casd.local</p> <p>The full authentication process: 1. user get a kerberos ticket (kinit ) (in ssh-client.casd.local) 2. user init an ssh connection with the cached kerberos ticket(ssh uid@ssh-server.casd.local) (in ssh-client.casd.local) 3. ssh-server receives the ssh connection requests (sshd config checks all the possible authentication methods) (in ssh-server.casd.local) 4. <code>sshd</code> delegate the authentication to <code>pam</code>(<code>UsePAM yes in sshd_config</code>), <code>pam</code> delegate to <code>sssd</code>, <code>sssd</code> delegate to <code>sssd-krb5</code>. Because we set <code>auth_provider</code>       as <code>krb5</code> in <code>sssd</code>. (in ssh-server.casd.local) 5. <code>sssd-krb5</code> sends a request to <code>krb.casd.local</code> to verify the authenticity of the kerberos ticket. This steps requires <code>ssh-server.casd.local</code>     has a valid principal in <code>krb.casd.local</code> (in ssh-server.casd.local) 6. <code>krb.casd.local</code> verify the ticket and send the result back to <code>ssh-server.casd.local</code>. (in krb.casd.local) 7. <code>sssd-krb5</code> in <code>ssh-server.casd.local</code> receives the result, if ok, it will ask the <code>id_provider</code> of the <code>sssd</code>,        in our case it's <code>openldap</code> to get <code>uid</code> and <code>gid</code> of the user with the uid of the kerberos ticket. The user       <code>uid</code> and <code>gid</code> information will be transfer to <code>nss</code>. <code>sssd</code> tells pam it's ok, <code>pam</code> tells sshd it's ok.      Then pam will create a user session in the server after user login. (in ssh-server.casd.local) 8. user will get a terminal on <code>ssh-server.casd.local</code> with uid and gids from the <code>openldap</code> account."},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#1-configure-and-test-krb-client-on-ssh-clientcasdlocal-and-ssh-servercasdlocal","title":"1. Configure and test krb client on ssh-client.casd.local and ssh-server.casd.local","text":"<p>We need to install the kerberos client on both servers: - ssh-client.casd.local - ssh-server.casd.local</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#11-install-the-required-packages","title":"1.1 Install the required packages","text":"<pre><code># krb client package\nsudo apt install krb5-user\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#12-configure-the-krb-client","title":"1.2 Configure the krb client","text":"<pre><code>sudo vim /etc/krb5.conf\n\n# add the below content\n[libdefaults]\n    default_realm = CASD.LOCAL\n\n# The following krb5.conf variables are only for MIT Kerberos.\n    kdc_timesync = 1\n    ccache_type = 4\n    forwardable = true\n    proxiable = true\n        rdns = false\n\n\n# The following libdefaults parameters are only for Heimdal Kerberos.\n    fcc-mit-ticketflags = true\n\n[realms]\n    CASD.LOCAL = {\n        kdc = krb.casd.local\n        admin_server = krb.casd.local\n    }\n\n\n[domain_realm]\n        casd.local = CASD.LOCAL\n        .casd.local = CASD.LOCAL\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#13-test-the-client","title":"1.3 Test the client","text":"<pre><code># generate a ticket, the principal must exist in the krb server\nkinit pliu@CASD.LOCAL\n\n# normally, you can view the ticket\nklist\n\n# clean the ticket\nkdestory\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#2-config-sshd-pam-sssd-on-ssh-servercasdlocal","title":"2. Config sshd, pam, sssd on ssh-server.casd.local","text":""},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#21-install-required-packages","title":"2.1 Install required packages","text":"<pre><code>sudo apt install sssd sssd-tools libnss-sss libpam-sss libpam-mkhomedir\n</code></pre> <ul> <li>sssd: package for daemon sssd((System Security Services Daemon))</li> <li>sssd-tools: provides command-line utilities for managing and troubleshooting SSSD</li> <li>nss: NSS (Name Service Switch) is a subsystem in Linux and Unix-like systems that allows applications           to retrieve information about users, groups, hosts, networks, services, and more from various           sources (like /etc/passwd, LDAP, NIS, or SSSD). By default, this daemon is running on a debian server,           no need to install the package. The main config is in <code>/etc/nsswitch.conf</code></li> <li>libnss-sss: This daemon allows <code>nss</code> to retrieve user information from <code>sssd</code></li> <li>libpam-sss: This daemon allows <code>pam</code> to use <code>sssd</code> as an authentication mechanism. </li> <li>libpam-mkhomedir: This daemon allows <code>pam</code> to create home dir for newly connected users.</li> </ul>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#22-configure-sshd-to-use-pam-as-an-authentication-mechanism","title":"2.2 Configure sshd to use pam as an authentication mechanism","text":"<pre><code>Include /etc/ssh/sshd_config.d/*.conf\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n#HostKey /etc/ssh/ssh_host_rsa_key\n#HostKey /etc/ssh/ssh_host_ecdsa_key\n#HostKey /etc/ssh/ssh_host_ed25519_key\n\n# Ciphers and keying\n#RekeyLimit default none\n\n# Logging\n#SyslogFacility AUTH\n#LogLevel INFO\n\n# Authentication:\n\n#LoginGraceTime 2m\n#PermitRootLogin prohibit-password\n#StrictModes yes\n#MaxAuthTries 6\n#MaxSessions 10\n\n#PubkeyAuthentication yes\n\n# Expect .ssh/authorized_keys2 to be disregarded by default in future.\n#AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2\n\n#AuthorizedPrincipalsFile none\n\n#AuthorizedKeysCommand none\n#AuthorizedKeysCommandUser nobody\n\n# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts\n#HostbasedAuthentication no\n# Change to yes if you don't trust ~/.ssh/known_hosts for\n# HostbasedAuthentication\n#IgnoreUserKnownHosts no\n# Don't read the user's ~/.rhosts and ~/.shosts files\n#IgnoreRhosts yes\n\n# To disable tunneled clear text passwords, change to no here!\n#PasswordAuthentication yes\n#PermitEmptyPasswords no\n\n# Change to yes to enable challenge-response passwords (beware issues with\n# some PAM modules and threads)\nChallengeResponseAuthentication no\n#AuthenticationMethods gssapi-with-mic,password\n\n# Kerberos options\n# KerberosAuthentication yes\n# KerberosOrLocalPasswd yes\n# KerberosTicketCleanup yes\n# KerberosGetAFSToken yes\n#UseDNS yes\n# GSSAPI options\nGSSAPIAuthentication yes\nGSSAPICleanupCredentials yes\nGSSAPIStrictAcceptorCheck no\n#GSSAPIKeyExchange no\nAllowTcpForwarding yes\nAllowAgentForwarding yes\nGssapiKeyExchange yes\n# Set this to 'yes' to enable PAM authentication, account processing,\n# and session processing. If this is enabled, PAM authentication will\n# be allowed through the ChallengeResponseAuthentication and\n# PasswordAuthentication.  Depending on your PAM configuration,\n# PAM authentication via ChallengeResponseAuthentication may bypass\n# the setting of \"PermitRootLogin without-password\".\n# If you just want the PAM account and session checks to run without\n# PAM authentication, then enable this but set PasswordAuthentication\n# and ChallengeResponseAuthentication to 'no'.\nUsePAM yes\nUseDNS yes\n#AllowAgentForwarding yes\n#AllowTcpForwarding yes\n#GatewayPorts no\nX11Forwarding yes\n#X11DisplayOffset 10\n#X11UseLocalhost yes\n#PermitTTY yes\nPrintMotd no\n#PrintLastLog yes\n#TCPKeepAlive yes\n#PermitUserEnvironment no\n#Compression delayed\n#ClientAliveInterval 0\n#ClientAliveCountMax 3\n#UseDNS no\n#PidFile /var/run/sshd.pid\n#MaxStartups 10:30:100\n#PermitTunnel no\n#ChrootDirectory none\n#VersionAddendum none\nPermitRootLogin yes\n#PasswordAuthentication yes\n\n# no default banner path\n#Banner none\n\n# Allow client to pass locale environment variables\nAcceptEnv LANG LC_*\n\n# override default of no subsystems\nSubsystem   sftp    /usr/lib/openssh/sftp-server\n\n# Example of overriding settings on a per-user basis\n#Match User anoncvs\n#   X11Forwarding no\n#   AllowTcpForwarding no\n#   PermitTTY no\n#   ForceCommand cvs server\n</code></pre> <p>In the above conf, you can notice that I have two authentication methods: <code>GSSAPI</code> and <code>PAM</code>. Here <code>GSSAPI</code> is used to allow user to submit his ticket kerberos to sshd server. The sssd-krb5 can only support user login and password auth via kerberos. </p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#23-configure-pam-to-use-sssd","title":"2.3 configure pam to use sssd","text":"<p><code>pam</code> has a list of configuration files(located in <code>/etc/pam.d/</code>): - common-auth: user authentication  - common-account: User account management - common-password: Allow user to modify password. - common-session: user session settings</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#231-common-auth","title":"2.3.1 common-auth","text":"<p>The simplest config example :</p> <pre><code>auth      sufficient  pam_unix.so\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <p>pam_unix.so: Uses local account to authenticate users pam_sss.so use_first_pass: Uses SSSD as first method to authenticate users. pam_deny.so: Denies access if all the above authentication method fails. pam_permit.so: Allows authentication if all previous steps succeed.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#232-common-account","title":"2.3.2 common-account","text":"<p>This controls how the user account can interact with the system.  Below is a simple config example. </p> <pre><code>account   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <p>don't add <code>account requisite  pam_deny.so</code> in the config, otherwise you can no longer become root with sudoers right.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#233-common-password","title":"2.3.3 common-password:","text":"<p>Allow user to modify password.</p> <pre><code>password  sufficient  pam_unix.so nullok md5 shadow use_authtok\npassword  sufficient  pam_sss.so try_first_pass\npassword  required    pam_deny.so\n</code></pre> <p>This configuration is not enough for user to change password. You need to change sssd, ldap/kerberos config to  allow users to change their passwords through sssd, Kerberos/LDAP.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#234-common-session","title":"2.3.4 common-session","text":"<pre><code>session   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre> <p>pam_mkhomedir.so: Create a home directory on first login if it doesn\u2019t exist with umask=0022. pam_sss.so: Ensures SSSD session modules are applied.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#24-configure-nss","title":"2.4 Configure NSS","text":"<p>Ensure SSSD is used for user and group lookup.</p> <p>The NSS (Name Service Switch) main config is located at <code>/etc/nsswitch.conf</code>:</p> <p>The following config is a simple example tells Linux to check both local files (/etc/passwd) and SSSD for user information.</p> <pre><code>sudo vim /etc/nsswitch.conf\n\npasswd:         files sss\ngroup:          files sss\nshadow:         files sss\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#25-configure-sssd","title":"2.5 Configure sssd","text":"<p>The <code>sssd</code> can query ldap/kerberos, AD/kerberos to check user authenticity(auth_provider), query ldap, or AD to get user id, groups, etc(id_provider).</p> <p><code>sssd</code> also allows user to change password of the backend(e.g. ldap, krb)</p> <pre><code>[sssd]\nservices = nss, pam, ssh\ndomains = casd.local\nconfig_file_version = 2\n\n[domain/casd.local]\nid_provider = ldap\nldap_uri = ldap://krb.casd.local\nldap_search_base = dc=casd,dc=local\n\nauth_provider = krb5\nchpass_provider = krb5\nkrb5_realm = CASD.LOCAL\nkrb5_server = krb.casd.local\nkrb5_kpasswd = krb.casd.local\ndebug_level = 5\nkrb5_validate = true\nkrb5_ccachedir = /var/tmp # note that RHEL-7 default to KERNEL ccaches, which are preferred in most cases to FILE\nkrb5_keytab = /etc/krb5.keytab\ncache_credentials = true\n\noverride_homedir = /home/%u\ndefault_shell = /bin/bash\n\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n</code></pre> <p>You need to restart the daemon <code>sssd</code>, after modifying the <code>sssd.conf</code></p> <pre><code>sudo systemctl restart sssd\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#251-debug-sssd-by-using-sssd-tools","title":"2.5.1 debug sssd by using sssd-tools","text":"<p>You need the admin right to run this command, otherwise you will get <code>command not found</code> error message. You can view the documentation of the tool with <code>sudo sssctl</code>.</p>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#check-the-validity-of-the-sssd-config","title":"Check the validity of the sssd config","text":"<pre><code>sudo sssctl config-check\n\n# output\nIssues identified by validators: 0\n\nMessages generated during configuration merging: 0\n\nUsed configuration snippet files: 0\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#list-all-available-domain-configured-in-sssd","title":"list all available domain configured in sssd","text":"<pre><code>sudo sssctl domain-list\n\n# output example\ncasd.local\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#check-the-status-of-a-domain","title":"check the status of a domain","text":"<pre><code>sudo sssctl domain-status casd.local\n\n# output example\nOnline status: Online\nActive servers:\nKPASSWD: krb.casd.local\nKERBEROS: krb.casd.local\nLDAP: krb.casd.local\nDiscovered KPASSWD servers:\n- krb.casd.local\nDiscovered KERBEROS servers:\n- krb.casd.local\nDiscovered LDAP servers:\n- krb.casd.local\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#check-the-status-of-a-user","title":"check the status of a user","text":"<pre><code>sudo sssctl user-checks pengfei\n\n# output example\nuser: pengfei\naction: acct\nservice: system-auth\n\nSSSD nss user lookup result:\n - user name: pengfei\n - user id: 3002\n - group id: 4000\n - gecos: pengfei\n - home directory: /home/pengfei\n - shell: /bin/bash\n\nSSSD InfoPipe user lookup result:\n - name: pengfei\n - uidNumber: 3002\n - gidNumber: 4000\n - gecos: pengfei\n - homeDirectory: /home/pengfei\n - loginShell: /bin/bash\n\ntesting pam_acct_mgmt\n\npam_acct_mgmt: Success\n\nPAM Environment:\n - no env -\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#252-clear-sssd-cache","title":"2.5.2 Clear sssd cache","text":"<p>Use the below command to clear SSSD cache, if SSSD is using outdated credentials.</p> <pre><code>sss_cache -E   # Clear all cached entries\nsss_cache -u username  # Clear cache for a specific user\nsss_cache -g groupname  # Clear cache for a specific group\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#26-create-service-principals-for-kerberos-authentication","title":"2.6 Create service principals for kerberos authentication","text":"<p>sssd-krb5 requires a service principal to be able to talk with the kerberos server. So we need to create a service account(principal) for <code>ssh-server.casd.local</code> to be able to access <code>krb.casd.local</code></p> <p>In <code>ssh-server.casd.local</code>, run the below command</p> <pre><code># connect to krb server via kadmin. The principal which you use to connect to the admin console \n# must has the admin rights in krb server\nsudo kadmin -p admin/admin@CASD.LOCAL\n\n# you should see the below terminal\nkadmin:\n\n# create a principal with a generated password for the ssh-server\nkadmin:  addprinc -randkey auth-agent/sssd-test.casd.local@CASD.LOCAL\n\n# export the principal with encrypted password to the default keytab\nkadmin:  ktadd auth-agent/sssd-test.casd.local@CASD.LOCAL\n\n# exit the kadmin shell\nquit\n\n# check the principal in the keytab\nsudo klist -k /etc/krb5.keytab\n\n# short version\nsudo klist -ke\n</code></pre> <p>if your principal does not have admin rights, you can edit the <code>/etc/krb5kdc/kadm5.acl</code> file to grant admin rights to certain principals</p> <p>The below file is an example. One common way to set up Kerberos administration is to <code>allow any principal   ending in /admin is given full administrative rights.</code></p> <pre><code># To enable this, uncomment the following line:\n*/admin *\n</code></pre>"},{"location":"adminsys/os_setup/security/06.Configure_ssh_sssd_kerberos_linux/#27-test-ssh-connexions","title":"2.7 Test ssh connexions","text":"<pre><code># on the ssh-server, you can already check if the pam, sssd, openldap config\ngetent passwd username\n\n# on the ssh-client, \nssh uid@ssh-server.casd.local\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/","title":"Guide pour int\u00e9grer une machine Debian 11 \u00e0 un domaine Active Directory et configurer SSH avec GSSAPI/Kerberos","text":"<p>In this tutorial, we show how to join a <code>debian 11</code> server </p>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-1-preparation-de-la-machine-debian","title":"\u00c9tape 1 : Pr\u00e9paration de la machine Debian","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#11-renommer-la-machine","title":"1.1 Renommer la machine","text":"<p>Modifier le nom d'h\u00f4te selon la politique de l'entreprise :</p> <pre><code>sudo nano /etc/hostname  # Exemple : debian.casdds.casd\nsudo hostnamectl set-hostname \"nouveau_nom\"\n</code></pre> <p>Metter \u00e0 jour <code>/etc/hosts</code> pour inclure le nom et l'IP statique :</p> <pre><code>sudo nano /etc/hosts  # Ajouter : 10.50.5.X   debian.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#12-mise-a-jour-du-systeme","title":"1.2 Mise \u00e0 jour du syst\u00e8me","text":"<pre><code>sudo apt update \n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#13-configurer-le-dns-sur-debian","title":"1.3 Configurer le DNS sur Debian","text":"<p>D\u00e9finir le serveur DNS AD dans <code>/etc/resolv.conf</code> :</p> <pre><code>search casdds.casd\nnameserver 10.50.5.64\nnameserver 8.8.8.8\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#14-installer-les-paquets-necessaires","title":"1.4 Installer les paquets n\u00e9cessaires","text":"<pre><code>sudo apt install realmd sssd sssd-tools libnss-sss libpam-sss adcli samba-common-bin krb5-user oddjob oddjob-mkhomedir packagekit -y\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-2-joindre-le-domaine-active-directory","title":"\u00c9tape 2 : Joindre le domaine Active Directory","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#21-decouvrir-le-domaine","title":"2.1. D\u00e9couvrir le domaine","text":"<p>V\u00e9rifier la connectivit\u00e9 avec le contr\u00f4leur de domaine :</p> <pre><code>realm discover CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#22-rejoindre-le-domaine","title":"2.2. Rejoindre le domaine","text":"<p>Utiliser un compte administrateur AD :</p> <pre><code>sudo realm join --user=Administrateur CASDDS.CASD\n</code></pre> <p>A ce stade, mon client Debian a bien rejoint mon domaine et appara\u00eet dans la console Utilisateurs et Ordinateurs Active Directory de mon serveur Windows. S\u2019il n\u2019apparait pas, on peut l\u2019ajouter manuellement dans Ordinateurs en s\u00e9lectionne l\u2019@ip static et d\u00e9l\u00e9gation kerberos </p> <p>{{:datascience:admin_system:linux:capture1.png?400|}} {{:datascience:admin_system:linux:capture2.png?400|}}</p>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-3-configuration-dns-et-kerberos","title":"\u00c9tape 3 : Configuration DNS et Kerberos","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#31-ajouter-lenregistrement-dns-sur-windows","title":"3.1. Ajouter l'enregistrement DNS sur Windows","text":"<p>Dans le serveur DNS Windows :</p> <p>Ajouter un enregistrement A pour la machine Debian dans la zone Forward Lookup*. (S'il n'est pas pr\u00e9sent) </p> <ul> <li>Cr\u00e9er un enregistrement PTR dans la zone Reverse Lookup.(S'il n'est pas pr\u00e9sent)  {{:datascience:admin_system:linux:capture1.png?400|}} {{:datascience:admin_system:linux:capture4.png?400|}}</li> </ul>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#32-enregistrer-le-spn-service-principal-name","title":"3.2. Enregistrer le SPN (Service Principal Name)","text":"<p>Pour assurer que l'enregistrement existant, on tape: </p> <p>Sur le contr\u00f4leur de domaine (PowerShell administrateur) :   </p> <pre><code>setspn -L debian\n</code></pre> <p>Si host/debian.casdds.casd n'est pas pr\u00e9sent, on l'ajouter avec Powershell admin :</p> <pre><code>setspn -S host/debian.casdds.casd debian\nktpass -princ host/debian.casdds.casd@CASDDS.CASD -mapuser DEBIAN$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#33-verifier-le-keytab-kerberos","title":"3.3. V\u00e9rifier le keytab Kerberos","text":"<p>Sur Debian :</p> <pre><code>klist -k /etc/krb5.keytab  # V\u00e9rifier la pr\u00e9sence de \"host/debian.casdds.casd\"\n</code></pre> <p>Si absent, quitter et rejoigner le domaine :</p> <pre><code>sudo realm leave CASDDS.CASD\nsudo realm join --user=Administrateur CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#34-generation-et-deploiement-dun-fichier-keytab","title":"3.4. G\u00e9n\u00e9ration et d\u00e9ploiement d\u2019un fichier keytab","text":"<p>G\u00e9n\u00e9ration du keytab sur Windows :</p> <pre><code>ktpass -princ user@CASDDS.CASD -mapuser user -crypto AES256-SHA1 -ptype KRB5_NT_PRINCIPAL -pass * -out user.keytab\n</code></pre> <p>Transfert vers Debian :</p> <pre><code>scp user.keytab user@debian.casdds.casd:/tmp/\n</code></pre> <p>Installation et s\u00e9curisation du keytab :</p> <pre><code>sudo cp /tmp/user.keytab /etc/\nsudo chmod 644 /etc/user.keytab\nsudo chown root:root /etc/user.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-4-configuration-de-sssd-pam-et-kerberos","title":"\u00c9tape 4 : Configuration de SSSD, PAM et Kerberos","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#41-fichier-etcsssdsssdconf","title":"4.1. Fichier <code>/etc/sssd/sssd.conf</code>","text":"<pre><code>[sssd]\nservices = nss, pam\ndomains = casdds.casd\nconfig_file_version = 2\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n\n[domain/casdds.casd]\nldap_sasl_authid = user@CASDDS.CASD\nkrb5_keytab = /etc/user.keytab\ndefault_shell = /bin/bash\nkrb5_store_password_if_offline = True\ncache_credentials = True\nkrb5_realm = CASDDS.CASD\nrealmd_tags = manages-system joined-with-adcli\nid_provider = ad\nfallback_homedir = /home/%u@%d\nad_domain = casdds.casd\nuse_fully_qualified_names = False\nldap_id_mapping = True\naccess_provider = ad\nldap_group_nesting_level = 2\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#42-configurer-pam","title":"4.2. Configurer PAM","text":"<p>Modifier les fichiers dans <code>/etc/pam.d/</code> pour inclure <code>pam_sss.so</code> </p> <pre><code>### /etc/pam.d/common-auth\nsudo: unable to resolve host debian118: Name or service not known\nauth      sufficient  pam_unix.so try_first_pass\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-account\nsudo: unable to resolve host debian118: Name or service not known\naccount   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <pre><code>### /etc/pam.d/common-password\nsudo: unable to resolve host debian118: Name or service not known\npassword  sufficient  pam_unix.so\npassword  sufficient  pam_sss.so\npassword  required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-session\nsudo: unable to resolve host debian118: Name or service not known\nsession   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#43-fichier-etckrb5conf","title":"4.3. Fichier <code>/etc/krb5.conf</code>","text":"<pre><code> [libdefaults]\n        default_realm = CASDDS.CASD\n\n        default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        permitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        proxiable = true\n        ticket_lifetime = 24h\n        dns_lookup_realm = true\n        dns_lookup_kdc = true\n        dns_canonicalize_hostname = false\n        rdns = false\n         allow_weak_crypto = true\n# The following libdefaults parameters are only for Heimdal Kerberos.\n        fcc-mit-ticketflags = true\n\n[realms]\n        CASDDS.CASD = {\n                kdc = 10.50.5.64\n                admin_server = 10.50.5.64\n        }\n\u2026..\n[domain_realm]\n\u2026.\n        .casdds.casd = CASDDS.CASD\n        casdds.casd = CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-5-configuration-ssh-avec-gssapi","title":"\u00c9tape 5 : Configuration SSH avec GSSAPI","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#51-sur-debian","title":"5.1. Sur Debian","text":"<p>Modifier <code>/etc/ssh/sshd_config</code> :</p> <pre><code>UsePam yes\nGSSAPIAuthentication yes # l'authentification bas\u00e9e sur GSSAPI pour les connexions SSH\nGSSAPICleanupCredentials yes # la suppression automatique des identifiants temporaires obtenus via GSSAPI apr\u00e8s leur utilisation pour renforcer la s\u00e9curit\u00e9\nGSSAPIKeyExchange yes # s\u00e9curiser l'\u00e9change de cl\u00e9s, prot\u00e9geant ainsi le processus de n\u00e9gociation contre les interceptions\nGSSAPIStrictAcceptorCheck no # D\u00e9sactive la v\u00e9rification stricte de l'identit\u00e9 de l'acceptateur, facilitant les connexions dans des environnements o\u00f9 les noms de principal peuvent varier\n</code></pre> <p>Modifier <code>/etc/ssh/ssh_config</code>:</p> <pre><code>   Host *\n       \u2026\n       GSSAPIAuthentication yes\n       GSSAPIDelegateCredentials yes # d\u00e9l\u00e9guer les identifiants GSSAPI du client au serveur pour \n</code></pre> <p>Red\u00e9marrer les services :</p> <pre><code>sudo systemctl restart sshd sssd\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#52-sur-windows","title":"5.2. Sur Windows","text":"<p>Installer OpenSSH via PowerShell:</p> <pre><code>Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\nStart-Service sshd\nSet-Service -Name sshd -StartupType 'Automatic'\n</code></pre> <p>Activer GSSAPI dans <code>C:\\ProgramData\\ssh\\sshd_config</code> :</p> <pre><code>GSSAPIAuthentication yes\nGSSAPICleanupCredentials yes\n</code></pre> <p>Red\u00e9marrer le service:</p> <pre><code> Restart-Service sshd\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#etape-6-validation","title":"\u00c9tape 6 : Validation","text":""},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#61-test-kerberos","title":"6.1. Test Kerberos","text":"<p>Sur Debian :</p> <pre><code> kinit user@CASDDS.CASD  # Authentifier avec le mot de passe AD\nklist    \n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#62-connexion-ssh","title":"6.2. Connexion SSH","text":"<p>Depuis Windows :</p> <pre><code>ssh -K user@debian.casdds.casd  # -K active la d\u00e9l\u00e9gation Kerberos\n</code></pre>"},{"location":"adminsys/os_setup/security/fr/02.Configure_ssh_pam_sssd_ad_fr/#appendix-notes-importantes","title":"Appendix Notes importantes :","text":"<ul> <li>Permissions SSSD : V\u00e9rifier que <code>/etc/sssd/sssd.conf</code> a les droits <code>600</code> :</li> </ul> <pre><code>sudo chmod 600 /etc/sssd/sssd.conf\n</code></pre>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/","title":"Install dukowiki on debian 11","text":""},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#prerequise","title":"Prerequise","text":"<p>Dukowiki needs a web server and a PHP env to run. So we need to install Nginx and PHP first.</p> <p>Dukowiki does not support PHP 8+ yet, so we need to install PHP7.x</p> <p>For debian 11, the default php version is 7.4, which is perfect for dukowiki. So the easiest way is to use the apt-get to install these packages</p> <pre><code>apt-get install nginx php-fpm php-curl php-gd php-opcache php-json php-mbstring php-intl php-imagick php-xml -y\n</code></pre> <p>The <code>php-fpm(FastCGI Process Manager)</code> is very important, which is a <code>SAPI</code> interface. It allows the communication between the <code>web server(nginx)</code> and <code>PHP run time</code> by using the <code>FastCGI</code> protocol.</p>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#remove-apache2","title":"Remove apache2","text":"<p>The default web server in debian 11 is apache2, so we need to delete it.</p> <pre><code>systemctl stop apache2\nsystemctl disable apache2\napt-get remove apache2 --purge\n</code></pre>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#check-the-installed-nginx-and-php-fpm","title":"Check the installed nginx and php-fpm","text":"<pre><code>systemctl start nginx\nsystemctl enable nginx\n\nsystemctl start php7.4-fpm\nsystemctl enable php7.4-fpm\n</code></pre>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#download-the-source-of-dokuwiki","title":"Download the source of Dokuwiki","text":"<pre><code># go to the default folder of the web static content\ncd /var/www/html\n\n# download the source\nwget https://download.dokuwiki.org/src/dokuwiki/dokuwiki-stable.tgz\n\n# extract the source\ntar -xvzf dokuwiki-stable.tgz\n\n# rename the file\nmv dokuwiki-* dokuwiki\n\n# change owner and acl of the dokuwiki\nchown -R www-data:www-data /var/www/html/dokuwiki\nchmod -R 755 /var/www/html/dokuwiki\n</code></pre>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#configure-nginx","title":"configure Nginx","text":"<p>Create an Nginx <code>virtual host</code> configuration file for DokuWiki</p> <pre><code>vim /etc/nginx/site-available/dokuwiki.conf\n</code></pre> <pre><code>server {\n    listen 80;\n    server_name wiki.casd.local;\n\n    # redirect http request to https\n    return 301 https://$host$request_uri;\n\n}\n\nserver {\n    listen 443 ssl;\n    root /var/www/html/dokuwiki;\n    index index.php index.html;\n\n    ssl_certificate /etc/ssl/certs/casd_k8s_wildcard.pem;\n    ssl_certificate_key /etc/ssl/private/casd_k8s_wildcard_key.pem;\n\n    location / {\n        try_files $uri $uri/ @dokuwiki;\n    }\n\n    location @dokuwiki {\n        rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last;\n        rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last;\n        rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&amp;id=$2 last;\n        rewrite ^/(.*) /doku.php?id=$1&amp;$args last;\n    }\n\n    location ~ \\.php$ {\n        include snippets/fastcgi-php.conf;\n        fastcgi_pass unix:/var/run/php/php7.4-fpm.sock;\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n        include fastcgi_params;\n    }\n\n}\n</code></pre> <p>Save and close the file, then verify Nginx config file validity. If its valide then activate the Nginx virtual host. You can use the following command:</p> <pre><code># check conf syntax\nsudo nginx -t\n\n# activate the virtual host\nsudo ln -s /etc/nginx/sites-available/dokuwiki.conf /etc/nginx/sites-enabled/\n</code></pre>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#some-trouble-shoot","title":"Some trouble shoot","text":"<p>If you experience this issue:</p> <pre><code>nginx: [emerg] could not build server_names_hash\n</code></pre> <p>You should edit the <code>/etc/nginx.conf</code> file, change server_names_hash_bucket_size: 32  to 64:</p> <pre><code>server_names_hash_bucket_size 64;\n</code></pre> <p>Next, restart the Nginx service to apply the changes:</p> <pre><code>systemctl restart nginx\n</code></pre>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#init-dokuwiki","title":"Init dokuwiki","text":"<p>open your web browser and access the DokuWiki installation using the URL http://doku.example.com/install.php. You should see a fomular which allows you to set <code>wiki name, admin user login, passwd, etc.</code></p> <p>Provide your Wiki name, admin username, password, and email and click on the Save button.</p> <p>Click on <code>your new DokuWiki</code> link. You will be redirected to the DokuWiki welcome screen</p>"},{"location":"adminsys/other_services/15.Install_dukowiki_on_debian_11/#migrate-from-older-version","title":"Migrate from older version","text":"<p>The advantage of dokuwiki is that it does not need database, so the migration is quite easy. </p> <p>All the content is stored under <code>/var/www/html/dokuwiki/data</code>. To migrate just copy your old wiki's data folder and replace the data folder of your newly installed wiki.</p> <p>New version of the duko wiki also store logs in log folder. If the data folder of your old wiki does not have <code>log folder</code>, you need to create it manually. And don't forget to <code>change owner and ACL</code>. </p>"},{"location":"adminsys/other_services/build_openssl_tool/","title":"Build openssl","text":"<p>There are some feature of signing certificate only exist in v3, and v3 is not in the standard repo or backports repo. So we have to build it manually.</p>"},{"location":"adminsys/other_services/build_openssl_tool/#get-the-source","title":"Get the source","text":"<p>You can find the full source list here. I use version <code>3.0.9</code> in this tutorial.</p> <pre><code># the -P option will put the download file in the target dir\nsudo wget -P /usr/src/ https://www.openssl.org/source/openssl-3.0.9.tar.gz\n\ncd /usr/src\n\n# unzip the source\nsudo tar -xzvf openssl-3.0.9.tar.gz\n</code></pre>"},{"location":"adminsys/other_services/build_openssl_tool/#config-and-build-the-bin","title":"Config and build the bin","text":"<pre><code>cd /usr/src/openssl-3.0.9\n\n# install dependencies\nsudo apt update\nsudo apt install build-essential checkinstall zlib1g-dev libssl-dev\n\n# you can replace the prefix by a custom path\n./config --prefix=/usr/local/openssl\n\n# build the source \nsudo make\n\nsudo make test\n\nsudo make install\n\n# if everything works well, you should find the below dirs in /usr/local/openssl\n/usr/local/openssl/\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib64\n\u251c\u2500\u2500 share\n\u2514\u2500\u2500 ssl\n</code></pre>"},{"location":"adminsys/other_services/build_openssl_tool/#post-installation-config","title":"Post installation config","text":"<pre><code># try the newly build\n/usr/local/openssl/bin/openssl version\n\n# normally, you should see the below error message \nopenssl: error while loading shared libraries: libssl.so.3: cannot open shared object file: No such file or directory\n\n# that's because the required lib is not loaded in your env\n# run the below command to load \necho 'export LD_LIBRARY_PATH=/usr/local/openssl/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# add the lib path (/usr/local/openssl/lib64) in the below file\nsudo vim /etc/ld.so.conf.d/openssl.conf\n\n# reload the ldconfig\nsudo ldconfig\n# check if the lib exist or not\nsudo ldconfig -p | grep libssl.so.3\n\n# if you can find the new lib, then rerun\n/usr/local/openssl/bin/openssl version \n</code></pre> <p>If you want to replace the old version of openssl, you can run the below command</p> <pre><code># remove the old version  \nmv /usr/bin/openssl /root/openssl-old\n\necho 'PATH=\"/usr/local/openssl/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"adminsys/other_services/install_gitlab_ce/","title":"Install on configure gitlab CE on debian 11","text":"<p>GitLab Community Edition (CE) is an open-source application for hosting Git repositories in your own infrastructure.  With GitLab you can do project planning and source code management to CI/CD and monitoring. GitLab has evolved to  become a complete DevOps platform, delivered as a single application.</p>"},{"location":"adminsys/other_services/install_gitlab_ce/#1-the-minimum-requirements","title":"1. The minimum requirements","text":"<p>The Gitlab CE is an application web with a database. To allow it to run correctly. We recommend you to meet the below minimum requirements: - 8GB of Ram - 4 vcpus - 40GB Disk space</p> <p>Having a domain name allows user to easily access it. So we recommand you to provide a domain name </p> <p>In this tutorial, we set the domain name as:  <code>git.casd.local</code></p>"},{"location":"adminsys/other_services/install_gitlab_ce/#2-install-the-gitlab-ce-dependencies-packages","title":"2. Install the gitlab ce dependencies packages","text":"<pre><code># update apt repo\nsudo apt update &amp;&amp; sudo apt -y full-upgrade\n\n# Install GitLab Server Dependencies\nsudo apt -y install curl vim openssh-server ca-certificates\n</code></pre>"},{"location":"adminsys/other_services/install_gitlab_ce/#3-configure-postfix-send-only-smtp","title":"3. Configure Postfix Send-Only SMTP","text":"<p>You can find the full doc here</p>"},{"location":"adminsys/other_services/install_gitlab_ce/#4-add-the-gitlab-ce-repository","title":"4. Add the GitLab CE Repository","text":"<p>With the below script, we will add the GitLab repository (<code>gitlab_gitlab-ce.list</code>) to <code>/etc/apt/sources.list.d/</code>.</p> <pre><code>curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash\n</code></pre> <p>The content of the <code>gitlab_gitlab-ce.list</code> should looks like:</p> <pre><code># this file was generated by packages.gitlab.com for\n# the repository at https://packages.gitlab.com/gitlab/gitlab-ce\n\ndeb [signed-by=/usr/share/keyrings/gitlab_gitlab-ce-archive-keyring.gpg] https://packages.gitlab.com/gitlab/gitlab-ce/debian/ bullseye main\ndeb-src [signed-by=/usr/share/keyrings/gitlab_gitlab-ce-archive-keyring.gpg] https://packages.gitlab.com/gitlab/gitlab-ce/debian/ bullseye main\n</code></pre>"},{"location":"adminsys/other_services/install_gitlab_ce/#5-install-gitlab-ce-on-debian","title":"5. Install GitLab CE on Debian","text":"<pre><code>export GITLAB_URL=\"http://git.casd.local\"\nsudo EXTERNAL_URL=\"${GITLAB_URL}\" apt install gitlab-ce\n</code></pre> <p>If everything goes well, you should see the success output. And the gitlab-ce server is up and running at  http://git.casd.local</p>"},{"location":"adminsys/other_services/install_gitlab_ce/#6test-the-gitlab-server","title":"6.Test the gitlab server","text":"<p>By default, it generates a root account with a password. You can get the password with the below command</p> <pre><code>sudo cat /etc/gitlab/initial_root_password \n</code></pre> <p>If everything goes well, you should be able to login with root/pwd</p>"},{"location":"adminsys/other_services/install_gitlab_ce/#7-custom-config","title":"7. Custom config","text":"<p>The main config file is located at <code>/etc/gitlab/gitlab.rb</code>. If you followed the above procedure, it will install a  gitlab with minimun config. - No external authentication - built-in postgres db - Etc.</p> <p>We need to modify the config to make the gitlab server production ready.</p> <pre><code># open the conf file\nsudo vim /etc/gitlab/gitlab.rb\n\n# do some change \n\n# apply the change\nsudo gitlab-ctl reconfigure\n</code></pre>"},{"location":"adminsys/other_services/install_gitlab_ce/#71-use-an-openldap-server-for-authentication","title":"7.1 Use an openldap server for authentication","text":"<p>You can find the official doc here https://docs.gitlab.com/ee/administration/auth/ldap/#updating-ldap-dn-and-email To enable the ldap authentication, you need to </p> <pre><code># enable the ldap authentication\ngitlab_rails['ldap_enabled'] = true\n\n# config the ldap server connexion\ngitlab_rails['ldap_servers'] = YAML.load &lt;&lt;-'EOS'\n     host: 'ldap.casd.local'\n     port: 389\n     uid: 'uid'\n     bind_dn: 'cn=gitlab,ou=serviceAccounts,dc=casd,dc=local'\n     password: 'gitlabServiceAccountPassword'\n     encryption: 'plain'\n     base: 'ou=people,dc=casd,dc=local'\n     verify_certificates: false\n     active_directory: false\n     lowercase_usernames: false\n     block_auto_created_users: false\n     attributes:\n        username: ['uid']\n        email: ['mail']\n        name: 'displayName'\n        first_name: 'givenName'\n        last_name: 'sn'\nEOS\n</code></pre>"},{"location":"adminsys/other_services/install_gitlab_ce/#72-use-external-postgresql-db","title":"7.2 Use external postgresql db","text":"<p>https://docs.gitlab.com/ee/administration/postgresql/external.html</p> <p>https://stackoverflow.com/questions/23580268/gitlab-omnibus-configuration-for-postgres</p>"},{"location":"adminsys/other_services/install_vscode_server/","title":"Deploy VS code as a web service","text":"<p>There are two options, which allows you to run VS Code in the browser, giving you access to your development  environment remotely.: - VS Code Server - code-server</p>"},{"location":"adminsys/other_services/install_vscode_server/#1-vs-code-server-remote-development-extension-pack","title":"1. VS Code Server (Remote Development Extension Pack)","text":"<p>In this mode, we use VS code desktop (on client machine) to install a <code>Remote Development Extension Pack</code> which allows  the client vs code to open a ssh tunnel with the vs code server(on remote server).</p> <p>You can find the official doc here</p> <p>We don't recommend this option, because ssh uses specific ports which requires extra firewall configuration.</p>"},{"location":"adminsys/other_services/install_vscode_server/#2-code-server","title":"2. Code server","text":"<p>In this mode, the code-server (vs code server backend) runs an application server(default port is 8080). For example user can access the vs code via any browser with http://url:8080/. If you add a proxy, you can run it with 80 or 443.</p>"},{"location":"adminsys/other_services/install_vscode_server/#21-installation","title":"2.1 Installation","text":"<p>code-server is an open-source project by Coder that allows you to run a standalone VS Code instance in a web  browser. You can install it on any machine, even a remote server, and access it from anywhere.</p> <p>The official github page of the <code>code-server</code> is here: https://github.com/coder/code-server</p> <pre><code># use the installation script\ncurl -fsSL https://code-server.dev/install.sh | sh\n\n# start the server interactivately\ncode-server\n</code></pre> <p>We recommand you to run code server via systemd</p> <pre><code># start the code-server daemon\nsudo systemctl start code-server@$USER\n\n# enable it from boot\nsudo systemctl enable --now code-server@$USER\n</code></pre> <p>The above command will generate a config  ~/.config/code-server/config.yaml. Below is an example of the config file</p> <pre><code># if you want to allow remote access, change the 127.0.0.1 to 0.0.0.0.\n# you can also change the \nbind-addr: 127.0.0.1:8080\nauth: password\npassword: changeMe\ncert: false\n# cert-key: \n</code></pre> <p>The default auth method is <code>password</code>, it also supports <code>OAuth2</code>. To disable authentication, you can put <code>none</code>.</p> <p>For the ssl certificate, we don't recommend to activate it, because nginx can do a better job.</p>"},{"location":"adminsys/other_services/install_vscode_server/#22-enable-nginx-as-reverse-proxy","title":"2.2 Enable nginx as reverse-proxy","text":"<p>Suppose we start the code-server at 127.0.0.1:8080, Below is a nginx config example</p> <pre><code># add a conf\nvim /etc/nginx/sites-available/vscode-server\n\n## Add below content\n# set a backend upstream\nupstream vscode-server {\n    server 127.0.0.1:8080 fail_timeout=0;\n}\n\n# a server listen to port 80, which will redirect request to port 443\nserver {\n    listen 80;\n    server_name vscode-server.casd.local;\n\n    # redirect http request to https\n    return 301 https://$host$request_uri;\n\n    }\n\n# a server listen to port 443, which will ensure https resolution\nserver {\n    listen 443 ssl;\n    ssl_certificate /etc/ssl/certs/wildcard-casd.pem;\n    ssl_certificate_key /etc/ssl/private/wildcard-casd.key;\n\n    # all request will be redirected to the backend_chart upstream\n    location / {\n        include proxy_params;\n        proxy_pass http://vscode-server;\n        proxy_set_header Host $host;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection upgrade;\n        proxy_set_header Accept-Encoding gzip;\n\n    }\n}\n\n# Activate the conf\nln -s /etc/nginx/sites-available/vscode-server /etc/nginx/sites-enabled/vscode-server\n</code></pre>"},{"location":"adminsys/other_services/install_vscode_server/#23-edit-a-systemd-service-file","title":"2.3 Edit a systemd service file","text":"<p>To run code-server as a daemon, it's recommended to edit a systemd service file.</p> <pre><code>sudo vim /etc/systemd/system/code-server.service\n\n# add the below content\n[Unit]\nDescription=code-server\nAfter=network.target\n\n[Service]\nType=simple\nUser=pliu\nExecStart=/usr/bin/code-server --bind-addr 0.0.0.0:8080 --auth password\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code># reload systemd daemon\nsudo systemctl daemon-reload\n# enable code-server as start up service\nsudo systemctl enable code-server\n\n# start the service\nsudo systemctl start code-server\n\n# check the status\nsudo systemctl status code-server\n\n# check the log \njournalctl -u code-server -f\n\n# stop the service\nsudo systemctl stop code-server\n</code></pre>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/","title":"Install Nginx","text":""},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step-1-install-the-binary","title":"Step 1. Install the binary","text":"<p>Nginx is available in Debian\u2019s default software repositories, making it possible to install it from conventional package management tools.</p> <pre><code>sudo apt update\nsudo apt install nginx\n</code></pre>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step-2-adjusting-the-firewall","title":"Step 2. Adjusting the firewall","text":"<p>Before testing Nginx, it\u2019s necessary to modify the firewall settings to allow outside access to the default web ports. Assuming that you followed the instructions in the prerequisites, you should have a UFW firewall configured to restrict access to your server.</p> <p>During installation, Nginx registers itself with UFW to provide a few application profiles that can be used to enable or disable access to Nginx through the firewall.</p> <p>List the ufw application profiles by typing:</p> <pre><code>sudo ufw app list\n\n# Output\nAvailable applications:\n...\n  Nginx Full\n  Nginx HTTP\n  Nginx HTTPs\n  OpenSSH\n</code></pre> <p>From the output, there are three profiles available for Nginx:</p> <ul> <li>Nginx Full: This profile opens both port 80 (normal, unencrypted web traffic) and port 443 (TLS/SSL encrypted traffic)</li> <li>Nginx HTTP: This profile opens only port 80 (normal, unencrypted web traffic)</li> <li>Nginx HTTPS: This profile opens only port 443 (TLS/SSL encrypted traffic)</li> </ul> <p>Base on your requirements, allow the traffic. In our case, it will be</p> <pre><code>sudo ufw allow 'Nginx Full'\n</code></pre>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step-3-check-nginx-status","title":"Step 3. Check nginx status","text":"<pre><code>systemctl status nginx\n</code></pre> <p>You can access the default Nginx landing page to confirm that the software is running properly by navigating to your server\u2019s IP address. If you do not know your server\u2019s IP address, you can type this at your server\u2019s command prompt:</p> <pre><code># get server ip\nip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\\/.*$//'\n\n# check the default nginx welcome page\nhttp://your_server_ip\n</code></pre>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step4-nginx-useful-commands","title":"Step4. Nginx useful commands","text":"<pre><code># \nsudo systemctl stop/start/restart/reload nginx\n\n# start automatically when the server boots\nsudo systemctl disable/enable nginx\n</code></pre>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step-5-setting-up-server-blocks-incomplete","title":"Step 5. Setting Up Server Blocks (Incomplete)","text":"<p>When using the Nginx web server, server blocks (similar to virtual hosts in Apache) can be used to encapsulate  configuration details and host more than one domain on a single server.</p> <p>https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-debian-11</p>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step-6-important-nginx-files-and-directories","title":"Step 6. Important Nginx Files and Directories","text":""},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#61-content","title":"6.1 Content","text":"<p>/var/www/html: The actual web content, which by default only consists of the default Nginx page you saw earlier, is served out of the /var/www/html directory. This can be changed by altering Nginx configuration files.</p>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#62-server-configuration","title":"6.2 Server Configuration","text":"<ul> <li>/etc/nginx: The Nginx configuration directory. All of the Nginx configuration files reside here.</li> <li>/etc/nginx/nginx.conf: The main Nginx configuration file. This can be modified to make changes to the Nginx global configuration.</li> <li>/etc/nginx/sites-available/: The directory where per-site server blocks can be stored. Nginx will not use the configuration files found in this directory unless they are linked to the <code>sites-enabled</code> directory. Typically, all server block configuration is done in this directory, and then enabled by linking to the other directory.</li> <li>/etc/nginx/sites-enabled/: The directory where enabled per-site server blocks are stored. Typically, these are created by linking to configuration files found in the <code>sites-available</code> directory.</li> <li>/etc/nginx/snippets: This directory contains configuration fragments that can be included elsewhere in the Nginx configuration. Potentially repeatable configuration segments are good candidates for refactoring into snippets.</li> </ul>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#63-server-logs","title":"6.3 Server Logs","text":"<ul> <li>/var/log/nginx/access.log: Every request to your web server is recorded in this log file unless Nginx is configured to do otherwise.</li> <li>/var/log/nginx/error.log: Any Nginx errors will be recorded in this log.</li> </ul>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#step-7-set-nginx-as-a-reverse-proxy","title":"Step 7. Set Nginx as a Reverse proxy","text":"<p>We suppose we already have a service which runs on <code>http://127.0.0.1:8080/</code>. We will set Nginx as the <code>Reverse proxy</code> of this service.</p>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#71-create-a-new-server-block","title":"7.1 Create a new server block","text":"<p>We suppose this service is called <code>my-app.casd.local</code>, we need to create a new server block for this domain.</p> <pre><code>sudo vim /etc/nginx/sites-availabe/my-app\n</code></pre> <p>Add below config to the <code>my-app</code> file (i.e. similar to the virtual hosts in apache)</p> <pre><code># set a backend upstream\nupstream my-app {\n    server 127.0.0.1:8080 fail_timeout=0;\n}\n\n# a server listen to port 80, which will redirect request to port 443\nserver {\n    listen 80;\n    server_name my-app.casd.local;\n\n    # redirect http request to https\n    return 301 https://$host$request_uri;\n\n    }\n\n# a server listen to port 443, which will ensure https resolution\nserver {\n    listen 443 ssl;\n    ssl_certificate /etc/ssl/certs/certificate.pem;\n    ssl_certificate_key /etc/ssl/private/private_key.pem;\n\n    # all request will be redirected to the backend_chart upstream\n    location / {\n        include proxy_params;\n        proxy_pass http://my-app;\n    }\n}\n</code></pre>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#8-set-up-firewall-to-block-remote-access-of-the-backend-service","title":"8. Set up firewall to block remote access of the backend service","text":"<p>To avoid users access the backend service directly, we can set port 8080 can be only accessed via 127.0.0.1.  </p>"},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#9-some-best-practices","title":"9. Some best practices","text":""},{"location":"adminsys/other_services/Nginx/01.Install_Nginx/#91-set-the-nginx-server-in-production","title":"9.1 Set the nginx server in production","text":"<p>We can notice in /etc/nginx, there are two folders - sites-available - sites-enabled</p> <p>The best solution is to write the configuration file in <code>sites-available</code> then create a symbolic link in <code>sites-enabled</code> All the server conf in <code>sites-enabled</code> will be activated automatically by the nginx server.</p> <p>For example</p> <pre><code>cd /etc/nginx\n# create the origin conf file\ntouch sites-available/test_site.conf\n# create the soft link to activate the conf\nln -s sites-available/test_site.conf sites-enabled/test_site.conf\n</code></pre> <pre><code>server {\n    listen 80;\n    server_name deb.casd.local;\n\n    # redirect http request to https\n    return 301 https://$host$request_uri;\n\n}\n\n\nserver {\n    listen 443 ssl;\n    ssl_certificate /etc/ssl/certs/casd_wildcard.pem;\n    ssl_certificate_key /etc/ssl/private/casd_wildcard_key.pem;\n    server_name deb.casd.local;\n\n    location / {\n        root /package-repo/aptly/.aptly/public;\n        autoindex on;\n        charset utf-8;\n        autoindex_exact_size off;\n        try_files $uri $uri/ =404;\n    }\n}\n</code></pre>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/","title":"Install nginx with basic http and https config","text":"<p>Nginx is a free and open-source web server used to host websites and applications of all sizes.  The software is known for its low impact on memory resources, high scalability, and its modular,  event-driven architecture which can offer secure, predictable performance. More than just a web server,  Nginx also works as a load balancer, an HTTP cache, and a reverse proxy.</p>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#install-nginx","title":"Install Nginx","text":"<p>Nginx is available in Debian\u2019s default software repositories, making it possible to install it from conventional  package management tools.</p> <pre><code>sudo apt update\nsudo apt install nginx\n</code></pre>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#adjusting-the-firewall","title":"Adjusting the Firewall","text":"<p>Before testing Nginx, it\u2019s necessary to modify the firewall settings to allow outside access to the default  web ports. Assuming that you followed the instructions in the prerequisites, you should have a UFW firewall  configured to restrict access to your server.</p> <p>During installation, Nginx registers itself with UFW to provide a few application profiles that can be used to enable or disable access to Nginx through the firewall.</p> <p>List the ufw application profiles by typing:</p> <pre><code>sudo ufw app list\n\n# Output\nAvailable applications:\n...\n  Nginx Full\n  Nginx HTTP\n  Nginx HTTPs\n  OpenSSH\n</code></pre> <p>It is recommended that you enable the most restrictive profile that will still allow the traffic you\u2019ve configured.  Since you will configure TLS/SSL for your server also in this guide, you will need to allow traffic for HTTP on port 80 and HTTPS on port 443.</p> <pre><code># You can enable this by typing:\nsudo ufw allow 'Nginx HTTP'\nsudo ufw allow 'Nginx HTTPS'\n\n# You can verify the change by typing:\nsudo ufw status\n</code></pre>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#check-nginx-status","title":"Check nginx status","text":"<p>At the end of the installation process, Debian 11 starts Nginx. The web server should already be up and running.</p> <p>You can check with the systemd init system to make sure the service is running by typing:</p> <pre><code># check the status\nsystemctl status nginx\n\n# get current server ip address\nip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\\/.*$//'\n\n# When you have your server\u2019s IP address, enter it into your browser\u2019s address bar:\n\nhttp://your_server_ip\n</code></pre> <p>Some useful command</p> <pre><code># To stop your web server\nsudo systemctl stop nginx\n\n# To start your web server\nsudo systemctl start nginx\n\n# To stop and then start the service again\nsudo systemctl restart nginx\n\n# If you are making configuration changes, Nginx can often reload without dropping connections.\nsudo systemctl reload nginx\n\n# Nginx is configured to start automatically when the server boots. If this is not what you want, \n# you can disable this behavior\nsudo systemctl disable nginx\n\nsudo systemctl enable nginx\n</code></pre>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#configure-a-server-block","title":"Configure a server block","text":"<p>When using the Nginx web server, server blocks (similar to virtual hosts in Apache) can be used to  encapsulate configuration details and host more than one domain on a single server. The following examples  use your_domain, but you should replace this with your actual domain name.</p> <pre><code># create a folder to put your site content\nsudo mkdir -p /var/www/your_domain/html\n\n# change the owner and acl\nsudo chown -R $USER:$USER /var/www/your_domain/html\nsudo chmod -R 755 /var/www/your_domain\n\n# add some demo page\nvim /var/www/your_domain/html/index.html\n</code></pre> <p>Put the below html content</p> <pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Welcome to your_domain&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Success! Your Nginx server is successfully configured for &lt;em&gt;your_domain&lt;/em&gt;. &lt;/h1&gt;\n&lt;p&gt;This is a sample page.&lt;/p&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>In order for Nginx to serve the above content(html page), you must create a server block with the correct  directives that point to your custom web root. Instead of modifying the default configuration file directly,  make a new one at <code>/etc/nginx/sites-available/your_domain</code>:</p> <pre><code>server {\n        listen 80;\n        listen [::]:80;\n\n        root /var/www/your_domain/html;\n        index index.html index.htm index.nginx-debian.html;\n\n        server_name your_domain www.your_domain;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n}\n</code></pre> <p>To enable a server block, you need to create a symbolic link to your custom configuration inside the <code>sites-enable</code>  directory</p> <pre><code>sudo ln -s /etc/nginx/sites-available/your_domain /etc/nginx/sites-enabled/\n</code></pre> <p>Normally, you will find two <code>server blocks</code> in /etc/nginx/sites-enabled/ - your_domain: Will respond to requests for your_domain and www.your_domain. - default: Will respond to any requests on port 80 that do not match the other two blocks.</p> <p>To avoid a possible hash bucket memory problem that can arise from adding additional server names to your  configuration, it is necessary to adjust a single value in the <code>/etc/nginx/nginx.conf</code> file. Open the file:</p> <pre><code>sudo vim /etc/nginx/nginx.conf\n\n# find the line server_names_hash_bucket_size 64; and uncomment it \n</code></pre> <p>Test the validity to make sure that there are no syntax errors in any of your Nginx files:</p> <pre><code>sudo nginx -t\n\n# if everything is ok, you can try to restart the nginx service\nsudo systemctl restart nginx\n</code></pre> <p>If everything goes well, you should see a new page, when you type the domain name in your browser</p>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#enable-https","title":"Enable HTTPS","text":"<p>The easiest way to enable https is to set up a new server blocks which listen on port 443 with ssl. Below is an example which convert the above http server block to https. </p> <pre><code>server {\n  listen 80;\n  server_name keycloak.casd.local;\n\n  # Redirect all traffic to SSL\n  rewrite ^ https://$host$request_uri? permanent;\n}\n\nserver {\n  listen 443 ssl default_server;\n\n  # enables SSLv3/TLSv1, but not SSLv2 which is weak and should no longer be used.\n  ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;\n\n  # disables all weak ciphers\n  ssl_ciphers ALL:!aNULL:!ADH:!eNULL:!LOW:!EXP:RC4+RSA:+HIGH:+MEDIUM;\n\n  server_name keycloak.casd.local;\n\n  ## Access and error logs.\n  access_log /var/log/nginx/access.log;\n  error_log  /var/log/nginx/error.log info;\n\n  ## Keep alive timeout set to a greater value for SSL/TLS.\n  keepalive_timeout 75 75;\n\n  ## See the keepalive_timeout directive in nginx.conf.\n  ## Server certificate and key.\n  ssl_certificate /opt/keycloak/keycloak-23.0.4/conf/wildcard-casd.pem;\n  ssl_certificate_key /opt/keycloak/keycloak-23.0.4/conf/wildcard-casd.key;\n  ssl_session_timeout  5m;\n\n  ## Strict Transport Security header for enhanced security. See\n  ## http://www.chromium.org/sts. I've set it to 2 hours; set it to\n  ## whichever age you want.\n  add_header Strict-Transport-Security \"max-age=7200\";\n\n  root /var/www/casd/html;\n  index index.html;\n}\n</code></pre>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#troubleshoot","title":"Troubleshoot","text":"<p>Even though, everything works on the server side, it does not mean the client side can view the server page correctly. Below are some common error you may encounter when you set up a https </p>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#err_ssl_version_or_cipher_mismatch","title":"ERR_SSL_VERSION_OR_CIPHER_MISMATCH","text":"<p>This error is caused by the compatibility between the supported ssl/tls protocol version between the server(e.g. nginx) and the client(e.g. chrome, curl). For example, for the newer version of Chrome, the protocol TLSv1 is not accepted, if Nginx server only supports this protocol, the handshake between client and server will fail.</p> <p>The solution is to add explicitly the newer version of the ssl protocol. For example, put the below line <code>ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;</code> in your https server blocks</p>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#err_ssl_key_usage_incompatible","title":"ERR_SSL_KEY_USAGE_INCOMPATIBLE","text":"<p>This error is caused by the key_usage option during the certificate generation. For <code>certain version of Chrome</code>, if the <code>key_usage</code> option is specified, and the certificate is self-signed. Chrome will reject the certificates with error mesage  ERR_SSL_KEY_USAGE_INCOMPATIBLE.</p> <p>The simplest solution is to remove the key usage option during the certificate generation.</p> <p>Or put <code>keyUsage = digitalSignature, keyEncipherment</code> as option. For more information on why chrome produce this error : https://chromeenterprise.google/policies/#RSAKeyUsageForLocalAnchorsEnabled</p>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#appendix","title":"Appendix","text":""},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#server-configuration","title":"Server Configuration","text":"<p>Below list all the import path for nginx service configuration</p> <ul> <li>/etc/nginx: The Nginx configuration directory. All the Nginx configuration files reside here.</li> <li>/etc/nginx/nginx.conf: The main Nginx configuration file. This can be modified to make changes to the Nginx global configuration.</li> <li>/etc/nginx/sites-available/: The directory where per-site server blocks can be stored. Nginx will not use the configuration files found in this directory unless they are linked to the sites-enabled directory. Typically, all server block configuration is done in this directory, and then enabled by linking to the other directory.</li> <li>/etc/nginx/sites-enabled/: The directory where enabled per-site server blocks are stored. Typically, these are created by linking to configuration files found in the sites-available directory.</li> <li>/etc/nginx/snippets: This directory contains configuration fragments that can be included elsewhere in the Nginx configuration. Potentially repeatable configuration segments are good candidates for refactoring into snippets.</li> </ul>"},{"location":"adminsys/other_services/Nginx/02.Enable_https_resolution/#server-logs","title":"Server Logs","text":"<ul> <li>/var/log/nginx/access.log: Every request to your web server is recorded in this log file unless Nginx is configured to do otherwise.</li> <li>/var/log/nginx/error.log: Any Nginx errors will be recorded in this log.</li> </ul>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/","title":"Using nginx as HTTP load balancer","text":"<p>Load balancing across multiple application instances is a commonly used technique for optimizing resource utilization, maximizing throughput, reducing latency, and ensuring fault-tolerant configurations.</p> <p>It is possible to use nginx as a very efficient HTTP load balancer to distribute traffic to several application  servers and to improve performance, scalability and reliability of web applications with nginx.</p>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/#load-balancing-methods","title":"Load balancing methods","text":"<p>The following load balancing mechanisms (or methods) are supported in nginx:</p> <ul> <li>round-robin: requests to the application servers are distributed in a round-robin fashion,</li> <li>least-connected: next request is assigned to the server with the least number of active connections,</li> <li>ip-hash:  a hash-function is used to determine what server should be selected for the next             request (based on the client\u2019s IP address).</li> </ul>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/#the-simplest-conf-example-round-robin","title":"The simplest conf example (round-robin)","text":"<pre><code>http {\n    upstream myapp1 {\n        server srv1.example.com;\n        server srv2.example.com;\n        server srv3.example.com;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://myapp1;\n        }\n    }\n}\n</code></pre> <p>In the example above, there are 3 instances of the same application running on srv1-srv3. <code>When the load balancing  method is not specifically configured, it defaults to **round-robin**</code>. All requests are proxied to the server  group myapp1, and nginx applies HTTP load balancing to distribute the requests.</p> <p>Reverse proxy implementation in nginx includes load balancing for <code>HTTP, HTTPS, FastCGI, uwsgi, SCGI, memcached, and gRPC</code>.</p> <p>To configure load balancing for HTTPS instead of HTTP, just use \u201chttps\u201d as the protocol.</p> <p>When setting up load balancing for FastCGI, uwsgi, SCGI, memcached, or gRPC, use fastcgi_pass, uwsgi_pass, scgi_pass,  memcached_pass, and grpc_pass directives respectively.</p>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/#least-connected-load-balancing","title":"Least connected load balancing","text":"<p>Another load balancing discipline is least-connected. Least-connected allows controlling the load on  application instances more fairly in a situation when some of the requests take longer to complete.</p> <p>With the least-connected load balancing, nginx will try not to overload a busy application server with excessive  requests, distributing the new requests to a less busy server instead.</p> <p>Least-connected load balancing in nginx is activated when the least_conn directive is used as part of the server  group configuration:</p> <pre><code>upstream myapp1 {\n    least_conn;\n    server srv1.example.com;\n    server srv2.example.com;\n    server srv3.example.com;\n}\n</code></pre>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/#session-persistence","title":"Session persistence","text":"<p>Please note that with round-robin or least-connected load balancing, each subsequent client\u2019s request can be  potentially distributed to a different server. There is no guarantee that the same client will be always directed to  the same server.</p> <p>If there is the need to tie a client to a particular application server  in other words, make the client\u2019s  session \u201csticky\u201d or \u201cpersistent\u201d in terms of always trying to select a particular server the ip-hash load balancing  mechanism can be used.</p> <p>With ip-hash, the client\u2019s IP address is used as a hashing key to determine what server in a server group should be  selected for the client\u2019s requests. This method ensures that the requests from the same client will always be directed  to the same server except when this server is unavailable.</p> <p>To configure ip-hash load balancing, just add the ip_hash directive to the server (upstream) group configuration:</p> <pre><code>upstream myapp1 {\n    ip_hash;\n    server srv1.example.com;\n    server srv2.example.com;\n    server srv3.example.com;\n}\n</code></pre>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/#weighted-load-balancing","title":"Weighted load balancing","text":"<p>It is also possible to influence nginx load balancing algorithms even further by using server weights.</p> <p>In the examples above, the server weights are not configured which means that all specified servers are treated as  equally qualified for a particular load balancing method.</p> <p>With the round-robin in particular it also means a more or less equal distribution of requests across the servers provided there are enough requests, and when the requests are processed in a uniform manner and completed fast enough.</p> <p>When the weight parameter is specified for a server, the weight is accounted as part of the load balancing decision.</p> <pre><code>upstream myapp1 {\n    server srv1.example.com weight=3;\n    server srv2.example.com;\n    server srv3.example.com;\n}\n</code></pre> <p>With this configuration, every 5 new requests will be distributed across the application instances as the  following: 3 requests will be directed to srv1, one request will go to srv2, and another one  to srv3.</p> <p>It is similarly possible to use weights with the least-connected and ip-hash load balancing in the recent versions of nginx.</p>"},{"location":"adminsys/other_services/Nginx/03.Use_Nginx_as_Http_load_balancer/#health-checks","title":"Health checks","text":"<p>Reverse proxy implementation in nginx includes in-band (or passive) server health checks. If the response from a  particular server fails with an error, nginx will mark this server as failed, and will try to avoid selecting this  server for subsequent inbound requests for a while.</p> <p>The max_fails directive sets the number of consecutive unsuccessful attempts to communicate with the server that  should happen during fail_timeout. By default, max_fails is set to 1. When it is set to 0, health checks are disabled  for this server. The fail_timeout parameter also defines how long the server will be marked as failed. After  fail_timeout interval following the server failure, nginx will start to gracefully probe the server with the live client\u2019s requests. If the probes have been successful, the server is marked as a live one.</p> <pre><code>    ssl on;\n    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n    ssl_certificate /etc/nginx/ssl/bundle.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n</code></pre>"},{"location":"container/Containerd/01.Introduction/","title":"Introduction of containerd","text":"<p>containerd is an industry-standard container runtime with an emphasis on simplicity, robustness, and portability.  It's available as a <code>daemon</code> for Linux and Windows. It manages the complete container lifecycle of its host system,  from image transfer and storage to container execution and supervision to low-level storage to network attachments  and beyond.</p> <p>containerd is designed to be embedded into a larger system(e.g. K8s), rather than being used directly by developers or end-users.</p>"},{"location":"container/Containerd/01.Introduction/#1-why-containerd","title":"1. Why containerd?","text":""},{"location":"container/Containerd/01.Introduction/#11-kubernetes-native-support-cri","title":"1.1 Kubernetes Native Support (CRI)","text":"<p>The old docker runtime does not directly support the CRI(Container Runtime Interface) of k8s, so Kubernetes had to use  a middle layer called <code>dockershim</code> to communicate with Docker. This adds overhead and complexity.</p>"},{"location":"container/Containerd/01.Introduction/#12-efficiency-and-performance","title":"1.2 Efficiency and Performance","text":"<p><code>Containerd</code> is a purpose-built container runtime that is <code>lightweight and efficient</code>. Since it is not tied to  Docker's other functionalities (like image building or extensive CLI commands), it is optimized solely for  container management.</p> <ul> <li>Lower Overhead: Docker includes additional features and a higher-level daemon that isn't needed for Kubernetes,                  adding extra layers of processing.</li> <li>Reduced Resource Consumption: By cutting out the Docker daemon, containerd consumes fewer resources,                   improving the overall performance and reducing the memory and CPU usage on each node.</li> </ul> <p>As a result, you can't use containerd to build docker image. You can only pull images and deploy a container.</p>"},{"location":"container/Containerd/01.Introduction/#13-oci-compliant","title":"1.3 OCI compliant","text":"<p>Containerd uses runc as low-level container runtime, which implements the Open Container Initiative (OCI) Runtime Specification.  OCI spec defines how containers should be created and run. As a result, runc is a standard across container runtimes.</p> <p><code>runc</code> interacts directly with the Linux kernel (e.g. cgroups and seccomp) to execute the containers, and make sure that containers run in isolated and secure environments. .</p>"},{"location":"container/Containerd/01.Introduction/#2-integration-in-docker-engine","title":"2. Integration in Docker engine","text":"<p>Containerd can be used inside <code>Docker engine</code>. The below figure shows the general architecture.</p> <p></p>"},{"location":"container/Containerd/01.Introduction/#3-integration-in-k8s","title":"3. Integration in K8s","text":"<p>In k8s worker nodes, a daemon called <code>kubelet</code> is responsible for communicating with the container runtime(e.g. containerd). This communication is ensured by CRI (Container Runtime Interface), which is a Kubernetes-specific API that  allows Kubernetes to interact with various container runtimes in a standardized way.  Kubernetes uses this interface to manage containers on nodes, regardless of the underlying container runtime. </p>"},{"location":"container/Containerd/01.Introduction/#31-general-workflow","title":"3.1 General workflow","text":"<p>Step1: users issue a command <code>kubectl apply -f deployment.yaml</code>, kubectl apply sends a <code>REST request</code> to the         Kubernetes API server to create or update the Deployment resource defined in the deployment.yaml file.</p> <p>Step2: The API server validates the request (ensuring the resource definition is correct) and stores the         Deployment object in etcd. This object represents the desired state: the number of replicas,         the pod template, labels, etc.</p> <p>Step3: The controller manager gets notified by the API server about the new Deployment. It continuously watches         for changes to Deployment resources, detects the new or updated Deployment object. If the current stat does not        correspond the desired state. New pods creation request will be sent to scheduler.</p> <p>Step4: The scheduler takes into account factors such as resource requests (CPU, memory), node availability, taints,         and affinity/anti-affinity rules to <code>assign the pod to a suitable node</code>.</p> <p>Step5: Kubelet of the target node receives the pod specs from the scheduler. It passes the request to <code>Containerd through the CRI</code>.</p> <p>Step6: Containerd then pulls the required container images from the specified container         registry (e.g., DockerHub, Google Container Registry) if they are not already present on the node.        Containerd creates and starts the containers based on the pod spec, handling everything from resource         allocation (CPU, memory) to networking and storage. It also manages container logs and any other         runtime-specific functions (e.g., container pause/unpause).</p> <p>Step7: The Kubelet continues to monitor the pod's health, and if a pod or container fails, Kubernetes will         attempt to restart it to maintain the desired state.</p>"},{"location":"container/Containerd/01.Introduction/#4-other-important-client","title":"4. Other important client","text":"<p>Containerd uses CRI to communicate with k8s, it also provides two other clients:</p> <ul> <li>CTR: is a low-level CLI tool for users interacting directly with containerd. </li> <li>crictl: </li> </ul>"},{"location":"container/Containerd/01.Introduction/#41-ctr","title":"4.1 CTR","text":"<p>It is primarily used for debugging, testing, and interacting with containerd without going through higher-level tools  like <code>CRI or Docker</code>. While ctr can be used to manage containers, images, namespaces, and snapshots,  it is not recommended for general-purpose use in production environments.</p>"},{"location":"container/Containerd/01.Introduction/#42-crictl","title":"4.2 crictl","text":"<p><code>crictl</code> is a lightweight command-line tool for managing container runtimes. It is particularly useful for  Kubernetes environments that use CRI-compatible runtimes. It's not bundled with contianerd. You need to follow the below steps to install <code>crictl</code> on an Ubuntu/debian system.</p> <p>You can find the official github page here</p> <p>My k8s cluster version <code>1.31.1</code>, so I choose 1.31.1 as the crictl version</p> <pre><code># Step1: get the source\nVERSION=\"v1.31.1\"  # You can change this to the latest version\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz\n\n# step2: unzip the source\ntar -xzvf crictl-v1.31.1-linux-amd64.tar.gz\n# you should see a bin called crictl after unzip\n\n# step3: copy the bin to your local bin, so you can call it from anywhere\nsudo mv crictl /usr/local/bin/\n\n# step4: check the version\ncrictl --version\n\n# step5: config crictl endpoint\nsudo vim /etc/crictl.yaml\n\n# put the following content in it, you need to choose one which fit your installation\n# example configuration for Docker:\nruntime-endpoint: unix:///var/run/cri-dockerd.sock\n\n# for contained:\nruntime-endpoint: unix:///run/containerd/containerd.sock\n\n# step6: check your endpoint info\nsudo crictl info\n</code></pre> <p>If you have an older version of kubeadm, the newer version of the crictl should work without problems. </p> <p>If you see the details of your endpoint, now you can run any crictl command  Below are a list of useful command</p> <pre><code># list all existing images\nsudo crictl images\n\n# pull a container image\nsudo crictl pull &lt;image-name&gt;:&lt;tag&gt;\n# for example\nsudo crictl pull nginx:latest\n\n# delete a image\nsudo crictl rmi &lt;image-name&gt;\n# example\nsudo crictl rmi nginx:latest\n\n# view all running pods\nsudo crictl pods\n\n# list all containers (running and stopped)\nsudo crictl ps -a\n\n# view container logs\nsudo crictl logs &lt;container-id&gt;\n\n# start/stop a container\nsudo crictl start/stop/pause/unpause &lt;container-id&gt;\n\n# Attach to a Running Container\nsudo crictl attach &lt;container-id&gt;\n\n# execute a command inside a running container\nsudo crictl exec &lt;container-id&gt; &lt;command&gt;\n# for example\nsudo crictl exec 712fe34fbf5b9  /bin/sh\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/","title":"Install and config containerd","text":"<p>You can find the official doc for installing <code>containerd</code> here.</p>"},{"location":"container/Containerd/02.Install_config_containerd/#1-prerequisites","title":"1. Prerequisites","text":"<p>To make <code>containerd</code> run correctly, we need to do the following config - add two moduls into the linux kernel(e.g. <code>overlay, br_netfilter</code>) - if <code>containerd</code> needs to work with <code>k8s cri</code>, you need to reconfigure some kernel parameters</p>"},{"location":"container/Containerd/02.Install_config_containerd/#11-add-modules-to-linux-kernel","title":"1.1 Add modules to linux kernel","text":"<p>The below command will create a file <code>containerd.conf</code> in <code>/etc/modules-load.d/</code>, and add two lines <code>overlay, and br_netfilter</code> in the <code>containerd.conf</code> file.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre> <p>The file specifies the kernel modules <code>overlay, and br_netfilter</code> should be automatically loaded at boot.</p> <ul> <li>overlay: This module is required for <code>containerd</code> and <code>Docker</code>. It enables the <code>OverlayFS filesystem</code>, which                  helps efficiently store container layers.</li> <li>br_netfilter: This module ensures that bridged traffic is correctly processed by iptables. It's an essential module for Kubernetes networking (iptables-based rules)</li> </ul> <p>To load the module without rebooting, you can load the module manually with the below command</p> <pre><code>sudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre> <p>To check if the module is loaded correctly or not, you can use the below command.</p> <pre><code>lsmod | grep -E 'overlay|br_netfilter'\n\n# the expected output\nbr_netfilter           32768  0\nbridge                262144  1 br_netfilter\noverlay               147456  0\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#12-reconfigure-kernel-parameters-if-containerd-is-installed-for-k8s-cluster","title":"1.2 Reconfigure kernel parameters (If containerd is installed for k8s cluster)","text":"<p>We need to modify the value of the below kernel parameters:</p> <ul> <li>net.bridge.bridge-nf-call-iptables = 1: this parameter ensures that <code>iptables processes traffic from bridged network interfaces</code>.                                   It's required for <code>Kubernetes networking</code> (especially CNI plugins like Flannel, <code>Calico</code>).</li> <li>net.ipv4.ip_forward = 1: this parameter enables <code>IP forwarding</code>, allowing the machine to route packets.                                  It's required for <code>Kubernetes pod-to-pod communication</code>.</li> <li>net.bridge.bridge-nf-call-ip6tables = 1: this parameter enables <code>ip6tables processes IPv6 bridged traffic</code>. It's                            useful if your cluster supports IPv6 networking.</li> </ul> <p>IP forwarding is a feature in the Linux kernel that allows a <code>machine to act as a router</code>, forwarding  network packets from one interface to another. By default, Linux does not forward packets between network interfaces unless explicitly enabled.</p> <p>As all kernel parameters are stored in <code>/etc/sysctl.d/</code>, by convention, we create a config file <code>9-kubernetes-cri.conf</code>, and put the above three lines in it. You can use the below command to do it.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre> <p>To apply the change you can use</p> <pre><code># The below command will trigger the system to read and apply all settings from:\n# /etc/sysctl.conf\n# /run/sysctl.d/*.conf\n# /etc/sysctl.d/*.conf\n# /usr/lib/sysctl.d/*.conf\nsudo sysctl --system\n\n# test the updated value\nsudo sysctl net.bridge.bridge-nf-call-iptables\nsudo sysctl net.ipv4.ip_forward\nsudo sysctl net.bridge.bridge-nf-call-ip6tables\n\n# or you can test all with oneliner\nsudo sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#2-install-containerd-via-apt","title":"2. Install containerd (via apt)","text":"<p>The <code>containerd.io</code> packages in <code>DEB</code> and <code>RPM</code> formats are distributed by <code>Docker (not by the containerd project)</code>.  The Docker documentation for how to  set up apt-get to install containerd.io packages:</p>"},{"location":"container/Containerd/02.Install_config_containerd/#21-remove-old-versions-if-exist","title":"2.1 Remove old versions if exist","text":"<p>Your Linux distribution may provide <code>unofficial Docker packages</code>, which may conflict with the official packages  provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.</p> <p>The unofficial packages to uninstall are:</p> <ul> <li>docker.io</li> <li>docker-compose</li> <li>docker-doc</li> <li>podman-docker</li> </ul> <p>Run the following command to uninstall all conflicting packages:</p> <pre><code># remove all conflicting packages\nfor pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done\n\n# Remove the installed default config file\nrm /etc/containerd/config.toml\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#22-configure-the-official-repo","title":"2.2 Configure the official repo","text":"<pre><code># install required packages\nsudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg\n\n# add gpg key for the containerd.io repo\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# add docker.io repo to source.list\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#23-install-containerd-binary-and-set-up-config","title":"2.3 Install containerd binary and set up config","text":"<p>Install the containerd binary via new repo</p> <pre><code>sudo apt update\nsudo apt install containerd.io\n\n# check the installed version\ncontainerd --version\n\n# expected output\ncontainerd containerd.io 1.7.25 bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#3-configure-containerd-after-installation","title":"3. Configure containerd after installation","text":"<p>There is no default configuration file after installation. You need to create one by yourself. </p>"},{"location":"container/Containerd/02.Install_config_containerd/#31-generate-a-default-config","title":"3.1 Generate a default config","text":"<pre><code># generate a default config\ncontainerd config default | sudo tee /etc/containerd/config.toml \n</code></pre> <p>The default config file needs to be modified, below are some points you need to pay attention to: - cgroup driver setting: <code>containerd</code> requires cgroup(linux kernel) to setup resources(e.g. cpu, memory, etc.) and                         security policies. It uses a cgroup driver to communicate with the cgroup.                          You need to adapt this value based on your system setting - sandbox image url: In k8s, all pods have a sandbox container that uses sandbox image. - custom image registry: If you want to use custom image registry, you need to change the default config too.</p>"},{"location":"container/Containerd/02.Install_config_containerd/#32-change-the-cgroup-driver-to-systemd","title":"3.2 Change the cgroup driver to systemd","text":"<p>Find the following section <code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]</code> in <code>/etc/containerd/config.toml</code>, then add this line <code>SystemdCgroup = true</code> under it.</p> <p>Or you can run the below command to change it, note it only works if the generated config contains <code>SystemdCgroup = false</code></p> <pre><code>sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml \n</code></pre> <p>After the above command, you should see the below lines in <code>etc/containerd/config.toml</code></p> <pre><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#33-change-sandbox-image-pull-url","title":"3.3 change sandbox image pull url","text":"<p>All pods in k8s require a sandbox image, the containerd default config has a default sandbox image pulling image. Based on the k8s version, you need to adapt this value.</p> <p>You should find the below line, and change the image url to your k8s required image url.</p> <pre><code>sandbox_image = \"k8s.lixx.cn/pause:3.10\"\n</code></pre> <p>Every Kubernetes Pod has a <code>Pause container</code>(sandbox container) that holds the <code>network namespace</code> and acts as the  parent of all other containers in the Pod. It has two main goals: - Keeps Pod Resources Active \u2013 It ensures that <code>networking and IPC resources remain stable</code> even if app containers restart.  - Efficient Namespace Sharing \u2013 Other containers in the Pod share its <code>PID, network, and IPC namespaces</code>.</p>"},{"location":"container/Containerd/02.Install_config_containerd/#34-custom-image-registry","title":"3.4 Custom image registry","text":"<p><code>containerd</code> allows us to use custom image registry. Suppose our image registry runs at https://reg.casd.local.</p> <p>You need to edit the containerd config file <code>/etc/containerd/config.toml</code></p> <pre><code>sudo vim /etc/containerd/config.toml\n\n# find the  plugins.\"io.containerd.grpc.v1.cri\".registry section in the config.tom\n# add the below line, you need to change the url value to your url\n[plugins.\"io.containerd.grpc.v1.cri\".registry]\n        [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]\n           [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"reg.casd.local\"]\n               endpoint = [\"https://reg.casd.local\"]\n        [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"reg.casd.local\".tls]\n           insecure_skip_verify = true\n        [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"reg.casd.local\".auth]\n           username = \"toto\"\n           password = \"chagneMe\"\n        [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"]\n          endpoint = [\"https://registry-1.docker.io\"]\n\n# now we need to reload the daemon\nsudo systemctl daemon-reload\n\n# restart containerd\nsudo systemctl restart containerd\n\n# try to pull an image from the private image registry\nsudo crictl pull reg.casd.local/casd/redis\n</code></pre> <p>DEPRECATION: The <code>mirrors</code> property of <code>[plugins.\"io.containerd.grpc.v1.cri\".registry]</code>  is deprecated since containerd v1.5 and will be removed in containerd v2.1. Use <code>config_path</code> instead.  For now, we don't migrate to <code>config_path</code>, because it does not provide all features(for example .auth).</p>"},{"location":"container/Containerd/02.Install_config_containerd/#35-restart-and-enable-containerd-service-on-all-the-nodes","title":"3.5 Restart and enable containerd service on all the nodes","text":"<pre><code>$ sudo systemctl restart containerd\n$ sudo systemctl enable containerd\n\n# you can check the service status\n$ sudo systemctl status containerd\n\n# check the version\ncontainerd --version\n\n# you can use the default containerd client to test\nctr version\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#4-install-and-setup-containerd-client","title":"4. Install and setup containerd client","text":"<p>Below is a list of <code>containerd</code> major clients: - ctr: default - crictl: client compatbile with k8s</p>"},{"location":"container/Containerd/02.Install_config_containerd/#41-ctr","title":"4.1 ctr","text":"<p>containerd provides a CLI called <code>ctr</code>. It's a low-level CLI tool designed for <code>direct interaction with containerd</code>. It supports the complete life-cycle of a container(e.g. pull image, create  container, etc.) But it does not support the Kubernetes <code>CRI (Container Runtime Interface)</code>.</p> <pre><code># list containers\nctr containers list\n\n# pull image\nctr images pull docker.io/library/nginx:latest\n\n# list images\nsudo ctr images ls\n\n# run a container\nctr run --rm -t docker.io/library/nginx:latest my-nginx\n</code></pre> <p>All the cri pluging configurations in <code>/etc/containerd/config.toml</code> are ignored by ctr, because it does not support CRI. That's we recommend you to use crictl for testing.</p> <p>For more details of ctr , you can visit this page</p>"},{"location":"container/Containerd/02.Install_config_containerd/#42-crictl","title":"4.2 crictl","text":"<p>The official docs of <code>crictl</code> can be found here.</p> <p>The latest release version can be found here.</p> <p>Follow the below steps to install and config crictl</p> <pre><code># install crictl bin\nVERSION=\"v1.32.0\"\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz\nsudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin\nrm -f crictl-$VERSION-linux-amd64.tar.gz\n\n# config crictl.yaml\ncat &lt;&lt;EOF | sudo tee /etc/crictl.yaml\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: false\nEOF\n</code></pre> <pre><code># list all images\nsudo crictl images\n\n# pull images\nsudo crictl pull reg.casd.local/casd/redis\n\n# list pods\nsudo crictl pods\n\n# list containers\nsudo crictl ps -a\n\n# get logs of a container\nsudo crictl logs &lt;conainer_id&gt;\n</code></pre>"},{"location":"container/Containerd/02.Install_config_containerd/#appendix-cgroup-and-systemd","title":"Appendix: cgroup and systemd","text":"<p>Control Groups (cgroups) and systemd are closely related because <code>systemd</code> is responsible for managing  system services and processes, and it heavily relies on <code>cgroups</code> to do so. </p> <ul> <li>systemd organizes processes using cgroups to track and manage resource usage; </li> <li>Every service, user session, or scope started by systemd gets its own cgroup. For example, the <code>nginx.service</code> runs in <code>/sys/fs/cgroup/system.slice/nginx.service/</code></li> <li>This allows fine-grained control over CPU, memory, I/O, and other resources.</li> </ul> <pre><code># test cgroup version\nmount | grep cgroup\n\n# expected output\ncgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)\n\n\n# we can set max cpu and memory for a service\nsystemctl set-property nginx.service CPUQuota=50%\nsystemctl set-property nginx.service MemoryMax=500M\n</code></pre>"},{"location":"container/Docker/01.Install_docker_dockerCompose/","title":"Docker and Docker Compose","text":"<p>In this tutorial, we learn how to install docker and run a docker image(container)</p>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#1introduction","title":"1.Introduction","text":""},{"location":"container/Docker/01.Install_docker_dockerCompose/#docker-editions","title":"Docker Editions","text":"<p>There are two editions of Docker available.</p> <ul> <li>Community Edition (CE): ideal for individual developers and small teams looking to get started with Docker and    experimenting with container-based apps.</li> <li>Enterprise Edition (EE): Designed for enterprise development and IT teams who build, ship, and run business-critical    applications in production at scale.</li> </ul> <p>This guide will cover installation of Docker CE on Debian Linux. But let\u2019s first look at common docker terminologies.</p>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#docker-components-terminologies","title":"Docker Components / Terminologies","text":"<p>Below are commonly used terminologies in Docker ecosystem.</p> <ul> <li>Docker daemon: This is also called Docker Engine, it is a background process which runs on the host system responsible for building and running of containers.</li> <li>Docker Client: This is a command line tool used by the user to interact with the Docker daemon.</li> <li>Docker Image: An image is an immutable file that\u2019s essentially a snapshot of a container. A docker image has a file system and application dependencies required for running applications.</li> <li>Docker container: This is a running instance of a docker image with an application and its dependencies. Each container has a unique process ID and isolated from other containers. The only thing containers share is the Kernel.</li> <li>Docker registry: This is an application responsible for managing storage and delivery of Docker container images. It can be private or public.</li> </ul>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#2-install-docker-ce-on-debian-121110","title":"2. Install Docker CE on Debian 12/11/10","text":"<p>1) Install Dependency packages Start the installation by ensuring that all the packages used by docker as dependencies are installed.</p> <pre><code>sudo apt update\nsudo apt -y install apt-transport-https ca-certificates curl gnupg2 software-properties-common\n</code></pre> <p>2) Add Docker\u2019s official GPG key Import Docker GPG key used for signing Docker packages.</p> <pre><code>curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker-archive-keyring.gpg\n</code></pre> <p>3) Add the Docker repository Add Docker repository which contain the latest stable releases of Docker CE.</p> <pre><code>sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/debian \\\n   $(lsb_release -cs) \\\n   stable\"\n</code></pre> <p>This command will add the line shown in <code>/etc/apt/sources.list</code> file.</p> <p>4) Install Docker and Docker Compose</p> <pre><code>#Update the apt package index.\nsudo apt update\n\n# To install Docker CE on Debian, run the command:\nsudo apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y\n\n\n# Start and enable docker service:\nsudo systemctl enable --now docker\n</code></pre> <p>This installation will add docker group to the system without any users. Add your user account to the group to run  docker commands as non-privileged user.</p> <pre><code># add docker group to current user\nsudo usermod -aG docker $USER\n\n# you need to re-login to get the updated group \n</code></pre>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#3-test-the-docker-and-docker-compose","title":"3. Test the docker and docker compose","text":"<p>Check docker and compose version.</p> <pre><code>docker version\n\ndocker compose version\n</code></pre>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#31-run-a-test-docker-container","title":"3.1 Run a test docker container","text":"<pre><code>docker run --rm -it  --name test alpine:latest /bin/sh\n\n# this will open a shell in the container, you can get the os info with below command\ncat /etc/os-release\nNAME=\"Alpine Linux\"\nID=alpine\nVERSION_ID=3.16.0\nPRETTY_NAME=\"Alpine Linux v3.16\"\nHOME_URL=\"https://alpinelinux.org/\"\nBUG_REPORT_URL=\"https://gitlab.alpinelinux.org/alpine/aports/-/issues\"\n\n# exit the shell\n</code></pre>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#32-test-docker-compose","title":"3.2 Test Docker Compose","text":"<pre><code># Create a test Docker Compose file.\nvim docker-compose.yml\n\n# Add below data to the file.\nversion: '3'  \nservices:\n  web:\n    image: nginx:latest\n    ports:\n     - \"8080:80\"\n    links:\n     - php\n  php:\n    image: php:7-fpm\n\n# Start service containers. the current working directory must contain the compose config file\nsudo docker compose up -d\n\n# show running containers\nsudo docker compose ps\n\n# Destroy containers\ndocker compose stop\ndocker compose rm\n\n# output\nGoing to remove vagrant_web_1, vagrant_php_1\nAre you sure? [yN] y\nRemoving vagrant_web_1 \u2026 done\nRemoving vagrant_php_1 \u2026 done\n</code></pre>"},{"location":"container/Docker/01.Install_docker_dockerCompose/#4-docker-supervisionui","title":"4. Docker supervision(UI)","text":"<p>https://computingforgeeks.com/install-docker-ui-manager-portainer/</p>"},{"location":"container/Docker/02.Use_docker/","title":"Use docker","text":"<p>In this tutorial, we will list the useful commands of the docker </p>"},{"location":"container/Docker/02.Use_docker/#1-docker-daemon-management-commands","title":"1. docker daemon management commands","text":"<pre><code># Start the docker daemon \ndocker -d \n\n# Get help with Docker. Can also use help on all subcommands \ndocker --help \n\n# Display system-wide information \ndocker info\n\n# show docker version\ndocker version\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#2-images-gestion","title":"2. Images gestion","text":""},{"location":"container/Docker/02.Use_docker/#21-build-image-from-docker-file","title":"2.1 Build image from docker file","text":"<p><code>docker build</code> command can build a <code>docker image</code> by using a <code>docker file</code>. The <code>-t</code> option is recommended, it ensures that your images are tagged properly.</p> <pre><code># general form\ndocker build -t &lt;image-name&gt;:&lt;tag-name&gt; &lt;docker-file-path&gt;\n\n# Build an Image from a Dockerfile, the current directory must contain a docker file \ndocker build -t &lt;image_name&gt;:&lt;version&gt; .\n\n# Build an Image from a Dockerfile without the cache\ndocker build -t &lt;image_name&gt;:&lt;version&gt; . no-cache   \n</code></pre> <p>Below is an example</p> <pre><code># create a folder to host docker file and related config file\nmkdir test_image\n\n# Put a docker file and config file in test_image\ntest_image/\n\u251c\u2500\u2500 config.sh\n\u2514\u2500\u2500 Dockerfile\n</code></pre> <p>The content of the <code>Dockerfile</code></p> <pre><code>FROM busybox:latest\nLABEL MAINTAINER=pengfei.liu@casd.eu\nLABEL version=\"1.0\"\nCOPY config.sh /etc/spark/config.sh\nRUN cat /etc/spark/config.sh      \n</code></pre> <p>The content of the <code>config.sh</code></p> <pre><code>export JAVA_HOME=/opt/java/java_8\n</code></pre> <p>You can find the full content of the docker file here</p> <p>Check the result</p> <pre><code># build an image with the given dockerfile\ndocker build -t my-img:0.0.1 ./test_image\n\n# check the image\ndocker image ls\n\n# output example\nREPOSITORY                            TAG                            IMAGE ID       CREATED          SIZE\nmy-img                                0.0.1                          b4ecf828f680   27 seconds ago   1.24MB\n</code></pre> <pre><code># convert a container to image\ndocker commit &lt;container_name/id&gt; &lt;image_name&gt;\n\n# List local images \ndocker images \n\n# Delete an Image \ndocker rmi &lt;image_name&gt; \n\n# Remove all unused images \ndocker image prune \n</code></pre>"},{"location":"container/Docker/02.Use_docker/#22-tag-and-push-the-local-image-to-remote-image-repo","title":"2.2 Tag and push the local image to remote image repo","text":"<p>The commands to tag and push images to remote repo may be different base on the remote repo. The below example shows how to tag and push images to: - docker hub - harbor</p>"},{"location":"container/Docker/02.Use_docker/#221-tag-an-image","title":"2.2.1 Tag an image","text":"<p>Before pushing the image to remote repo, we need to tag it properly.</p> <pre><code># general form\ndocker tag old_name[:TAG] new_name[:TAG]\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#222-tag-and-push-to-docker-hub","title":"2.2.2 Tag and push to docker hub","text":"<p>The general form for the docker hub tag is /:.  In below example, we will push the local image <code>my-img:0.0.1</code> to dockerhub <code>liupengfei99/test:v2</code> <p>The repo <code>test</code> must be created before push </p> <pre><code># change the tag for docker hub\ndocker tag my-img:0.0.1 liupengfei99/test:v2\n\n# check the result\ndocker images\n\n# output\nREPOSITORY                            TAG                            IMAGE ID       CREATED          SIZE\nmy-img                                0.0.1                          b4ecf828f680   13 minutes ago   1.24MB\nliupengfei99/test                     v2                             b4ecf828f680   13 minutes ago   1.24MB\n\n# login to docker hub\ndocker login\n\n# push the image\ndocker push liupengfei99/test:v2\n</code></pre> <p>In your docker hub web ui, you should see the newly pushed image</p>"},{"location":"container/Docker/02.Use_docker/#223-tag-and-push-the-local-image-to-harbor","title":"2.2.3 Tag and push the local image to Harbor","text":"<p>In below example, we will push the local image <code>my-img:0.0.1</code> to harbor <code>reg.casd.local/test/test-img:v1</code></p> <p>The general form for the harbor tag is //:.  <pre><code># change the tag for harbor\ndocker tag my-img:0.0.1 reg.casd.local/test/test-img\n\n# check the new tag\ndocker images\n\n# output\nREPOSITORY                            TAG                            IMAGE ID       CREATED          SIZE\nmy-img                                0.0.1                          b4ecf828f680   25 minutes ago   1.24MB\nliupengfei99/test                     v2                             b4ecf828f680   25 minutes ago   1.24MB\nreg.casd.local/test/test-img          latest                             b4ecf828f680   25 minutes ago   1.24MB\n\n\n# login to harbor\ndocker login reg.casd.local\n\n# push the image\ndocker push reg.casd.local/test/test-img\n</code></pre> <p>Now you can check your harbor web UI, in the <code>project test</code>, you should see the image <code>test-img</code>.</p> <pre><code># login to docker public registry (docker hub)\ndocker login\n\n# tag the image\ndocker tag &lt;local-image-name&gt; &lt;username&gt;/repository:tag\n\n# for example, here pengfei99 is my docker hub account name, test is a repo of this account\ndocker tag pythondemo pengfei99/test:v1\n\n# check the tagged image\n# You can notice there is a new row liupengfei99/test which shares the same image id as pythondemo\ndocker image ls\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\npythondemo          latest              3ea6dc02e4a6        24 minutes ago      131MB\nliupengfei99/test   v1                  3ea6dc02e4a6        24 minutes ago      131MB\npython              2.7-slim            ca96bab3e2aa        2 weeks ago         120MB\n\n\n# push the image to docker hub\ndocker push username/repository:tag\n\n# example\ndocker push liupengfei99/test:v1\n\n# pull and run the image from the remote repository\ndocker run -p 4000:80 liupengfei99/test:v1\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#3-containers-management","title":"3. Containers management","text":"<pre><code># Create and run a container from an image, with a custom name: \ndocker run --name &lt;container_name&gt; &lt;image_name&gt; \n\n# Run a container with and publish a container\u2019s port(s) to the host. \ndocker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt; \n\n# Run a container in the background \n# -d option means run container in the detach mode\ndocker run -d &lt;image_name&gt; \n\n# Run a container in interactive mode\n# you can add -rm option to remove the container after stop\ndocker run -it &lt;image_name&gt; &lt;shell_path&gt;\n\n# Start or stop an existing container: \ndocker start|stop &lt;container_name&gt; (or &lt;container-id&gt;) \n\n# Remove a stopped container: \ndocker rm &lt;container_name&gt; \n\n# Fetch and follow the logs of a container: \ndocker logs -f &lt;container_name&gt; \n\n# To inspect a running container: \ndocker inspect &lt;container_name&gt; (or &lt;container_id&gt;) \n\n# To list currently running containers: \ndocker ps \n\n# List all docker containers (running and stopped): \ndocker ps --all/-a \n\n# View resource usage stats \ndocker container stats\n\n# copy data from container to local\ndocker cp &lt;container-name/id&gt;:&lt;data-path&gt; &lt;local-path&gt;\n\n# un example copy folder /apache-atlas/conf/ from container to /tmp/conf on local\ndocker cp atlas:/apache-atlas/conf/ /tmp/conf/\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#31-debug-a-container","title":"3.1 Debug a container","text":"<p>To debug a container, you can show the logs, get a shell, etc.</p> <pre><code># Open a shell inside a running container: \n# in the shell_path, you need to put the path of which shell you want to use. \n# It also depends on the base image, for example, for the debian base image, you can use /bin/bash\ndocker exec -it &lt;container_name&gt; &lt;shell_path&gt; \n\n# show the live logs of a running daemon container\ndocker logs -f &lt;container_name&gt;\n\n# show the exposed ports of a container\ndocker port &lt;container_name&gt;\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#32-mount-volume-on-container","title":"3.2 Mount volume on container","text":"<p>Docker containers are immutable by nature. This means that restarting a container erases all your stored data in  the container. To persist data, Docker provides two mechanisms: - docker volumes (The docker volume is a directory created by docker and host at the docker storage directory) - bind mounts (local directory which is created and managed by user.)</p>"},{"location":"container/Docker/02.Use_docker/#33-local-directory-binding","title":"3.3 Local directory binding","text":"<pre><code>docker run --name mysql-db -v $(pwd)/datadir:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:8.0.28-debian\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#4-docker-volume","title":"4. docker volume","text":"<pre><code># create the volume\ndocker volume create &lt;volume-name&gt;\n\n# list existing volume\ndocker volume list\n\n# mount volume on the container\ndocker run --name &lt;container-name&gt; -v &lt;volume-name&gt;:&lt;container-mount-path&gt; &lt;image-name&gt;\n\n# remove volume\ndocker volume remove &lt;volume-name&gt; \n</code></pre>"},{"location":"container/Docker/02.Use_docker/#5-docker-container-network","title":"5. Docker container network","text":"<p>Container networking refers to the ability for containers to communicate with other containers and/or host services.  <code>Containers have networking enabled by default</code>. A container has no information about what kind of network  it's attached to, or whether their peers are also Docker workloads or not. A container only sees a network  interface with an IP address, a gateway, a routing table, DNS services, and other networking details. </p>"},{"location":"container/Docker/02.Use_docker/#51-user-define-networks","title":"5.1 User define networks","text":"<p>You can create <code>user-defined networks, and connect multiple containers to the same network</code>. Once connected to a  user-defined network, containers can communicate with each other using container IP addresses or container names.</p> <p>The following example creates a network using the bridge network driver and running a container in the created network:</p> <pre><code># create a docker network\ndocker network create -d bridge my-network\n\n# create a container which uses the custom network\ndocker run --network=my-network -itd --name=container3 busybox\n</code></pre> <p>You can notice, when we create a network, we need to specify the network driver type.</p> <p>The following network drivers are available by default:</p> Driver Description bridge The default network driver. host Remove network isolation between the container and the Docker host. none Completely isolate a container from the host and other containers. overlay Overlay networks connect multiple Docker daemons together. ipvlan IPvlan networks provide full control over both IPv4 and IPv6 addressing. macvlan Assign a MAC address to a container."},{"location":"container/Docker/02.Use_docker/#52-attach-to-other-container-network","title":"5.2 Attach to other container network","text":"<p>In addition to user-defined networks, you can attach a <code>container to another container's networking stack directly,</code>  using the --network container: flag format. <p>The following flags aren't supported for containers using the container: networking mode:</p> <ul> <li>--add-host</li> <li>--hostname</li> <li>--dns</li> <li>--dns-search</li> <li>--dns-option</li> <li>--mac-address</li> <li>--publish</li> <li>--publish-all</li> <li>--expose</li> </ul> <p>The following example runs a Redis container, with Redis binding to localhost, then running the redis-cli command and  connecting to the Redis server over the localhost interface.</p> <pre><code>docker run -d --name redis redis --bind 127.0.0.1\ndocker run --rm -it --network container:redis redis-cli -h 127.0.0.1\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#53-published-ports","title":"5.3 Published ports","text":"<p>By default, when you create or run a container using docker create or docker run, the container doesn't expose any  of its ports to the outside world. Use the --publish or -p flag to make a port available to services outside of  Docker. This creates a firewall rule in the host, mapping a container port to a port on the Docker host to the outside  world. Here are some examples:</p> Flag value Description -p 8080:80 Map port 8080 on the Docker host to TCP port 80 in the container. -p 192.168.1.100:8080:80 Map port 8080 on the Docker host IP 192.168.1.100 to TCP port 80 in the container. -p 8080:80/udp Map port 8080 on the Docker host to UDP port 80 in the container. -p 8080:80/tcp -p 8080:80/udp Map TCP port 8080 on the Docker host to TCP port 80 in the container, and map UDP port 8080 on the Docker host to UDP port 80 in the container. <p>Publishing container ports is insecure by default. Meaning, when you publish a container's ports it becomes available  not only to the Docker host, but to the outside world as well.</p> <p>If you include the localhost IP address (127.0.0.1) with the publish flag, only the Docker host can access the published container port.</p> <pre><code>docker run -p 127.0.0.1:8080:80 nginx\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#6-delete-docker-objects","title":"6. ## Delete docker objects","text":"<p>After we created docker image, container, volume, network, we may need to delete them to clear the working space</p>"},{"location":"container/Docker/02.Use_docker/#61-purging-all-unused-or-dangling-resources","title":"6.1 Purging all unused or dangling resources","text":"<p>The first command is <code>system prune</code>, which will delete all unused Docker objects: - containers - images - networks - volumes</p> <p>Below are some command example</p> <pre><code># remove all unused objects\ndocker system prune\n\n# with the --filter option, we can filter which objects we want to delete.\n# the below example deletes containers that have been stopped for more than 24 hours.\n# -a option can clear the build cache and the intermediate image.\ndocker system prune -a --filter \"until = 24h\"\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#62-deleting-container","title":"6.2 Deleting container","text":"<p>Below commands only delete containers</p> <pre><code># remove a single container\ndocker rm &lt;container_id/name&gt;\n\n# remove multiple containers\ndocker rm container_id1 container_id2 \n\n# remove all stopped containers\ndocker container prune \n\n# when you run a container, you can add option -rm to delete the container when it exists.\ndocker run -rm image_id/name\n\n# show all container id as a list\ndocker ps -a -q\n\n# stop all container\ndocker stop $(docker ps -a -q)\n\n# remove all container\ndocker rm $(docker ps -a -q)\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#63-delete-container-image","title":"6.3 Delete container image","text":"<pre><code># delete a docker image\ndocker rmi image_name/id\n\n# delete multiple docker image\ndocker rmi image_id1 image_id2\n\n# remove image by using tag\ndocker rmi -f tag_name\n\n# remove all dangling image\ndocker image prune\n\n# remove all unused images(not linked to an existing container)\ndocerk image prune -a \n\n# remove all image\ndocker rmi $(docker images -a -q)\n</code></pre> <p>A dangling image just means that you've created the new build of the image, but it wasn't given a new name.  So the old images you have becomes the \"dangling image\". Those old image are the ones that are untagged and  displays \"\" on its name when you run docker images."},{"location":"container/Docker/02.Use_docker/#64-delete-container-volume","title":"6.4 Delete container volume","text":"<pre><code># delete one volume by using its name\ndocker volume rm volume_name\n\n# delete multiple volume\ndocker volume rm vol1 vol2\n\n# remove all unused volume\ndocker volume prune\n\n# add filter to remove\ndocker volume prune --filter \"label=test\"\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#65-delete-docker-networks","title":"6.5 Delete docker networks","text":"<pre><code>docker network rm network_name/id\n\ndocker network rm net1 net2\n\n# remove all unused network\ndocker network prune\n\n# add a filter \ndocker network prune --filter \"until=24h\"\n</code></pre>"},{"location":"container/Docker/02.Use_docker/#66-remove-docker-compose-deployment","title":"6.6 Remove docker compose deployment","text":"<p>The below command example removes containers, images, volumes, networks, and undefined containers.</p> <pre><code># --rmi all Remove all images\n# -v Remove the named volumes declared in the volumes section of docker-compose.yml and the anonymous volumes attached to the container\n# --remove-orphans Remove containers not defined in docker-compose.yml\ndocker-compose down --rmi all -v --remove-orphans\n</code></pre> <p>You can not a delete a volume in use, if you try to delete, an error message <code>volume is in use</code> will be printed</p>"},{"location":"container/Docker/03.Use_docker_compose/","title":"Docker compose","text":"<p>In previous tutorial, we have seen how to use docker to run single container. Imagine that we have a list of container  to run for one service, and they need to be coordinated. For this kind of situation, we can use docker compose.</p> <p>Docker compose is used to manage applications and increase efficiency in container development. Configurations are defined in a single YAML file, making applications easy to build and scale. </p> <p>In short, Docker Compose uses a single docker-compose.yml configuration file to create a list of services(i.e. containers).</p>"},{"location":"container/Docker/03.Use_docker_compose/#1-requirements","title":"1. Requirements","text":"<p>To run docker compose, you need both Docker Engine and Docker Compose binaries. There are two ways:</p> <ul> <li>Install standalone binaries of Docker Engine and Docker Compose.</li> <li>Install Docker Desktop, It contains the Development environment with graphical user interface              including Docker Engine and Docker Compose.</li> </ul>"},{"location":"container/Docker/03.Use_docker_compose/#2-installation","title":"2. Installation","text":"<p>Check the first tutorial 01.Install_docker_dockerCompose.md.</p>"},{"location":"container/Docker/03.Use_docker_compose/#3-important-terms","title":"3. Important terms","text":"<p>There are three important component in the docker-compose.yml file:  - services  - volumes  - networks</p> <p>A simple example of docker-compose.yml</p> <pre><code>version: \"3.7\"\nservices:\n  ...\nvolumes:\n  ...\nnetworks:\n  ...\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#31-services","title":"3.1 Services","text":"<p>services refer to the containers\u2019 configuration.</p> <p>For example, let\u2019s take a dockerized web application consisting of a front end, a back end, and a database.  We\u2019d likely split these components into three images, and define them as three different services in the configuration:</p> <pre><code>services:\n  frontend:\n    image: my-vue-app\n    ...\n  backend:\n    image: my-springboot-app\n    ...\n  db:\n    image: postgres\n    ...\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#32-volumes","title":"3.2 Volumes","text":"<p>Volumes, are physical areas of disk space shared between the host and a container, or even between containers.  In other words, a volume is a shared directory in the host, visible from some or all containers.</p>"},{"location":"container/Docker/03.Use_docker_compose/#33-networks","title":"3.3 Networks","text":"<p>Networks define the communication rules between containers, and between a container and the host. Common network  zones will make the containers\u2019 services discoverable by each other, while private zones will segregate them in virtual sandboxes.</p>"},{"location":"container/Docker/03.Use_docker_compose/#4-more-about-services","title":"4. More about services","text":"<p>A service contains the below parts: - image  - network - volume - Dependencies</p>"},{"location":"container/Docker/03.Use_docker_compose/#41-getting-the-image","title":"4.1 Getting the image","text":"<p>There are two possibility: - pull image from docker registry (e.g docker hub, etc.):  - build locally from a docker file</p>"},{"location":"container/Docker/03.Use_docker_compose/#pull-image-from-docker-registry","title":"Pull image from docker registry","text":"<p>You need to configure the docker registry url (docker hub by default). Then you need to specify the image name and tag.</p> <p>Below is an example, which pulls an image <code>ubuntu</code> with tag <code>latest</code></p> <pre><code>services:\n   my-service:\n      image: ubuntu:latest\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#build-image-from-docker-file","title":"Build image from docker file","text":"<p>We will use the keyword build, and a docker file.</p> <p>Below is an example where the docker file is hosted locally</p> <pre><code>services: \n  my-custom-app:\n    build: /path/to/dockerfile/\n    ...\n</code></pre> <p>The docker file can be hosted remotely too. Below is an example where the docker file is hosted on github</p> <pre><code>services: \n  my-custom-app:\n    build: https://github.com/my-repo/my-project.git\n    ...\n</code></pre> <p>If you want to share the build image with others, you can add another line <code>(image:&lt;image-name&gt;)</code> like in below example</p> <pre><code>services: \n  my-custom-app:\n    build: /path/to/dockerfile/\n    image: my-project-image\n    ...\n</code></pre> <p>This will create an image on your local image registry after the build process</p>"},{"location":"container/Docker/03.Use_docker_compose/#42-configuring-the-network","title":"4.2 Configuring the network","text":"<p>There are two types of communication: - comm between host and containers - comm between containers</p>"},{"location":"container/Docker/03.Use_docker_compose/#communication-between-host-and-containers","title":"Communication between host and containers","text":"<p>** To reach a container from the host, the ports must be exposed declaratively through the ports keyword**. It will  match the container exposed port with the host port. The first value is the host port, the second value is the  container exposing port</p> <p>In below example, we have three services: - helloworld: expose the container port 80, and match it with the host port 80 - myapp1: expose the container port 3000, and match it with the host port 8080 - myapp2: expose the container port 3000, and match it with the host port 8081</p> <p>So if you type  - localhost:80, you will reach the helloworld service - localhost:8080, you will reach the myapp1 service - localhost:8081, you will reach the myapp2 service</p> <pre><code>services:\n  network-example-service:\n    image: helloworld:latest\n    ports:\n      - \"80:80\"\n    ...\n  my-custom-app:\n    image: myapp1:latest\n    ports:\n      - \"8080:3000\"\n    ...\n  my-custom-app-replica:\n    image: myapp2:latest\n    ports:\n      - \"8081:3000\"\n    ...\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#communication-between-containers","title":"Communication between containers","text":"<p><code>Docker containers communicate between themselves in networks created, implicitly or explicily.</code> By default, all  containers in the same services share the same default network. A service can communicate with another service on  the same network by simply referencing it by using : (e.g. container1:80). We can expose a  container port by using the expose keyword. <p>The below example expose port 80 of the service app 1. Other services inside the same network can access it by using <code>app1:80</code></p> <pre><code>services:\n  network-example-service:\n    image: app1:latest\n    expose:\n      - \"80\"\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#custom-networks","title":"Custom networks","text":"<p>If we don't want to use the default network set up, we can use custom network configuration. We can use networks keyword to define virtual networks to segregate containers. In below example, we create two virtual network: - public-network - private-network</p> <p>The <code>pub-service1</code> and <code>pub-service2</code> are in the <code>public-network</code>. so they can communicate between them.  The <code>private-service</code> is the only container in the <code>private-network</code>, so it can't communicate with  <code>pub-service1</code> and <code>pub-service2</code></p> <pre><code>services:\n  pub-service1:\n    image: alpine:latest\n    networks: \n      - public-network\n    ...\n  pub-service2:\n    image: alpine:latest\n    networks: \n      - public-network\n    ...\n  private-service:\n    image: alpine:latest\n    networks: \n      - private-network\n    ...\nnetworks:\n  public-network: {}\n  private-network: {}\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#43-configure-volumes","title":"4.3 Configure Volumes","text":"<p>There are three types of volumes:  - anonymous - named - host</p> <p>Anonymous and named volumes are managed by the Docker engine, Docker will generate the directories and store data  in the host. These volumes are automatically mounted when the container is started. </p> <p>The good practice is to use the named volume. Below are the commands of named volume management</p> <pre><code># create the volume\ndocker volume create &lt;volume-name&gt;\n\n# list existing volume\ndocker volume list\n\n# mount volume on the container\ndocker run --name &lt;container-name&gt; -v &lt;volume-name&gt;:&lt;container-mount-path&gt; &lt;image-name&gt;\n\n# remove volume\ndocker volume remove &lt;volume-name&gt; \n</code></pre> <p>Host volumes allow us to specify an existing folder in the host and mount it with a specific path on the container.</p> <p>In the below example, we have two services: - app1: has three volumes, the two first volumes are <code>host volumes</code> which matches existing host directory with container         directory. The last one uses a <code>named volume</code>. You can also notice, we need to declare the <code>named volume</code>          first in the upper level volumes specs.</p> <p>To mount a volume in read-only mode by appending :ro to the volume declaration. For example <code>/home:/my-volumes/readonly-host-volume:ro</code>  specifies that the <code>/home</code> folder is read only. (we don\u2019t want a Docker container erasing our users by mistake).</p> <pre><code>services:\n  app1:\n    image: alpine:latest\n    volumes: \n      - /tmp:/my-volumes/host-volume\n      - /home:/my-volumes/readonly-host-volume:ro\n      - my-named-volume:/my-volumes/named-global-volume\n    ...\n  app2:\n    image: alpine:latest\n    volumes:\n      - my-named-volume:/another-path/the-same-named-global-volume\n    ...\nvolumes:\n  my-named-volume: \n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#44-container-dependencies","title":"4.4 Container dependencies","text":"<p>We need to create a <code>dependency chain between our services</code> so that some services get loaded before (and unloaded after)  other ones. We can achieve this result through the depends_on keyword:</p> <p>Below examples specifies that service kafka needs zookeeper to run first.</p> <pre><code>services:\n  kafka:\n    image: kafka\n    depends_on:\n      - zookeeper\n    ...\n  zookeeper:\n    image: zookeeper\n    ...\n</code></pre> <p>We should be aware, however, that Compose won\u2019t wait for the zookeeper service to finish loading before starting the  kafka service; it\u2019ll simply wait for it to start. If we need a service to be fully loaded before starting another service,  we need to get deeper control of the startup and shutdown order in Compose.</p>"},{"location":"container/Docker/03.Use_docker_compose/#5-managing-environment-variables","title":"5. Managing Environment Variables","text":"<p>Working with environment variables is easy in Compose. We can define static environment variables, as well  as dynamic variables, with the ${} notation:</p> <p>To define the environment values, we have the following approaches: 1. Compose file 2. Shell environment variables 3. Environment file 4. Dockerfile 5. Variable not defined.</p> <p>We can mix the above approaches, but let\u2019s keep in mind that Compose uses the priority order (1 has the highest order),  overwriting the value of less important approaches with the higher priorities approaches</p> <p>Once you have declared the <code>Environment Variables</code>, you can use them in your docker-compose file. Below is an example on how to use env var in the <code>docker-compose file</code>.</p> <pre><code>services:\n  database: \n    image: \"postgres:${POSTGRES_VERSION}\"\n    environment:\n      DB: mydb\n      USER: \"${USER}\"\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#51-declare-env-var-in-docker-compose-file","title":"5.1 Declare env var in docker compose file","text":"<p>You can set environment variables directly in your Compose file. This option has many limitation. The value is visible which makes it hard to version your compose file.</p> <pre><code>services:\n  webapp:\n    image: my-webapp-image\n    environment:\n      DB: mydb\n      USER: toto\n</code></pre> <p>You can also use the -e option in the docker run/compose command. For example</p> <pre><code>docker run -e \"[variable-name]=[new-value]\"\ndocker run -e \"DEBUG=1\"\n\ndocker compose -e \"[variable-name]=[new-value]\"\ndocker compose -e \"DEBUG=1\"\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#52-declare-env-var-in-shell-environment-variables","title":"5.2 Declare env var in Shell environment variables","text":"<pre><code># we declare the env var before calling the docker compose command\nexport POSTGRES_VERSION=alpine\nexport USER=foo\n\ndocker-compose -f docker-compose-file.yaml up\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#53-declare-env-var-in-environment-file","title":"5.3 Declare env var in Environment file","text":"<p>An .env file in Docker Compose is a <code>text file</code> used to define environment variables that should be made  available to Docker containers when running docker compose up. This file typically contains <code>key-value pairs of  environment variables</code>, and it allows you to centralize and manage configuration in one place. </p> <p>The .env file is the default method for setting environment variables in your containers. It is very useful if     you have multiple environment variables you need to store.</p> <p>The .env file should be placed at the root of the project directory next to your compose.yaml file.  For more information on formatting an environment file,  see Syntax for environment files.</p> <p>When you run <code>docker compose up</code>, all the env var inside the compose file will be replaced by the values of the  env file and generate the final config file. You can verify the generated config file by using below command:</p> <pre><code>docker compose config\n</code></pre> <p>For example If you define an environment variable <code>DEBUG=1</code> in your <code>.env file</code>, and your <code>compose.yml</code> file  looks like this:</p> <pre><code> services:\n    webapp:\n      image: my-webapp-image\n      environment:\n        - DEBUG=${DEBUG}\n</code></pre> <p>Docker Compose replaces ${DEBUG} with the value <code>1</code> from the <code>.env file</code>.</p>"},{"location":"container/Docker/03.Use_docker_compose/#multiple-env-file","title":"Multiple env file","text":"<p>You can use multiple .env files in your compose.yml with the env_file attribute, and <code>Docker Compose reads them  in the order specified</code>. If the same variable is defined in multiple files, the last definition takes precedence:</p> <pre><code>services:\n  webapp:\n    image: my-webapp-image\n    env_file:\n      - path: ./default.env\n        required: true # default value\n      - path: ./override.env\n        required: false # this env file is Optional\n</code></pre> <p>You can also use the <code>--env-file</code> to add custom env file while running the docker compose command</p>"},{"location":"container/Docker/03.Use_docker_compose/#54-declare-env-var-in-docker-file","title":"5.4 Declare env var in Docker file","text":"<p>You can define as many env var in the Docker file as want use ENV VAR1=$TEST1</p>"},{"location":"container/Docker/03.Use_docker_compose/#6-scaling-and-replicas","title":"6. Scaling and Replicas","text":"<p>The <code>docker-compose scale</code> command. Newer versions deprecated it, and replaced it with the scale option.</p> <p>Below is an example how to use Docker swarn(a cluster of Docker engines) to autoscale our containers.</p> <p>The deploy section is effective only when deploying to docker swarn.</p> <pre><code>services:\n  worker:\n    image: my-webapp-image\n    networks:\n      - frontend\n      - backend\n    deploy:\n      mode: replicated\n      replicas: 6\n      resources:\n        limits:\n          cpus: '0.50'\n          memory: 50M\n        reservations:\n          cpus: '0.25'\n          memory: 20M\n      ...\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#7-lifecycle-management","title":"7. Lifecycle management","text":"<p>The general docker compose command can be found Here</p> <p>Note the newer version, the command is no longer <code>docker-compose</code>, but <code>docker compose</code> </p> <p>The docker container lifecycle can be described as: - Create - Run - Pause - Stop - Delete</p> <p>The below image shows the commands to change the state of the container </p> <p></p>"},{"location":"container/Docker/03.Use_docker_compose/#71-servicelist-of-containers-creation","title":"7.1 Service(List of containers) creation","text":"<p>The <code>docker compose up</code> command builds, (re)creates, starts, and attaches to containers for a service.</p> <p>Unless they are already running, this command also starts any linked services.</p> <pre><code>docker compose [-f &lt;arg&gt;...] [options] [COMMAND] [ARGS...]\n\n# container creation, If the config file has a different name than the default one (docker-compose.yml), we must use\n# the option -f to specify the config file path\n# the -d option makes the compose process run in the background\ndocker compose -f &lt;docker-compose-file-path&gt; up -d\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#72-running-the-services","title":"7.2 Running the services","text":"<p>Starts existing containers for a service</p> <pre><code># run container, if the containers are already created \ndocker compose -f &lt;docker-compose-file-path&gt; start\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#73-pauseunpause-the-services","title":"7.3 Pause/Unpause the services","text":"<p>Pause/Unpause a running service</p> <pre><code>docker compose -f &lt;docker-compose-file-path&gt; pause/unpasue\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#74-stop-the-services","title":"7.4 Stop the services","text":"<p>There are different level of stop. </p> <pre><code># Stops running containers without removing them. They can be started again with docker compose start.\ndocker compose stop\n\n# Stops containers and removes containers, networks, volumes, and images created by up.\ndocker compose down\n</code></pre>"},{"location":"container/Docker/03.Use_docker_compose/#an-application-example","title":"An application example","text":"<p>You can follow this tutorial to have a first idea how a docker  compose service runs.</p> <p>You can find the source file in src/composetest</p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/","title":"Setup local helm chart registry","text":"<p>A local helm chart registry is essential when your k8s cluster does not have internet connection, or it can't use  <code>public helm chart repo</code> to pull the helm chart. </p> <p>In this tutorial, we will introduce two ways to set up a private helm chart registry - Harbor oci registry - chart museum </p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#1-use-harbor-oci-registry","title":"1. Use Harbor oci registry","text":"<p>Harbor provides an oci registry which can store, share helm charts. </p> <p>If you want to play with the oci registry in Harbor, you can read this Harbor_helm_chart_management.md </p> <p>But Onyxia does not support OCI registry. So we can not use harbor to manage helm chart.</p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#2use-chart-museum","title":"2.Use chart museum","text":"<p>chartmuseum is an open-source Helm Chart Repository server  written in Go (Golang), with support for various cloud storage backends.</p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#21-install-chart-museum","title":"2.1 Install Chart museum","text":"<p>You can find the official installation doc here. In this tutorial, I only focus on linux bare metal installation</p> <pre><code># get the installation script and run it\ncurl https://raw.githubusercontent.com/helm/chartmuseum/main/scripts/get-chartmuseum | bash\n\n# you should see below output, it means the chartmuseum binary is installed in /usr/local/bin\nDownloading https://get.helm.sh/chartmuseum-v0.15.0-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install chartmuseum into /usr/local/bin\nchartmuseum installed into /usr/local/bin/chartmuseum\n\n# check the version\nchartmuseum --version\n\n# get help\nchartmuseum --help\n</code></pre>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#22-configure-chartmuseum","title":"2.2 Configure chartmuseum","text":"<p>There are three ways to configure chartmuseum - command line options - env var - config file</p> <p>Here we use config file, because it's simpler to communicate how the chartmuseum is built. </p> <p>The options that can be used in the config file can be found in this file</p> <p>Below is a simple <code>config.yaml</code> to run a minimum instance for test. In linux os-system, it's recommended to but the config.yaml file in <code>/etc/chartmuseum</code>.</p> <pre><code>debug: true\nport: 8080\nstorage.backend: local\nstorage.local.rootdir: /data\nbasicauth.user: admin\nbasicauth.pass: changeMe\nauthanonymousget: true\ndepth: 0\n</code></pre> <pre><code># run the chartmuseum with the given config file\nchartmuseum --config /etc/chartmuseum/config.yaml\n\n# you can access the web interface\nhttp://ip:8080\n</code></pre> <p>You can use <code>Nginx as the reverse proxy</code> to protect the chart museum. For more information, please visit this doc</p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#23-upload-chart-to-chartmuseum","title":"2.3 Upload chart to chartMuseum","text":"<p>There are two ways to push charts to ChartMuseum: - via the <code>api of chartMuseum</code> - via helm cm-push plugin, the easiest way is to use helm cm-push plugin. You can find the official github page here </p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#231-upload-chart-via-the-api-of-chartmuseum","title":"2.3.1 Upload chart via the api of chartMuseum","text":"<pre><code># push a chart via the api of chartMuseum\ncurl -F \"chart=@hello-world-0.1.0.tgz\" https://chart.casd.local/api/charts\n\ncurl --data-binary \"@hello-world-0.1.0.tgz\" https://chart.casd.local/api/charts\n\n# If you\u2019ve signed your package and generated a provenance file, upload it with:\ncurl --data-binary \"@hello-world-0.1.0.tgz.prov\" http://chart.casd.local/api/prov\n\n# Or you can upload both at same time\ncurl -F \"chart=@hello-world-0.1.0.tgz\" -F \"prov=@hello-world-0.1.0.tgz.prov\" http://chart.casd.local/api/charts\n</code></pre> <p>The name of the .tgz will not impact the version of the chart, the chartMuseum will read the <code>Chart.yaml</code> in the  package to determine version. </p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#232-upload-chart-via-helm","title":"2.3.2 Upload chart via helm","text":"<pre><code># install the binary of helm push plugin\nhelm plugin install https://github.com/chartmuseum/helm-push\n\n# check the installed plugin\nhelm cm-push  --help\n\n# add your private chartmuseum as a repo\nhelm repo add --username admin --password changeMe cm https://chart.casd.local/\n\n# list all added repo\nhelm repo list\n\n# update the index of a repo\nhelm repo update\n\n# Search a chart on all the added repo with a give keyword\nhelm search repo &lt;keyword&gt;\n\n# if you want to use regex, you need to use option -r\nhelm search repo -r \".*\"\n\n# to further filter your result, you can add an grep after\nhelm search repo -r \"nginx\" | grep -i \"bitnami\"\n\n# push the chart, with the plugin, you don't need to do helm package anymore\n# you can push the directory directly, the plugin will package the chart, then push\nhelm cm-push hello-world/ cm\n\n# Push .tgz package is still supported\nhelm cm-push hello-world-0.1.0.tgz cm\n\n# push with a custom version\nhelm cm-push hello-world/ --version=\"0.2.0\" cm\n\n# If your ChartMuseum install is configured with ALLOW_OVERWRITE=true, chart versions will be automatically overwritten upon re-upload.\n# Otherwise, the upload will be denied with message file already exist. Unless your install is configured with DISABLE_FORCE_OVERWRITE=true (ChartMuseum &gt; v0.7.1), you can use the --force/-f option to to force an upload to overwrite an existing chart\nhelm cm-push --force hello-world-0.2.1.tgz chartmuseum\n\n# push without adding chart repo. Below example shows how to push to an repo directly\nhelm cm-push hello-world-0.2.1.tgz http://chart.casd.local/\n\n\n# Remove a repo\nhelm repo remove &lt;repo-name&gt;\n</code></pre> <p>note you need to run helm repo update to fetch the new index.yaml of each repo to get the latest uploaded chart</p>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#24-chartmuseum-authentication","title":"2.4 chartMuseum Authentication","text":""},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#basic-auth","title":"Basic auth","text":"<p>If the chartMuseum is installed with basic authentication enabled, you need to add user credential  when you add repo</p> <pre><code># option 1\nhelm repo add --username admin --password changeMe cm https://chart.casd.local/\n\n# option 2\n# The plugin will use the auth info located in ~/.config/helm/repositories.yaml\n\n# option 3\n# Use below env var\nexport HELM_REPO_USERNAME=\"myuser\"\nexport HELM_REPO_PASSWORD=\"mypass\"\n</code></pre>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#tls","title":"TLS","text":"<p>ChartMuseum uses the linux system ca-cert folder. If you use a self signed certificat, you can add the custom CA certificat on the server where you have installed the helm cm-push plugin.  </p> <p>If you don't have admin rights to do so, you can use below option to make changes on the plugin when adding repo</p> <ul> <li>--ca-file string:  Verify certificates of HTTPS-enabled servers using this CA bundle [$HELM_REPO_CA_FILE]</li> <li>--cert-file string:  Identify HTTPS client using this SSL certificate file [$HELM_REPO_CERT_FILE]</li> <li>--key-file string:   Identify HTTPS client using this SSL key file [$HELM_REPO_KEY_FILE]</li> <li>--insecure:          Connect to server with an insecure way by skipping certificate verification [$HELM_REPO_INSECURE]</li> </ul>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#appendix","title":"Appendix","text":""},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#set-up-a-systemd-daemon","title":"Set up a systemd daemon","text":"<p>To be able to run chartmuseum as a daemon, you can add the following file <code>/etc/systemd/system/chartmuseum.service</code> We recommend you to use the <code>config.yaml</code> to configure the chartmuseum daemon.</p> <pre><code>[Unit]\nDescription=chartmuseum\nDocumentation=Helm Chart Repository\nRequires=network-online.target\nAfter=network.target\n\n[Service]\nUser=root\nRestart=allways\nExecStart=/usr/local/bin/chartmuseum --config /etc/chartmuseum/config.yaml\nExecStop=/usr/local/bin/chartmuseum step-down\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>There is another way if you don't want to use the config.yaml, you can use $ARGS to specify the configuration. (Not recommended)</p> <pre><code>[Unit]\nDescription=chartmuseum\nDocumentation=Helm Chart Repository\nRequires=network-online.target\nAfter=network-online.target\n\n[Service]\nEnvironmentFile=/etc/chartmuseum/chartmuseum.config\nUser=root\nRestart=allways\nExecStart=/usr/local/bin/chartmuseum $ARGS\nExecStop=/usr/local/bin/chartmuseum step-down\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>The <code>chartmuseum.config</code> looks like</p> <pre><code>ARGS=\\\n--port=8080 \\\n--storage=\"local\" \\\n--storage-local-rootdir=\"/data\" \\\n--log-json \\\n--basic-auth-user=admin \\\n--basic-auth-pass=\"changeMe\" \\\n--auth-anonymous-get\n</code></pre>"},{"location":"container/Helm_chart_registry/01.Deploy_a_helm_chart_registry/#how-to-generate-a-provenance-file","title":"How to generate a provenance file ?","text":""},{"location":"container/Image_registry/04.Use_private_image_registry/","title":"Use private image registry with container runtime","text":"<p>Suppose we have a harbor instance runs at 192.168.0.5. The url of our Harbor instance  is (https://reg.casd.local) and it uses self-signed certificate or singed by a private CA.</p> <p>There are many container runtimes, in this tutorial here we only shows two: - Docker - Containerd</p>"},{"location":"container/Image_registry/04.Use_private_image_registry/#1-add-the-certificate-as-trusted-in-your-system-optional","title":"1. Add the certificate as trusted in your system. (Optional)","text":"<p>If the certificate which enables the https of your harbor is a self-signed certificate, you only need to copy the  certificate. If your certificate is signed by a CA, you need to copy the CA certificate. </p> <p>First add the certificate as accepted root ca in your system.</p> <pre><code># The debian distro only accepts pem or crt as valid certificate. If your certificate is in other format, you need to\n# convert it to the valid format.\ncp your-ca.crt /usr/local/share/ca-certificates/.\n\n# update the certificate cache\nsudo update-ca-certificates\n\n# test it with a site which uses the certificate or signed by the certificate\ncurl https://target-url\n\n# if the certificate is added correctly, you should not see error message\n</code></pre> <p>If you are admin of the Harbor server too, don't copy the private key in any case. </p>"},{"location":"container/Image_registry/04.Use_private_image_registry/#2-docker-client-use-private-image-registry","title":"2. Docker client use private image registry","text":"<p>There is two ways to connect a docker engine to a private image registry: - Add the certificate of the private image registry as trusted certificate. - Add the private image registry as the allowed insecure-registries (by default only localhost is allowed)</p>"},{"location":"container/Image_registry/04.Use_private_image_registry/#21-add-the-private-image-registry-as-the-allowed-insecure-registries","title":"2.1 Add the private image registry as the allowed insecure-registries","text":"<p>This solution is quite simple after you installed docker engine under debian, a directory /etc/docker should be created.</p> <pre><code># create a daemon.json file in /etc/docker\nsudo vim /etc/docker/daemon.json\n\n# put the below line in it, where reg.casd.local is the url of the private image registry. If the service runs on 80 or\n# 443, you don't need to specify the prot. If it runs on another port (e.g. 5000). You need to put \"reg.casd.local:5000\"\n{\n    \"insecure-registries\" : [ \"reg.casd.local\" ]\n}\n\n# update the docker daemon\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n</code></pre> <p>Now let's do some test. Here we recommend you to add the docker group to current user. Because docker login will create credential files and stores on your user home directory. And sudo will change the home directory.</p> <pre><code># add docker group to current user\nsudo usermod -aG docker $USER\n</code></pre> <pre><code># 1. login to the private registry\ndocker login reg.casd.local\n\n# 2. Download an image from docker hub\ndocker pull redis\n\n# 3. tag it to push to private registry, here reg.casd.local is the url. casd is the project name\ndocker tag redis reg.casd.local/casd/redis \n\n# 4. push the image to the private registry\ndocker push reg.casd.local/casd/redis\n\n# 5. pull image from the private registry\ndocker pull reg.casd.local/casd/redis\n</code></pre>"},{"location":"container/Image_registry/04.Use_private_image_registry/#22-add-the-certificate","title":"2.2 Add the certificate","text":"<p>As we mentioned before, the url of our Harbor instance is (https://reg.casd.local) and it uses a certificate signed by a private CA.</p> <p>In your server which runs the docker runtime, you can put all trusted CA inside this folder <code>/etc/docker/certs.d</code>.  For each registry, you need to create a sub-folder named with the <code>url or IP</code> of the image registry. For example, the url our image registry is <code>reg.casd.local</code>. So the folder should be like <code>/etc/docker/certs.d/reg.casd.local</code> Then you put the CA certificate in this folder. (Tested under debian)</p>"},{"location":"container/Image_registry/04.Use_private_image_registry/#23-add-the-private-repo-as-content-trustto-be-tested","title":"2.3 Add the private repo as content trust(To be tested)","text":"<pre><code>export DOCKER_CONTENT_TRUST=1\nexport DOCKER_CONTENT_TRUST_SERVER=https://reg.casd.local:4443\n</code></pre>"},{"location":"container/Image_registry/04.Use_private_image_registry/#k8s-integration","title":"K8s integration","text":"<p>Even thought the containerd daemon can pull/push images from the private registry, k8s deployment does not work directly we still need to add a secret to host the login password of the registry</p> <pre><code># general form\nkubectl create secret docker-registry &lt;secret-name&gt; \\\n--docker-server=&lt;your-registry-server-url&gt; \\\n--docker-username=&lt;your-name&gt; \\\n--docker-password=&lt;your-pword&gt; \\\n--docker-email=&lt;your-email&gt;\n\n# for example\nkubectl create secret docker-registry harbor-auth \\\n--docker-server=\"reg.casd.local\" \\\n--docker-email=pengfei.liu@casd.eu \\\n--docker-username='toto' \\\n--docker-password='changeMe' \n</code></pre> <p>In the <code>deployment.yaml</code> which uses images from the private registry, you need to add the <code>imagePullSecrets:</code> spec which specifies the credential to access the private registry.</p> <p>Below is an example, as the above secret creation example, the secret name is <code>harbor-auth</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mario\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mario\n  template:\n    metadata:\n      labels:\n        app: mario\n    spec:\n      containers:\n        - name: mario\n          image: reg.casd.local/casd/docker-supermario\n          ports:\n            - name: http\n              containerPort: 8080\n      imagePullSecrets:\n        - name: harbor-auth\n</code></pre>"},{"location":"container/Image_registry/Image_secu_inspection/","title":"Docker image security inspection","text":"<p>To ensure the docker image security, we will follow the below step: - If download from docker hub, check if it has the <code>official Badge</code>. - If build locally, choose well the <code>base image</code>(will less vulnerability) - Use docker image security scanner tools to identify static vulnerabilities - Use <code>docker bench</code>, <code>falco</code> to check run time vulnerabilities and anomalies  - Regularly Update and Rebuild Images - Issue documentation on docker images usage best practices  - Regularly audit your Docker images, container configurations, and deployment environments for compliance with security policies.</p>"},{"location":"container/Image_registry/Image_secu_inspection/#1-security-check-of-the-image-provided-by-docker-hub","title":"1. Security check of the image provided by docker hub","text":""},{"location":"container/Image_registry/Image_secu_inspection/#11-the-basic-status-of-an-imagedocker-hub","title":"1.1 The basic status of an image(docker hub)","text":"<p>Everyone can upload his local build docker image to docker hub. To distinguish with the homemade image, the official  supported image has an <code>official bage</code>. You can view it :  - from the docker hub website,  official images have a special \"official\" badge next to their name. This badge is            usually a blue ribbon icon or a label that says \"Official Image\".   - from docker client: You can use the below command</p> <p>For more information about DOI(docker official image), you can visit this page https://docs.docker.com/trusted-content/official-images/</p> <pre><code>docker search nginx\n\n# output example\nNAME                               DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nnginx                              Official build of Nginx.                        19946     [OK]       \nbitnami/nginx                      Bitnami container image for NGINX               189                  [OK]\n</code></pre> <p>Dockerhub has its own security controls on the official images. In general, we can trust the image with an <code>official bage</code> The <code>automated bage</code> means the image is built with an automated CI/CD pipeline. Bitnami is an organization which provide helm chart, it builds its own image for better suiting their helm chart.  </p>"},{"location":"container/Image_registry/Image_secu_inspection/#other-sign-of-official-images","title":"Other sign of Official images","text":"<ul> <li>Image Namespace: Official images typically reside in the root namespace, meaning they don\u2019t have a username             prefix. For example, the official image for <code>Nginx is just nginx, not username/nginx</code>.</li> <li>Description and Documentation: Official images have thorough documentation and a well-maintained description page.               They often include detailed usage instructions, environment variables, and configuration options.</li> </ul>"},{"location":"container/Image_registry/Image_secu_inspection/#2-check-the-docker-image-metadata","title":"2 Check the docker image metadata","text":"<p>Docker provide tools to inspect the docker image metadata. Below is the command example</p> <pre><code># general form\ndocker inspect &lt;image_name_or_id&gt;\n\n# inspect the nginx image\ndocker inspect nginx\n\n# the output is a json file, below is an output example\n[\n    {\n        \"Id\": \"sha256:6b1eed27cadeada9d1497f51c98c8e87d82753b7582ff5f94b4f9e6e1a6e2b7e\",\n        \"RepoTags\": [\n            \"nginx:latest\"\n        ],\n        \"RepoDigests\": [\n            \"nginx@sha256:4c6909e8f15c97b39b1d9151c5c48c8d4b70c8be94e89f6b6e3e2b53d5c3b18f\"\n        ],\n        \"Parent\": \"\",\n        \"Comment\": \"\",\n        \"Created\": \"2021-03-01T23:05:29.495312831Z\",\n        \"Container\": \"a8e6a8dcb9fbf7ab8d9b9e5e4f67f5a2d53e2b7e1a6b2b7b8a9e2c4d5e1b2e3d\",\n        \"ContainerConfig\": {\n            \"Hostname\": \"a8e6a8dcb9fb\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"80/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"nginx\",\n                \"-g\",\n                \"daemon off;\"\n            ],\n            \"Image\": \"sha256:b231e36b123b8c9c72b68d8e74f1c7a6b9b9f8c8d5b7e2b3e6b6f2b2e3d4a7f8\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {}\n        },\n</code></pre> <p>You need to pay attention on the below fields: - Id: The unique identifier of the image. - RepoTags: The tags associated with the image. - Created: The timestamp when the image was created. - DockerVersion: The version of Docker used to build the image. - Architecture: The CPU architecture the image is built for. - Os: The operating system the image is built for. - Size: The size of the image. - VirtualSize: The total size of the image, including its base layers. - Config: Configuration details of the image, including environment variables, exposed ports, commands, etc.</p>"},{"location":"container/Image_registry/Image_secu_inspection/#3-image-security-scanner-tool","title":"3. Image security scanner tool","text":""},{"location":"container/Image_registry/Image_secu_inspection/#31-trivy","title":"3.1 Trivy","text":"<p>You can visit their repo github.</p> <p>Trivy is a comprehensive and versatile security scanner. It can be applied on the following targets:</p> <ul> <li>Container Image</li> <li>Filesystem</li> <li>Git Repository (remote)</li> <li>Virtual Machine Image</li> <li>Kubernetes</li> <li>AWS</li> </ul> <p>what Trivy can detect on these target:</p> <ul> <li>OS packages and software dependencies in use (SBOM)</li> <li>Known vulnerabilities (CVEs)</li> <li>IaC issues and misconfigurations</li> <li>Sensitive information and secrets</li> <li>Software licenses</li> </ul> <pre><code># show the security scan of image nginx \ntrivy image --severity HIGH nginx\n\n# output example\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Library      \u2502 Vulnerability  \u2502 Severity \u2502    Status    \u2502    Installed Version    \u2502     Fixed Version      \u2502                            Title                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bash             \u2502 CVE-2022-3715  \u2502 HIGH     \u2502 affected     \u2502 5.1-2+deb11u1           \u2502                        \u2502 bash: a heap-buffer-overflow in valid_parameter_transform    \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2022-3715                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bsdutils         \u2502 CVE-2024-28085 \u2502          \u2502 fixed        \u2502 1:2.36.1-8+deb11u1      \u2502 2.36.1-8+deb11u2       \u2502 util-linux: CVE-2024-28085: wall: escape sequence injection  \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2024-28085                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 curl             \u2502 CVE-2022-42916 \u2502          \u2502 will_not_fix \u2502 7.74.0-1.3+deb11u3      \u2502                        \u2502 curl: HSTS bypass via IDN                                    \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2022-42916                   \u2502\n\u2502                  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u2502              \u2502                         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  \u2502 CVE-2022-43551 \u2502          \u2502              \u2502                         \u2502                        \u2502 curl: HSTS bypass via IDN                                    \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2022-43551                   \u2502\n\u2502                  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  \u2502 CVE-2023-27533 \u2502          \u2502 fixed        \u2502                         \u2502 7.74.0-1.3+deb11u8     \u2502 curl: TELNET option IAC injection                            \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2023-27533                   \u2502\n\u2502                  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u2502              \u2502                         \u2502                        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  \u2502 CVE-2023-27534 \u2502          \u2502              \u2502                         \u2502                        \u2502 curl: SFTP path ~ resolving discrepancy                      \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2023-27534                   \u2502\n\u2502                  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  \u2502 CVE-2024-2398  \u2502          \u2502 affected     \u2502                         \u2502                        \u2502 curl: HTTP/2 push headers memory-leak                        \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2024-2398                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u2502              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 e2fsprogs        \u2502 CVE-2022-1304  \u2502          \u2502              \u2502 1.46.2-2                \u2502                        \u2502 e2fsprogs: out-of-bounds read/write via crafted filesystem   \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2022-1304                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 libblkid1        \u2502 CVE-2024-28085 \u2502          \u2502 fixed        \u2502 2.36.1-8+deb11u1        \u2502 2.36.1-8+deb11u2       \u2502 util-linux: CVE-2024-28085: wall: escape sequence injection  \u2502\n\u2502                  \u2502                \u2502          \u2502              \u2502                         \u2502                        \u2502 https://avd.aquasec.com/nvd/cve-2024-28085                   \u2502\n</code></pre>"},{"location":"container/Image_registry/Image_secu_inspection/#cve-and-cvss","title":"CVE and CVSS","text":"<p>CVE: Common Vulnerabilities and Exposures (CVE) CVSS: Common Vulnerability Scoring System</p> <p>https://www.imperva.com/learn/application-security/cve-cvss-vulnerability/ https://fr.wikipedia.org/wiki/Common_Vulnerability_Scoring_System</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/","title":"Private container image registry","text":"<p>As our platform can't have internet access, so we can't use public image registry. As a result, we need to have private image registry.</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#which-one-is-the-best-for-us","title":"Which one is the best for us?","text":"<p>This artical compares many existing container image registry.</p> <p>For now, we choose Harbor as our beta test solution</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#docker-registry","title":"Docker registry","text":""},{"location":"container/Image_registry/Introduction_of_private_image_registry/#harbor","title":"Harbor","text":"<p>This article give a nice introduction about Harbor.</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#test-instance","title":"Test instance","text":"<p>A test instance has been installed by using this doc. The host machine is 10.50.6.62 with url https://reg.casd.local</p> <p>The login and password are in the keypass</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#appendix","title":"Appendix","text":""},{"location":"container/Image_registry/Introduction_of_private_image_registry/#1-what-is-oci-oci-imageartifactregistry","title":"1. What is OCI, OCI image/artifact/registry?","text":""},{"location":"container/Image_registry/Introduction_of_private_image_registry/#11-oci","title":"1.1 OCI","text":"<p>The OCI (Open Containers Initiative) manages a few specifications and projects related to the <code>storage, distribution, and execution of container images</code>.</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#12-oci-registry","title":"1.2 OCI registry","text":"<p>The OCI registry is used for storing and distributing <code>container images</code>. It's possible to use OCI registry to store other types of data. There are a couple techniques for doing this, and one of them is commonly referred as OCI Artifacts</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#13-oci-image-vs-docker-image","title":"1.3 OCI image VS Docker image","text":"<p>Docker image and OCI image are not exactly the same thing. Below example is an <code>Docker manifest</code></p> <pre><code>{\n    \"schemaVersion\": 2,\n    \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n    \"config\": {\n       \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n       \"size\":233,\n       \"digest\": \"sha256:12335wq34sdfasdfasdf93432440sdfsdfsdfs0sdfsdfs0fsdfsfsdfs\"\n    },\n    \"layers\": [\n        {\n            \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\"\n            \"size\":680,\n            \"digest\": \"sha256:12335wq34sdfasdfasdf93432440sdfsdfsdfs0sdfsdfs0fsdfsfsdfs\"\n        }\n    ]\n}\n</code></pre> <p>You can notice there are three <code>mediaType</code> configuration: - manifest level: \"application/vnd.docker.distribution.manifest.v2+json\" - config level: \"application/vnd.docker.container.image.v1+json\", - layer level: \"application/vnd.docker.image.rootfs.diff.tar.gzip\"</p> <p>You can notice they both have docker hardcoded in it. This is not acceptable for an OCI image manifest.</p> <p>Below example is an <code>OCI manifest</code></p> <pre><code>{\n    \"schemaVersion\": 2,\n    \"config\": {\n       \"mediaType\": \"application/vnd.oci.image.config.v1+json\",\n       \"size\":233,\n       \"digest\": \"sha256:12335wq34sdfasdfasdf93432440sdfsdfsdfs0sdfsdfs0fsdfsfsdfs\"\n    },\n    \"layers\": [\n        {\n            \"mediaType\": \"application/vnd.oci.image.layer.v1.tar+gzip\"\n            \"size\":680,\n            \"digest\": \"sha256:12335wq34sdfasdfasdf93432440sdfsdfsdfs0sdfsdfs0fsdfsfsdfs\"\n        }\n    ]\n}\n</code></pre> <p>You can notice that there are only two <code>mediaType</code> configuration: - config level: \"application/vnd.oci.image.config.v1+json\" - layer level: \"application/vnd.oci.image.layer.v1.tar+gzip\"</p> <p>The manifest level mediaType config are not supported in the OCI manifest. Docker still has it because it wants to keep retro-compatibility with older version.</p>"},{"location":"container/Image_registry/Introduction_of_private_image_registry/#14-oci-artifact","title":"1.4 OCI Artifact","text":"<p>The OCI artifact is a OCI manifest. But it will not be used to build an image. Below example is an OCI artifact</p> <pre><code>{\n    \"schemaVersion\": 2,\n    \"config\": {\n       \"mediaType\": \"application/vnd.mycustomartifact+json\",\n       \"size\":233,\n       \"digest\": \"sha256:12335wq34sdfasdfasdf93432440sdfsdfsdfs0sdfsdfs0fsdfsfsdfs\"\n    },\n    \"layers\": [\n        {\n            \"mediaType\": \"application/vnd.mycustomformat.tar+gzip\"\n            \"size\":680,\n            \"digest\": \"sha256:12335wq34sdfasdfasdf93432440sdfsdfsdfs0sdfsdfs0fsdfsfsdfs\"\n        }\n    ]\n}\n</code></pre> <p>You can notice the <code>two mediaType</code> (e.g. vnd.mycustomartifact+json; vnd.mycustomformat.tar+gzip) is customized to host custom file format. So this manifest will no longer produce an image.</p> <p>As a result, we can differ an OCI artifact from a OCI image manifest : - artifact sets a custom type in the <code>config.mediaType</code> field (unlike image manifest: vnd.oci.image.config.v1+json) - artifact is storee in a registry - artefact will not produce an image</p>"},{"location":"container/Image_registry/Trivy_image_scanner/","title":"Scan vulnerability of an container image","text":"<p>Trivy is an tool for detecting vulnerability of a target. The target can be: - Container Image - Filesystem - Git Repository (remote) - Virtual Machine Image - Kubernetes - AWS</p> <p>It can detect:</p> <ul> <li>OS packages and software dependencies in use (SBOM)</li> <li>Known vulnerabilities (CVEs)</li> <li>IaC issues and misconfigurations</li> <li>Sensitive information and secrets</li> <li>Software licenses</li> </ul> <p>For more information, you can visit their github </p>"},{"location":"container/Image_registry/Trivy_image_scanner/#install-trivy","title":"Install Trivy","text":"<p>For debian/ubuntu</p> <p>Run the following script</p> <pre><code>sudo apt-get install wget apt-transport-https gnupg lsb-release\nwget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/trivy.gpg &gt; /dev/null\necho \"deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main\" | sudo tee -a /etc/apt/sources.list.d/trivy.list\nsudo apt-get update\nsudo apt-get install trivy\n</code></pre>"},{"location":"container/Image_registry/Trivy_image_scanner/#use-trivy-to-analyse-an-image","title":"Use trivy to analyse an Image","text":"<p>By default, trivy use docker hub as the image registry</p> <pre><code># print all vulnerability\ntrivy image python:3.4-alpine\n\n# filter result by their severity\ntrivy image --severity HIGH python:3.4-alpine\n</code></pre>"},{"location":"container/Image_registry/Trivy_image_scanner/#use-trivy-to-analyse-various-sources","title":"Use trivy to analyse various sources","text":"<p>If you have sources such as <code>Dockerfile</code>, <code>terraform</code>, <code>k8s deployment</code></p> <pre><code># analyse source file under a folder\ntrivy fs --security-checks vuln,secret,config &lt;parent-folder&gt;\n\n# you can use the sample docker file in the resources/harbor/Trivy\ntrivy fs --security-checks vuln,secret,config resources/harbor/Trivy/python_ds/\n</code></pre>"},{"location":"container/Image_registry/Trivy_image_scanner/#integrate-trivy-into-harbor","title":"Integrate Trivy into Harbor","text":""},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/","title":"Install docker engine","text":"<p>You can find the official doc here.</p>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#remove-old-version","title":"Remove old version","text":"<p>Uninstall any such older versions before attempting to install a new version:</p> <pre><code>sudo apt-get remove docker docker-engine docker.io containerd runc\n</code></pre> <p><code>Images, containers, volumes, and networks</code> stored in /var/lib/docker/ aren\u2019t automatically removed when you uninstall Docker. If you want to start with a clean installation, and prefer to clean up any existing data, you need to remove them too.</p>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#install-via-apt-repository","title":"Install via apt repository","text":"<p>This procedure works for <code>Debian on x86_64 / amd64, armhf, arm64, and Raspbian</code>.</p>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#step1-set-up-prerequisites","title":"Step1. Set up prerequisites","text":"<pre><code>sudo apt-get update\n\nsudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n</code></pre>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#step2-add-gpg-key","title":"Step2. Add GPG Key","text":"<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n</code></pre>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#step3-set-up-repository","title":"Step3. Set up repository","text":"<pre><code>echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#step4-install-docker-engine","title":"Step4. Install Docker Engine","text":"<pre><code># update apt package index\nsudo apt-get update\n\n# install the latest version of docker engine, docker client and docker compose\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-compose\n</code></pre>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#step5-test-docker-engine","title":"Step5. Test Docker engine","text":"<p>Below command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.</p> <pre><code>sudo docker run hello-world\n</code></pre>"},{"location":"container/Image_registry/Docker/02.Docker_engine_installation/#step-5-post-install-config","title":"Step 5. Post install config","text":"<p>You have now successfully installed and started Docker Engine. The <code>docker user group</code> exists but contains no users, which is why you\u2019re required to use <code>sudo</code> to run Docker commands. If you want to allow <code>non-privileged users</code> to run Docker commands, you need to do following config.</p> <pre><code># check if the docker group exist or not\ncat /etc/group | grep -i \"docker\"\n\n# if you see below line, it's good\ndocker:x:996:\n\n# if not, you can create the group \nsudo groupadd docker\n\n# add current user to the group, you can replace $USER with any user id\nsudo usermod -aG docker $USER\n\n# now you need to logout and relogin to see the effect\n# If you\u2019re running Linux in a virtual machine, it may be necessary to \n# restart the virtual machine for changes to take effect.\n\n# relogin \nsu -l $USER\n\n# check your docker group\nid \n\n# you should see below output\nuid=1000(pliu) gid=1000(pliu) groups=1000(pliu),24(cdrom),25(floppy),29(audio),30(dip),44(video),46(plugdev),108(netdev),996(docker)\n\n# now you can run docker command without sudo\ndocker image list\n\n# You should see \nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    feb5d9fea6a5   13 months ago   13.3kB\n</code></pre>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/","title":"Docker common commands","text":""},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#delete-docker-objects","title":"Delete docker objects","text":"<p>After we created docker image, container, volume, network, we may need to delete them to clear the working space</p>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#purging-all-unused-or-dangling-resources","title":"Purging all unused or dangling resources","text":"<p>The first command is <code>system prune</code>, which will delete all unused Docker objects: - containers - images - networks - volumes</p> <p>Below are some command example</p> <pre><code># remove all unused objects\ndocker system prune\n\n# with the --filter option, we can filter which objects we want to delete.\n# the below example deletes containers that have been stopped for more than 24 hours.\n# -a option can clear the build cache and the intermediate image.\ndocker system prune -a --filter \"until = 24h\"\n</code></pre>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#deleting-container","title":"Deleting container","text":"<p>Below commands only delete containers</p> <pre><code># remove a single container\ndocker rm &lt;container_id/name&gt;\n\n# remove multiple containers\ndocker rm container_id1 container_id2 \n\n# remove all stopped containers\ndocker container prune \n\n# when you run a container, you can add option -rm to delete the container when it exists.\ndocker run -rm image_id/name\n\n# show all container id as a list\ndocker ps -a -q\n\n# stop all container\ndocker stop $(docker ps -a -q)\n\n# remove all container\ndocker rm $(docker ps -a -q)\n</code></pre>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#delete-container-image","title":"Delete container image","text":"<pre><code># delete a docker image\ndocker rmi image_name/id\n\n# delete multiple docker image\ndocker rmi image_id1 image_id2\n\n# remove image by using tag\ndocker rmi -f tag_name\n\n# remove all dangling image\ndocker image prune\n\n# remove all unused images(not linked to an existing container)\ndocerk image prune -a \n\n# remove all image\ndocker rmi $(docker images -a -q)\n</code></pre> <p>A dangling image just means that you've created the new build of the image, but it wasn't given a new name.  So the old images you have becomes the \"dangling image\". Those old image are the ones that are untagged and  displays \"\" on its name when you run docker images."},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#delete-container-volume","title":"Delete container volume","text":"<pre><code># delete one volume by using its name\ndocker volume rm volume_name\n\n# delete multiple volume\ndocker volume rm vol1 vol2\n\n# remove all unused volume\ndocker volume prune\n\n# add filter to remove\ndocker volume prune --filter \"label=test\"\n</code></pre>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#delete-docker-networks","title":"Delete docker networks","text":"<pre><code>docker network rm network_name/id\n\ndocker network rm net1 net2\n\n# remove all unused network\ndocker network prune\n\n# add a filter \ndocker network prune --filter \"until=24h\"\n</code></pre>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#remove-docker-compose-deployment","title":"Remove docker compose deployment","text":"<p>The below command example removes containers, images, volumes, networks, and undefined containers.</p> <pre><code># --rmi all Remove all images\n# -v Remove the named volumes declared in the volumes section of docker-compose.yml and the anonymous volumes attached to the container\n# --remove-orphans Remove containers not defined in docker-compose.yml\ndocker-compose down --rmi all -v --remove-orphans\n</code></pre> <p>You can not a delete a volume in use, if you try to delete, an error message <code>volume is in use</code> will be printed</p>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#docker-build","title":"Docker build","text":"<p>Docker build takes a docker file and build a docker image The <code>-t</code> option is recommended. </p> <pre><code># general form\ndocker build -t &lt;image-name&gt;:&lt;tag-name&gt; &lt;docker-file-path&gt;\n\n# We have write a docker file and config file in test_image\ntest_image/\n\u251c\u2500\u2500 config.sh\n\u2514\u2500\u2500 Dockerfile\n\n# an example\ndocker build -t my-img:0.0.1 ./test_image\n</code></pre> <p>The content of the <code>Dockerfile</code></p> <pre><code>FROM busybox:latest\nLABEL MAINTAINER=pengfei.liu@casd.eu\nLABEL version=\"1.0\"\nCOPY config.sh /etc/spark/config.sh\nRUN cat /etc/spark/config.sh      \n</code></pre> <p>The content of the <code>config.sh</code></p> <pre><code>export JAVA_HOME=/opt/java/java_8\n</code></pre> <p>You can find the full content of the docker file here</p> <p>Check the result</p> <pre><code>docker image ls\nREPOSITORY                            TAG                            IMAGE ID       CREATED          SIZE\nmy-img                                0.0.1                          b4ecf828f680   27 seconds ago   1.24MB\n</code></pre>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#docker-tag","title":"Docker tag","text":"<p>With the above command we tagged our image with <code>my-img:0.0.1</code>. If we want to push it to a remote repository, we need to change the tag</p> <pre><code># general form\ndocker tag old_name[:TAG] new_name[:TAG]\n</code></pre> <p>For different image registry, the tag convention is different. We will show two different example</p> <ul> <li>docker hub</li> <li>harbor </li> </ul>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#tag-and-push-the-local-image-to-docker-hub","title":"Tag and push the local image to docker hub","text":"<p>The general form for the docker hub tag is /:.  In below example, we will push the local image <code>my-img:0.0.1</code> to dockerhub <code>liupengfei99/test:v2</code> <p>The repo <code>test</code> must be created before push </p> <pre><code># change the tag for docker hub\ndocker tag my-img:0.0.1 liupengfei99/test:v2\n\n# check the result\ndocker images\n\n# output\nREPOSITORY                            TAG                            IMAGE ID       CREATED          SIZE\nmy-img                                0.0.1                          b4ecf828f680   13 minutes ago   1.24MB\nliupengfei99/test                     v2                             b4ecf828f680   13 minutes ago   1.24MB\n\n# login to docker hub\ndocker login\n\n# push the image\ndocker push liupengfei99/test:v2\n</code></pre> <p>In your docker hub web ui, you should see the newly pushed image</p>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#tag-and-push-the-local-image-to-harbor","title":"Tag and push the local image to Harbor","text":"<p>In below example, we will push the local image <code>my-img:0.0.1</code> to harbor <code>reg.casd.local/test/test-img:v1</code></p> <p>The general form for the harbor tag is //:.  <pre><code># change the tag for harbor\ndocker tag my-img:0.0.1 reg.casd.local/test/test-img\n\n# check the new tag\ndocker images\n\n# output\nREPOSITORY                            TAG                            IMAGE ID       CREATED          SIZE\nmy-img                                0.0.1                          b4ecf828f680   25 minutes ago   1.24MB\nliupengfei99/test                     v2                             b4ecf828f680   25 minutes ago   1.24MB\nreg.casd.local/test/test-img          latest                             b4ecf828f680   25 minutes ago   1.24MB\n\n\n# login to harbor\ndocker login reg.casd.local\n\n# push the image\ndocker push reg.casd.local/test/test-img\n</code></pre> <p>Now you can check your harbor web UI, in the <code>project test</code>, you should see the image <code>test-img</code>.</p>"},{"location":"container/Image_registry/Docker/03.Docker_common_commands/#docker-login","title":"Docker login","text":"<p>In the above command, we have used <code>docker login</code>. This will create create a file <code>~/.docker/config.json</code> which stores the user credential to connect to the remote server.</p> <pre><code>{\n        \"auths\": {\n                \"demo.goharbor.io\": {\n                        \"auth\": \"changeMe\"\n                },\n                \"https://index.docker.io/v1/\": {\n                        \"auth\": \"changeMe\"\n                },\n                \"reg.casd.local\": {\n                        \"auth\": \"changeMe\"\n                }\n        }\n}\n</code></pre> <p>Note the user credential is stored in plain text in the config.json. It's not recommended for production envrionment. Please use this doc to setup a secret store.</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/","title":"Install Harbor","text":"<p>The official installation doc can be found here</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#1-prepare-prerequisites","title":"1. Prepare prerequisites","text":"<p>You can find the complete requirement here</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#hardware","title":"Hardware","text":"<p>The following table lists the minimum and recommended hardware configurations for deploying Harbor.</p> <pre><code>Resource    Minimum Recommended\nCPU 2 CPU   4 CPU\nMem 4 GB    8 GB\nDisk    40 GB   160 GB\n</code></pre>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#software","title":"Software","text":"<p>The following table lists the software versions that must be installed on the target host.</p> <ul> <li>docker engine</li> <li>docker compose</li> <li>openssl</li> </ul> <p>To install docker engine and compose, you can follow this doc.</p> <pre><code># install openssl\nsudo apt install openssl\n</code></pre>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#2-download-the-harbor-installer","title":"2. Download the harbor Installer","text":"<p>The official release page is here. You can find two type of installer: - Online: The online installer downloads the Harbor images from Docker hub. For this reason, the installer is very small in size. - Offline: The offline installer contains pre-built images, so it is larger than the online installer. Use the offline installer if the host to which are deploying Harbor does not have a connection to the Internet. </p> <p>In this tutorial, we use the offline installer of harbor v2.6.1 (latest of 02/11/2022)</p> <pre><code># download the installer \nwget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz\n\n# unzip it\ntar -xzvf harbor-offline-installer-version.tgz\n\n# after unzip, you should see a folder harbor with below content\nharbor\n\u251c\u2500\u2500 common.sh\n\u251c\u2500\u2500 harbor.v2.6.1.tar.gz\n\u251c\u2500\u2500 harbor.yml.tmpl\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 prepare\n</code></pre> <ul> <li>harbor.yml.tmpl: is the config template</li> <li>prepare : is the preconfig script for setup https and required certficate</li> </ul>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#3-prepare-certificate","title":"3. Prepare certificate","text":"<p>If you don't have CA and client certs, you can follow the PKI_cfssl doc to generate them. </p> <p>If you already have them, you can put them in  - certificate folder of your harbor host: <code>/data/cert/</code>. In the <code>harbor.yml</code>, we will mount <code>/data</code> to the harbor container. - docker certificate folder <code>/etc/docker/certs.d/yourdomain.com/</code>. In our case, it should be <code>/etc/docker/certs.d/casd.local</code></p> <p>The Docker daemon interprets <code>.crt</code> files as <code>CA certificates</code> and <code>.cert</code> files as client certificates. So you may need to convert your client certificate from .crt to .cert format</p> <pre><code># convert client certificate format\n# in fact, the content is the same for the two format, so you can just rename it with .cert.\nopenssl x509 -inform PEM -in casd.local.crt -out casd.local.cert\n\n# copy them into harbor cert folder\ncp casd.local.crt /data/cert/\ncp casd.local.key /data/cert/\n\n# copy them into the docker cert folder\ncp casd.local.cert /etc/docker/certs.d/casd.local/\ncp casd.local.key /etc/docker/certs.d/casd.local/\ncp ca.crt /etc/docker/certs.d/casd.local/\n</code></pre>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#custom-port","title":"Custom port","text":"<p>If you mapped the default nginx <code>port 443 to a different port</code>, create the folder with the custom port</p> <pre><code># with a domain name\n/etc/docker/certs.d/yourdomain.com:port\n\n# or with an ip if you want to expose harbor with an IP\n/etc/docker/certs.d/harbor_IP:port.\n</code></pre> <p>You need to restart docker <code>systemctl restart docker</code> to make change effective.</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#4-configure-harbor-yaml-file","title":"4. Configure Harbor Yaml file","text":"<pre><code># use the template as the base of the config\ncp harbor.yml.tmpl harbor.yml.\n</code></pre> <p>You can find a complete explication about every attribute on this page. </p> <p>We recommend you to at least change the </p> <ul> <li><code>hostname</code></li> <li><code>https</code> with appropriate certificates</li> <li><code>admin password</code></li> <li><code>data_volume</code></li> </ul> <p>You can find an example in harbor.yaml</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#5-run-the-installer-script","title":"5. Run the installer script","text":"<p>Once you have configured <code>harbor.yml</code>, you can install and run Harbor by using <code>install.sh</code> script.</p> <p>By default, it only deploys Harbor, you can enable other modules with extra options - Notary : The module which can verify the origin of an image. More doc here</p> <ul> <li>Trivy : Vulnerabilites scanner of image. More doc here</li> <li>chartmuseum: an open source <code>helm chart repository server</code>. More doc here</li> </ul> <p>Notary and chartmuseum is deprecated since Harbor v2.7.0.</p> <pre><code># Without any extra module\nsudo ./install.sh\n\n# with all module\nsudo ./install.sh --with-trivy \n</code></pre> <p>This command will first generate all required manifest and config for docker compose in <code>/path/to/harbor/common/config</code>, then apply them with docker compose.</p> <p>--with-notary --with-chartmuseum option are deprecated, don't use them.</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#some-bug","title":"Some bug","text":"<p>There is some kind of bug with the current release that I'm unable to identify. Sometime when you start the harbor service, you can see the tool bar of a project and you can't use docker login to connet with harbor. </p> <p>To overcome this bug, you need to restart it</p> <pre><code># Restart Docker Engine.\n\nsudo systemctl restart docker\n\n# Stop Harbor. This command must run under the /path/to/harbor\ndocker compose down -v\n\n# start harbor. This command must run under the /path/to/harbor\ndocker compose up -d\n</code></pre> <p>You can find the official doc on harbor reconfigure here</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#6-working-with-harbor","title":"6. Working with harbor","text":"<p>https://goharbor.io/docs/1.10/working-with-projects/</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#61-create-a-project-in-harbor","title":"6.1 Create a project in Harbor","text":"<p>There are two types of project in Harbor:</p> <ul> <li>Public: Any user can pull images from this project. This is a convenient way for you to share repositories with others.</li> <li> <p>Private: Only users who are members of the project can pull images </p> </li> <li> <p>Go to Projects and click New Project.</p> </li> <li> <p>Provide a name for the project.</p> </li> <li> <p>(Optional) Check the Public check box to make the project public.</p> </li> </ul> <p>For more detail, please visit this page</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#62-config-a-project","title":"6.2 Config a project","text":"<p>Web User Interface</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#63-push-a-docker-image-to-the-created-project","title":"6.3 Push a docker image to the created project","text":"<p>Note if you want to push an image to harbor, you must tag the image in the local repo with below general form is <code>&lt;harbor-host-name&gt;/&lt;project-name&gt;/&lt;repo-name&gt;:&lt;tag&gt;</code>. tag is optional, if ommited, latest version will be used. </p> <p>For example, below is a minimum docker file. You can find the full example in sample_docker_file</p> <pre><code>FROM busybox:latest\nLABEL MAINTAINER=pengfei.liu@casd.eu\nLABEL version=\"1.0\"\nCOPY config.sh /etc/spark/config.sh\nRUN cat /etc/spark/config.sh        \n</code></pre> <pre><code># login to harbor registry\ndocker login &lt;harbor-url&gt;\n\n# Build an image from this Dockerfile and tag it.\n\ndocker build -t reg.casd.local/test/test-image .\n\n# Push the image from local repo to remote repo\ndocker push reg.casd.local/test/test-image\n</code></pre> <p>If you pull the image from other registry, you need to re-tag it to push to harbor. Below example shows how to pull image from dockerhub, then push the image to harbor</p> <pre><code># pull image from dockerhub\ndocker pull liupengfei99/mlflow\n\n# retag the image, the first argument is the source, second is the destination\ndocker tag liupengfei99/mlflow reg.casd.local/test/mlflow\n</code></pre> <p>For more example on how to push local image to remote repository, you can visit this page</p> <p>After this step, you should see a new repository <code>test-image</code> created in project <code>test</code></p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#64-pull-a-docker-image-from-harbor","title":"6.4 Pull a docker image from harbor","text":"<p>To pull an image from harbor via docker client, please follow below command</p> <pre><code># login to harbor registry\ndocker login &lt;harbor-url&gt;\n\n# pull the image from remote repo to local repo\ndocker pull reg.casd.local/test/test-image\n</code></pre>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#65-managing-labels","title":"6.5 Managing labels","text":""},{"location":"container/Image_registry/harbor/02.Harbor_installation/#global-level-label","title":"Global level label","text":"<p>The Harbor <code>system administrators</code> can list, create, update and delete the <code>global level labels</code> under <code>Administration-&gt;Configuration-&gt;Labels</code></p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#project-level-label","title":"Project level label","text":"<p>The <code>project administrators</code> and Harbor <code>system administrators</code> can list, create, update and delete the project level labels under <code>Labels</code> tab.</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#adding-and-removing-labels-to-and-from-images","title":"Adding and Removing Labels to and from Images","text":"<p>Users who have Harbor <code>system administrator, project administrator or project developer</code> role can click the <code>ADD LABELS</code> button to add labels to or remove labels from images. The label list contains both globel level labels(come first) and project level labels.</p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#66-tag-and-re-tag-image","title":"6.6 Tag and re-tag image","text":"<p>Harbor allows an image to have multiple tags. Open an image and click on <code>add a tag</code> button to  add a new tag. </p>"},{"location":"container/Image_registry/harbor/02.Harbor_installation/#retag-copy-to-another-project-with-new-tag","title":"Retag (copy to another project with new tag)","text":"<p>Harbor allows you to re-tag an image.</p> <p>For more information, please visit this page</p>"},{"location":"container/Image_registry/harbor/03.Harbor_helm_chart_management/","title":"Harbor helm chart management","text":"<p>Since version 1.6.0, Harbor allows users to manage helm chart.</p> <p>If you are not familliar with Helm chart, please read the helm_chart.md first.</p> <p>There are three options to push helm charts to Harbor</p> <ol> <li>Use the helm chartmuseum/helm-push plugin to push Helm chart to Harbor</li> <li>Use the Harbor web UI to upload and download the helm chart (a *.tgz file)</li> <li>Since version 3.8 Helm support pushing and pulling Charts from OCI compliant container registries such as Harbor.</li> </ol> <p>In this tutorial, we choose <code>option 3</code>, as Chartmuseum is already marked as deprecated in Harbor.</p>"},{"location":"container/Image_registry/harbor/03.Harbor_helm_chart_management/#link-helm-cli-with-harbor","title":"Link helm cli with harbor","text":"<p>As a helm chart repository, Harbor can work smoothly with Helm CLI. Run command <code>helm version</code> to make sure the version of Helm CLI is v3.8.1+.</p> <pre><code># check helm version\nhelm version\n\n# example output\nversion.BuildInfo{Version:\"v3.9.4\", GitCommit:\"dbc6d8e20fe1d58d50e6ed30f09a04a77e4c68db\", GitTreeState:\"clean\", GoVersion:\"go1.17.13\"}\n\n# helm oci registry config \nhelm registry login -u admin reg.casd.local\n</code></pre>"},{"location":"container/Image_registry/harbor/03.Harbor_helm_chart_management/#push-charts-to-repository-server","title":"Push charts to repository server","text":"<pre><code># general form for pushing helm chart to harbor\nhelm push &lt;char-package&gt; oci://&lt;harbor-url&gt;/&lt;project-name&gt;\n\n# an example of pushing chart\nhelm push hello-world-0.1.0.tgz oci://reg.casd.local/test\n</code></pre>"},{"location":"container/Image_registry/harbor/03.Harbor_helm_chart_management/#pull-and-install-charts-from-repository-server","title":"Pull and install charts from repository server","text":"<p>Below pulling command will pull the tgz file to your current directory. Unlike with the common helm command where you would first <code>add a repo</code> and then <code>pull from it</code>. With OCI registry, you can install a Chart with one line without adding the OCI registry repository(project) one by one.</p> <pre><code># general form for pulling\nhelm pull oci://&lt;harbor-url&gt;/&lt;project-name&gt;/&lt;chart-name&gt; --version &lt;chart-version&gt;\n\n# an example for pulling chart\nhelm pull oci://reg.casd.local/test/hello-world --version 0.1.0\n\n# general form for installing a release from a remote chart\nhelm install &lt;release-name&gt; oci://&lt;harbor-url&gt;/&lt;project-name&gt;/&lt;chart-name&gt; --version &lt;chart-version&gt;\n\n# an example for installing a release\nhelm install myrelease  oci://reg.casd.local/test/hello-world --version 0.1.0\n</code></pre>"},{"location":"container/Image_registry/harbor/03.Harbor_helm_chart_management/#extra-command-for-helm-to-interact-with-oci","title":"Extra command for helm to interact with oci","text":"<p>Helm also provides various other subcommands for the oci:// protocol. </p> <pre><code>helm pull\nhelm show\nhelm template\nhelm install\nhelm upgrade\n</code></pre> <p>As OCI registry does not have the notion of repository, you can't do a search on the repo to get all the helm chart list</p>"},{"location":"container/Image_registry/harbor/03.Harbor_helm_chart_management/#onyxia-api-does-not-support-oci-registry","title":"Onyxia API does not support OCI registry","text":""},{"location":"container/Image_registry/harbor/Harbor_export_import_image/","title":"Harbor export/import images","text":"<p>Harbor do not support export and import images from local file system. But we can use docker client to pull(export) or push(import) images to a harbor server </p>"},{"location":"container/Image_registry/harbor/Harbor_export_import_image/#export-an-image-from-harbor-server","title":"Export an image from Harbor server","text":"<pre><code># login to harbor registry\ndocker login &lt;harbor-url&gt;\n\n# pull the image from remote harbor repo to local docker\ndocker pull &lt;harbor-url&gt;/test/test-image\n\n# for example, if the url is reg.casd.local, then do\ndocker login reg.casd.local\ndocker pull reg.casd.local/test/test-image\n\n# check the image \ndocker image list | grep -i \"test-image\"\n\n# export the image to local file system as a tar file\ndocker save -o &lt;img-name&gt;.tar &lt;repo-name&gt;:&lt;tag-name&gt;\n\n# for example to exmport an nginx with tag 1.24.0-bullseye\ndocker save -o nginx.tar nginx:1.24.0-bullseye\n\n# copy the tar file to where you want\ncp nginx.tar ./to/destination\n</code></pre> <p>If your skip the tag name in the docker save step, you will never be able to get back this information again.</p>"},{"location":"container/Image_registry/harbor/Harbor_export_import_image/#import-an-image-into-a-harbor-server","title":"Import an image into a Harbor server","text":"<pre><code># get the tar file of the target image\ncp ./source/path/nginx.tar ./\n\n# load the image into current docker repo\ndocker load -i nginx.tar\n\n# check the image \ndocker image list | grep -i \"nginx\"\n\n# login to harbor registry\ndocker login &lt;harbor-url&gt;\n\n# tag the image with harbor registry path\ndocker tag nginx:1.24.0-bullseye &lt;harbor-url&gt;/&lt;project-name&gt;/nginx:1.24.0-bullseye\n\n# push the image from local docker repot to remote harbor repo\ndocker push &lt;harbor-url&gt;/&lt;project-name&gt;/nginx:1.24.0-bullseye\n</code></pre> <p>For more infor, read this page https://goharbor.io/docs/1.10/working-with-projects/working-with-images/pulling-pushing-images/</p>"},{"location":"container/k8s/01.Introduction/","title":"Introduction of k8s","text":"<p>Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of <code>containerized applications</code>.</p> <p>K8s is <code>Cloud Native application</code>, so it follows the 12 factor.</p> <p>Kubernetes takes care of service discovery, scaling, load balancing, self-healing, leader election, etc. therefore,  developers no longer have to build these services inside their application.</p> <p></p> <p>In the above figure, you can notice that K8s cluster contains two types of nodes:</p> <ul> <li>Master node:</li> <li>Worker node:</li> </ul>"},{"location":"container/k8s/01.Introduction/#master-nodes","title":"Master nodes","text":"<p>It is responsible for managing whole cluster. It monitors the health check of all nodes in the cluster.  It stores the members information regarding the different nodes, planning which containers are  schedules to which worker nodes, monitoring the containers and nodes, etc. So, when a worker node  failed, it moves the workload from failed node to another healthy worker node.  Kubernetes master is responsible for scheduling, provisioning, configuring and exposing API\u2019s to the client. So, all these done by a master node using the components called as control plane components.</p> <p>Four basic components of the master node(control plane):</p> <ul> <li>API server : is a centralized component where all the cluster components are communicated. Scheduler,                 controller manager and other worker node component communicate with the API server. Scheduler                 and controller manager request information to API server before taking any action.                  This API server exposes the Kubernetes API.</li> <li>Scheduler : is responsible for assigning your application to worker node. It will automatically detect which pod                to place on which node based on the resource requirements, hardware constraints and other factors. It                will smartly find out the optimum node which fulfills the requirements to run the application.</li> <li>Controller manager : maintains the cluster, it handles node failures, replicating components, maintaining the                        correct number of pods, etc. It constantly tries to keep system in desired state by comparing                        it with current state of system.</li> <li>etcd : is a data store that stores the cluster configuration. It is recommended that you have a back-up as it is           the source of truth for your cluster. And, if anything happened, you can restore all the cluster components           from this stored cluster configuration. Etcd is a distributed reliable key-value store where all the           configuration is stored in a documents and it\u2019s schema-less.</li> </ul>"},{"location":"container/k8s/01.Introduction/#worker-node","title":"Worker node","text":"<p>The Worker node are nothing but a virtual machine(VM\u2019s) running in cloud or on-prem, a physical server   running inside your data center. So, any hardware capable of running container runtime can become a worker node.   These nodes expose the underlying compute, storage and networking to the applications. They do the heavy-lifting of   application running inside the Kubernetes cluster. Together, these nodes form a cluster and run a workload assign to   them by master node component as same as manager assign a task to individual team member. This way we could   be able to achieve fault-tolerance and replication.</p> <p>Three basic components of the Worker Node(Data plane)</p> <ul> <li>Kubelet : runs and manages the containers on node, and it talks to API server. The scheduler will update the             spec.NodeName with respective worker nodes name and kubelet controller will get a notification from            API server, and it will then contact the container runtime like Docker for e.g. to go out and pull images            that requires to run the pod.</li> <li>Kube-proxy : load balances traffic between application components It is also called as service proxy                 which run on each node in the Kubernetes cluster. It will constantly look for                  new services and appropriately create the rules on each node to forward traffic to services to                 the back-end pods respectively. Container runtime: which runs the containers like Docker, rkt or containerd. Once you have the specification                  that describe your image for your application, the container runtime will pull the images and run                  the containers.</li> </ul>"},{"location":"container/k8s/01.Introduction/#basic-concepts","title":"Basic concepts","text":""},{"location":"container/k8s/01.Introduction/#pod-kubernetes-concept","title":"Pod (Kubernetes Concept)","text":"<p><code>A pod is the smallest deployable unit in Kubernetes</code>. It represents a group of one or more containers that are  scheduled and managed together.</p> <p>Pod Characteristics:  - Shared Environment: Containers within a pod share the same network namespace                            (they can communicate with each other using localhost) and storage volumes.   - Single IP Address: A pod gets a single IP address, and all containers within that pod share this IP.   - Multiple Containers: A pod can have multiple containers, usually working together as a single unit.                          For example, A main container (e.g., an NGINX web server) responsible for serving web content.                         A sidecar container (e.g., a logging agent like Fluentd) to handle logging.   - Lifecycle Management: Kubernetes handles the lifecycle of the pod, ensuring that the desired number of              replicas are running, restarting containers if needed, and ensuring the pod matches its defined state.    - High-Level Abstraction: A pod abstracts away container specifics, letting Kubernetes manage the                            complexity of container scheduling, scaling, networking, and storage.</p>"},{"location":"container/k8s/01.Introduction/#pod-sandbox","title":"Pod Sandbox","text":"<p>A pod sandbox sets up the environment for the containers, including network, storage, and DNS settings. Each pod is  associated with one sandbox. All containers within the same pod share the same sandbox, meaning  they share the same network namespace and IP address.</p> <p>The sandbox also isolates the resources that belong to a pod from others.</p>"},{"location":"container/k8s/01.Introduction/#sandbox-status","title":"Sandbox status","text":""},{"location":"container/k8s/01.Introduction/#container-containerd-concept","title":"Container (containerd Concept):","text":"<p>A container is a runtime instance of a containerized application (such as an individual Docker container).  It is a <code>single, isolated process</code> on the system with its own filesystem, networking, and process tree.</p> <p>Container Characteristics:  - Single Process Isolation: Containers are isolated environments, running a single application process.               Each container has its own filesystem, but by default, it is isolated from other containers.   - Managed by containerd: In Kubernetes, containerd is responsible for creating, starting, stopping,          and managing containers on each node. Each pod consists of one or more containers managed by containerd.   - Networking: A container has its own network namespace (unless it is part of a pod where containers                       share the same network).   - Single Unit: A container is usually thought of as a single unit of an application, typically mapped to             one image (e.g., an NGINX server running in a container).</p>"},{"location":"container/k8s/04.Managing_k8s_certificate/","title":"Certificate management with kubeadm","text":"<p>The official doc of the certificate management for v1.30 can be found here </p>"},{"location":"container/k8s/04.Managing_k8s_certificate/#default-behavior","title":"Default behavior","text":"<p>If you are doing nothing special before running <code>kubeadm init</code> in the master, all the certificates needed for a cluster to run are generated by kubeadm. The generated certificates and private keys are located in folder <code>/etc/kubernetes/pki/</code>.</p>"},{"location":"container/k8s/04.Managing_k8s_certificate/#use-custom-certificates","title":"Use custom certificates","text":"<p>You can also generate all the required certificates in a specific folder and tell kubeadm to use this folder as  the certificate root dir.</p> <p>The cert directory path can be specified by the <code>--cert-dir</code> flag or the <code>certificatesDir</code> field of  kubeadm's <code>ClusterConfiguration</code>. The default value is <code>/etc/kubernetes/pki/</code>.</p>"},{"location":"container/k8s/04.Managing_k8s_certificate/#use-custom-ca-recommended","title":"Use custom CA (Recommended).","text":"<p>The simplest way is to put your root CA certificate and private key in <code>/etc/kubernetes/pki/</code> before calling  <code>kubeadm init</code>. After calling <code>kubeadm init</code>, all the required certificate will be signed by this CA.</p> <pre><code># copy the ca cert and private key\ncp ca.crt /etc/kubernetes/pki/ca.crt\ncp ca.key /etc/kubernetes/pki/ca.key\n\n# run the kubeadm init\n</code></pre>"},{"location":"container/k8s/04.Managing_k8s_certificate/#renew-certificates-of-an-existing-cluster","title":"Renew certificates of an existing cluster","text":"<pre><code># step0: view the validity of the current certificates\nsudo kubeadm certs check-expiration\n\n# step1: backup existing certificates\nsudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki-backup\n\n# step2: copy your root ca certificate and private key to the below file\nsudo vim /etc/kubernetes/pki/ca.crt  \nsudo vim /etc/kubernetes/pki/ca.key\n\n# step3: CALL the renew command to renew the certificates for all services\nsudo kubeadm certs renew all\n</code></pre>"},{"location":"container/k8s/04.Managing_k8s_certificate/#manage-certificate-with-kubeadm-post-installation","title":"Manage certificate with kubeadm (post installation)","text":"<p>https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</p>"},{"location":"container/k8s/04.Managing_k8s_certificate/#best-practices","title":"best practices","text":"<p>https://v1-30.docs.kubernetes.io/docs/setup/best-practices/certificates/</p>"},{"location":"container/k8s/05.k8s_cluster_supervision/","title":"supervision tools","text":"<p>kubernetesui/dashboard kubernetesui/metric-scraper</p>"},{"location":"container/k8s/11.Copy_secret_between_name_space/","title":"11. Sync secret between name space","text":""},{"location":"container/k8s/11.Copy_secret_between_name_space/#111-without-tools","title":"11.1 Without tools","text":"<p>Export existing secret in yaml format</p> <pre><code>kubectl get secret &lt;secret-name&gt; -n &lt;namespace&gt; -o yaml\n</code></pre> <p>For example, you can see the below output, put it in a file <code>secret.yaml</code></p> <pre><code>apiVersion: v1\ndata:\n  password: dGVzdFBAc3N3b3Jk\n  username: dGVzdC11c2Vy\nkind: Secret\nmetadata:\n  creationTimestamp: \"2022-10-11T21:21:02Z\"\n  name: test-secret-1\n  namespace: testns1\n  resourceVersion: \"307939\"\n  uid: 6a8d9a6d-9648-4a39-a362-150e682c9a42\ntype: Opaque\n</code></pre> <p>The only attribute you need to change is namespace</p> <p>Change <code>namespace: testns1 to namespace: testns2</code></p> <p>Then apply the new secert</p> <pre><code>kubectl apply -f secret.yaml\n</code></pre>"},{"location":"container/k8s/11.Copy_secret_between_name_space/#112-with-tools","title":"11.2 With tools","text":"<p>There are three possilbe tools that can sync configMap/secret between namespaces</p> <ul> <li>config-syncer (aka. kubed) : A tuto of kubed</li> <li>reflector</li> <li>kubernetes-replicator </li> </ul> <p>No time to test them.</p>"},{"location":"container/k8s/Debug_application_in_cluster_k8s/","title":"Debug application in a cluster","text":""},{"location":"container/k8s/Debug_application_in_cluster_k8s/#identify-bug-level","title":"Identify bug level","text":"<p>One application needs three levels to work: - pod level - service level - ingress level</p> <p>We need to identify which level has problems</p>"},{"location":"container/k8s/Debug_application_in_cluster_k8s/#debug-a-pod-level-problem","title":"Debug a pod level problem","text":""},{"location":"container/k8s/Debug_application_in_cluster_k8s/#1-check-pod-status","title":"1. Check Pod Status","text":"<p>Each Pod has a status, we start by identifying its current status:</p> <pre><code># general form\nkubectl get pod &lt;pod-name&gt; -n &lt;name-space&gt; -o wide\n\n# an example\nkubectl get pod ingress-nginx-controller-qt489 -n ingress-nginx -o wide\n\n# output example\nNAME                                  READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES\nargocd-repo-server-57b9648b6c-g7dmx   1/1     Running   0          21m   192.168.14.203   onyxia-w01   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Look at <code>STATUS, RESTARTS, and NODE</code>.</p> <p>Typical statuses:</p> <ul> <li>Pending: can't be scheduled</li> <li>ContainerCreating: image pull, volume, or init issues</li> <li>CrashLoopBackOff: container repeatedly fails</li> <li>Error: container exited non-zero</li> </ul>"},{"location":"container/k8s/Debug_application_in_cluster_k8s/#2-inspect-events","title":"2. Inspect Events","text":"<pre><code>kubectl describe pod ingress-nginx-controller-qt489 -n ingress-nginx\n</code></pre> <p>Look at the \"Events\" section at the bottom, check if there are: - Volume mount errors - Image pull errors - Init container failures - Admission webhook issues - Node scheduling constraints</p>"},{"location":"container/k8s/Debug_application_in_cluster_k8s/#3-check-container-logs","title":"3. Check Container Logs","text":"<p>If the pod reached running state briefly:</p> <pre><code>kubectl logs ingress-nginx-controller-qcd  -n ingress-nginx\n</code></pre> <p>If the pod has multiple containers (e.g., controller + admission):</p> <pre><code>kubectl logs ingress-nginx-controller-qt489 -n ingress-nginx -c &lt;container-name&gt;\n</code></pre> <p>If restarting:</p> <pre><code>kubectl logs --previous ingress-nginx-controller-qt489 -n ingress-nginx\n</code></pre>"},{"location":"container/k8s/Debug_application_in_cluster_k8s/#4-check-events-on-node-if-stuck-in-pending","title":"4. Check Events on Node (if stuck in Pending)","text":"<pre><code>kubectl get events --sort-by='.metadata.creationTimestamp' -A | grep ingress-nginx\n</code></pre> <p>You might see: - Insufficient CPU/memory - Taints not tolerated - Node selector mismatch</p>"},{"location":"container/k8s/Debug_application_in_cluster_k8s/#5verify-volume-mounts","title":"5.Verify Volume Mounts","text":"<p>If you mounted the CA:</p> <p>Ensure secret exists:</p> <pre><code>kubectl get secret root-ca -n ingress-nginx -o yaml\n</code></pre> <p>Ensure volume and volumeMount path is correct. Check file permissions (readOnly: true + non-root user?).</p>"},{"location":"container/k8s/k8s_cluster_upgrade/","title":"Upgrade k8s cluster","text":"<p>The official doc</p> <p>https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p>"},{"location":"container/k8s/k8s_cluster_upgrade/#changing-the-k8s-package-repository","title":"Changing the k8s package repository","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/change-package-repository/</p>"},{"location":"container/k8s/k8s_cluster_upgrade/#update-repo-gpg-key","title":"update repo gpg key","text":"<p>When you update k8s package from the repo, you may encounter the below problems</p> <pre><code>Err:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease\n  The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\n\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease: The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\n\nW: Failed to fetch https://pkgs.k8s.io/core:/stable:/v1.31/deb/InRelease  The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\n\nW: Some index files failed to download. They have been ignored, or old ones used instead.\n</code></pre> <p>The problem is caused by the repo who has changed its pgp key. To fix it, you need to download the new gpg key.</p> <pre><code>sudo apt-get update\n# apt-transport-https may be a dummy package; if so, you can skip that package\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\n\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n</code></pre>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/","title":"Prepare environment for installing k8s offline","text":"<p>We need to prepare three things before we are able to install k8s offline - A system packages repo which provides below packages (e.g. kubeadm, containerd, crictl, runc, etc. ) - A container image repo which provides container images (e.g. kube-apiserver, kube-proxy, etc ) - A helm chart repo which provides chart to deploy services on k8s cluster (e.g. ingress-nginx).</p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#1-build-a-private-apt-package-repo","title":"1. Build a private apt package repo","text":"<p>As our scenario is offline installation, so we need to have a private debian package repo.</p> <p>For more details on how to build a private apt package repo, you can go to docs/debian_server/private_package_repo</p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#11-configure-all-vms-in-k8s-cluster-to-use-the-private-apt-repo-as-system-package-repo","title":"1.1 Configure all vms in k8s cluster to use the private apt repo as system package repo","text":"<p>Suppose we already have a debian package repo is built by using aptly. It has all the basic packages of  debian 11, k8s-main(kubeadm, kubelet, etc.), containerd, and docker. The url of this repo server is <code>deb.casd.local</code></p> <p>To configure vms to use it, follow the below steps</p> <pre><code># step1: add the repo in your source list of the target server\n# open the config file\nsudo vim /etc/apt/sources.list\n# comments the default config, below are some example\n# deb http://deb.debian.org/debian bullseye main\n# deb http://security.debian.org/debian-security bullseye-security main\n# deb http://deb.debian.org/debian bullseye-updates main\n\n# add the private repo, suppose the url is deb.casd.local. we don't want to use ssl\ndeb [trusted=yes] http://deb.casd.local/ bullseye main\n\n# if you want to enable ssl, you need to add the pgp key of the repo into your target server\nwget -qO- http://deb.casd.local/casd_gpg_key.asc | sudo tee /etc/apt/trusted.gpg.d/casd_gpg_key.asc\ndeb https://deb.casd.local/ bullseye main\n\n# step2: Update the package cache in the target server\nsudo apt update\n\n# step3: install the containerd package to test\nsudo apt install containerd.io\n</code></pre>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#2-container-image-registry","title":"2. Container image registry","text":"<p>As kubeadm can't pull image from the internet, we need to build a private image repo. For more details on how to build a private image repo, you can go to docs/container/Image_registry/harbor/02.Harbor_installation.md</p> <p>The official installation guide of harbor can be found here: https://goharbor.io/docs/2.12.0/install-config/</p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#21-get-the-required-container-image-in-image-registry","title":"2.1 Get the required container image in image registry","text":"<p>To init a cluster k8, we must have the required images in the image registry.</p> <p>The below list is the minimum container images that you need to deploy a k8s cluster: - k8s cluster required images:         - pause: Every Kubernetes Pod has a <code>pause container</code> that holds the network namespace and acts as the parent of all                    other containers in the Pod. Other containers in the Pod share its <code>PID, network, and IPC namespaces</code>.                   It ensures that networking and IPC resources remain stable even if app containers restart.         - kube-apiserver:         - kube-controller-manager         - kube-scheduler         - kube-proxy         - etcd: image to run etcd(a distributed database to store the k8s cluster stat and information).         - coredns - calico required images:         - kube-controllers         - node         - cni - ingress-nginx require images:         - controller </p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#211-mirror-k8s-cluster-required-images","title":"2.1.1 Mirror k8s cluster required images","text":"<p>You can get the complete image list that the k8s cluster(of a specific version) requires with the below command</p> <pre><code>k8s_version=1.31.1\nkubeadm config images list --kubernetes-version=$k8s_version\n\n# output example\nregistry.k8s.io/kube-apiserver:v1.31.1\nregistry.k8s.io/kube-controller-manager:v1.31.1\nregistry.k8s.io/kube-scheduler:v1.31.1\nregistry.k8s.io/kube-proxy:v1.31.1\nregistry.k8s.io/coredns/coredns:v1.11.3\nregistry.k8s.io/pause:3.10\nregistry.k8s.io/etcd:3.5.15-0\n</code></pre> <p>The official Kubernetes image registry on Google Container Registry (GCR) is called <code>registry.k8s.io</code> </p> <p>Below script (<code>k8s_img_sync.bash</code>) pull k8s images from official repo and pushes them to casd repo</p> <pre><code>#!/bin/bash\n# before running the script, make sure to adapt your config\nrepo_url=reg.casd.local\nproject_name=k8s_image\n\nimages=(\nregistry.k8s.io/kube-apiserver:v1.31.1\nregistry.k8s.io/kube-controller-manager:v1.31.1\nregistry.k8s.io/kube-scheduler:v1.31.1\nregistry.k8s.io/kube-proxy:v1.31.1\nregistry.k8s.io/coredns/coredns:v1.11.3\nregistry.k8s.io/pause:3.10\nregistry.k8s.io/etcd:3.5.15-0\n)\n\nfor image_name in ${images[@]} ; do\ndocker pull $image_name\ncasd_image_name=\"${repo_url}/${project_name}/${image_name#*/}\"\ndocker tag $image_name $casd_image_name\ndocker push $casd_image_name\ndone\n</code></pre>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#212-mirror-the-calico-required-images","title":"2.1.2 Mirror the calico required images","text":"<p>calico network addon handles all the virtual networks of the k8s cluster. The calico project git page  is here. You need to choose the version which fits better your k8s cluster.</p> <p>The calico service needs the below images to run - docker.io/calico/cni:<code>&lt;version&gt;</code> - docker.io/calico/node:<code>&lt;version&gt;</code> - docker.io/calico/kube-controllers:<code>&lt;version&gt;</code></p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#22-configure-all-vms-in-k8s-cluster-to-use-the-private-image-registry","title":"2.2 Configure all vms in k8s cluster to use the private image registry","text":"<p>We build a container image repo by using harbor. You need to configure the containerd daemon of all the servers in the k8s cluster to use the private image repo for pulling images.</p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#23-use-private-image-registry-to-init-k8s-cluster","title":"2.3 Use private image registry to init k8s cluster","text":"<p>You can find the official doc of kubeadm here.</p> <p>The complete list of configuration options for kubeadm init can be found here</p> <pre><code>kubeadmin init \\\n  --apiserver-advertise-address=192.168.32.128\\\n  --image-repository reg.casd.local/k8s_images\n  --control-plane-endpoint=k8s-master \\\n  --kubernetes-version v1.31.1\n</code></pre> <p>The default value of the image-repository is <code>k8s.gcr.io</code> for <code>kubeadmin</code>. So to user our private image registry, we  need to change the default value. </p> <p>We recommend you to use a <code>config.yaml</code> to encapsulate all k8s configurations </p>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#24-use-private-image-registry-to-init-calico","title":"2.4 Use private image registry to init calico","text":"<p>Below is an exampl of yaml file to deploy calico kube controllers. If you use the private image registry, you need to change the <code>docker.io/calico/kube-controllers:v3.25.1</code> to <code>reg.casd.local/calico/kube-controllers:v3.25.1</code>. If there is  authentication required, you need to add also <code>imagePullSecrets</code> specs.</p> <pre><code>---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n        - key: node-role.kubernetes.io/control-plane\n          effect: NoSchedule\n      serviceAccountName: calico-kube-controllers\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: docker.io/calico/kube-controllers:v3.25.1\n          imagePullPolicy: IfNotPresent\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          livenessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -l\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n            timeoutSeconds: 10\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n            periodSeconds: 10\n</code></pre>"},{"location":"container/k8s/deploy_k8s_offline/02.Prepare_env_for_installing_k8s_offline/#appendix-download-the-image-as-tar-files","title":"Appendix: Download the image as tar files","text":"<p>If you don't have an image registry, you can download the image and package it as a tar file.</p> <p>The below script <code>save_k8s_images.bash</code>, save the all images in the list as <code>.tar</code> file.</p> <pre><code>#!/bin/bash\n#change the output path to a dir where you want\nout_path=.\n\nimages=(\nregistry.k8s.io/kube-apiserver:v1.31.1\nregistry.k8s.io/kube-controller-manager:v1.31.1\nregistry.k8s.io/kube-scheduler:v1.31.1\nregistry.k8s.io/kube-proxy:v1.31.1\nregistry.k8s.io/coredns/coredns:v1.11.3\nregistry.k8s.io/pause:3.10\nregistry.k8s.io/etcd:3.5.15-0\n)\n\nfor image_name in ${images[@]} ; do\ndocker pull $image_name\ntar_name=\"${out_path}/${image_name##*/}.tar\"\ndocker save -o $tar_name $image_name\ndone\n</code></pre> <p>The image in .tar still has the origin repository tag. For example, the api server image still has  the tag <code>registry.k8s.io/kube-apiserver</code> </p> <p>You can use the below script to mirror the calico images</p> <p>Make sure the image version is compatible with the calico.yaml version. And make sure the calico you want to  install is compatible with the k8s cluster.</p> <pre><code>#!/bin/bash\n# before running the script, make sure to adapt your config\nrepo_url=reg.casd.local\nproject_name=calico\n\nimages=(\ndocker.io/calico/cni:v3.28.2\ndocker.io/calico/node:v3.28.2\ndocker.io/calico/kube-controllers:v3.28.2\n)\n\nfor image_name in \"${images[@]}\" ; do\ndocker pull \"${image_name}\"\ncasd_image_name=\"${repo_url}/${project_name}/${image_name#*/}\"\ndocker tag \"${image_name}\" \"${casd_image_name}\"\ndocker push \"${casd_image_name}\"\ndone\n</code></pre> <p>The below script download calico image as tar </p> <pre><code>#!/bin/bash\n#change the output path to a dir where you want\nout_path=.\n\nimages=(\ndocker.io/calico/cni:v3.28.2\ndocker.io/calico/node:v3.28.2\ndocker.io/calico/kube-controllers:v3.28.2\n)\n\nfor image_name in \"${images[@]}\" ; do\ndocker pull \"${image_name}\"\ntar_name=\"${out_path}/${image_name##*/}.tar\"\ndocker save -o \"${tar_name}\" \"${image_name}\"\ndone\n</code></pre>"},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/","title":"Install a cluster k8s without internet access","text":""},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#1-cluster-setup","title":"1.  Cluster setup","text":""},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#2-cluster-network-config","title":"2. Cluster Network config","text":""},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#3-diable-swap","title":"3. Diable swap","text":""},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#4-configure-firewall-rules","title":"4. Configure Firewall Rules","text":"<p>Step 1. to 4. do not require internet connection, so follow the doc of 01.Install_with_internet.md </p>"},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#5-install-containerd-run-times-on-all-nodes","title":"5. Install Containerd run times on all nodes","text":"<p>The containerd.io deb package should be in the local deb repo (deb.casd.local). So follow the doc  of 01.Install_with_internet.md. </p>"},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#6-setup-k8s-apt-repository","title":"6. Setup k8s apt repository","text":"<p>SKIP, no need</p>"},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#7-install-kubelet-kubectl-and-kubeadm-on-all-nodes","title":"7. Install kubelet, kubectl and kubeadm on all nodes","text":"<p>The kubelet kubeadm kubectl deb package should be in the local deb repo (deb.casd.local). So follow the doc  of 01.Install_with_internet.md. </p>"},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#8-create-kubernetes-cluster-with-kubeadm","title":"8. Create Kubernetes Cluster with Kubeadm","text":"<p>For <code>running kubeadm without an Internet connection</code> you have to pre-pull the required control-plane images.</p> <p>You can list and pull the images using the kubeadm config images sub-command:</p> <pre><code>kubeadm config images list\nkubeadm config images pull\n</code></pre> <p>You can pass --config to the above commands with a kubeadm configuration file to control the kubernetesVersion and imageRepository fields.</p> <p>All default registry.k8s.io images that kubeadm requires support multiple architectures.</p>"},{"location":"container/k8s/deploy_k8s_offline/03.Install_k8s_offline/#81-init-the-k8s-control-plane-master-node","title":"8.1 Init the k8s control plane (master node)","text":""},{"location":"container/k8s/deploy_k8s_with_kubeadm/01.Introduction_of_Kubeadm/","title":"What is Kubeadm?","text":"<p>Kubeadm is a tool to set up a minimum viable Kubernetes cluster without much complex configuration. Also, Kubeadm makes the whole process easy by running a series of prechecking to ensure that the server has all the essential components and configs to run Kubernetes.</p> <p>It is developed and maintained by the official Kubernetes community. There are other options like <code>minikube</code>, <code>kind</code>, etc., that are pretty easy to set up.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/01.Introduction_of_Kubeadm/#how-does-kubeadm-work","title":"How Does Kubeadm Work?","text":"<p>When you initialize a Kubernetes cluster using Kubeadm, it does the following.</p> <ol> <li>When you initialize kubeadm, first it runs all the preflight checks to validate the system state,      and it downloads all the <code>required cluster container images</code> from the <code>registry.k8s.io</code> container registry.</li> <li>It then generates required TLS certificates and stores them in the /etc/kubernetes/pki folder.</li> <li>Next, it generates all the kubeconfig files for the cluster components in the /etc/kubernetes folder.</li> <li>Then it starts the kubelet service and generates the static pod manifests for all the cluster components and saves it in the <code>/etc/kubernetes/manifests</code> folder.</li> <li>Next, it starts all the control plane components from the static pod manifests.</li> <li>Then it installs core DNS and Kubeproxy components</li> <li>Finally, it generates the node bootstrap token.</li> <li>Worker nodes use this token to join the control plane.</li> </ol> <p></p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/01.Introduction_of_Kubeadm/#kubeadm-port-requirements","title":"Kubeadm Port Requirements","text":"<p>Please refer to the following image and make sure all the ports are allowed for the control plane (master) and the worker nodes. If you are setting up the kubeadm cluster cloud servers, ensure you allow the ports in the firewall configuration.</p> <p></p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/","title":"Deploy a k8s cluster","text":"<p>This tutorial shows how to deploy a k8s cluster with internet access on debian servers.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#1-prerequisites","title":"1. Prerequisites","text":"<p>Suppose we have three servers with Debian 11 and below hardware: - 4 CPU / vCPU - 8 GB RAM - 20 GB free disk space - Sudo User with Admin rights - Stable Internet Connectivity (optional)</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#11-cluster-setup","title":"1.1 Cluster setup","text":"<ul> <li>Master Node (k8s-master) \u2013 10.50.5.67</li> <li>Worker Node 1 (k8s-worker1) \u2013 10.50.5.68</li> <li>Worker Node 2 (k8s-worker2) \u2013 10.50.5.69</li> </ul>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#12-cluster-network-config","title":"1.2 Cluster Network config","text":"<p>To enable the communication between Master node and worker node, we need to set up hostnames</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#121-change-server-hostname","title":"1.2.1: Change server hostname","text":"<pre><code># run this on master node\nsudo hostnamectl set-hostname k8s-master\n\n# run this on worker1\nsudo hostnamectl set-hostname k8s-worker1\n\n# run this on worker2\nsudo hostnamectl set-hostname k8s-worker2\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#122-list-server-hostnames-of-the-cluster-in-the-etchosts","title":"1.2.2: List server hostnames of the cluster in the <code>/etc/hosts</code>","text":"<p>Add the following lines into <code>/etc/hosts</code>:</p> <pre><code>10.50.5.67       k8s-master\n10.50.5.68       k8s-worker1\n10.50.5.69       k8s-worker2\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#123-check-connectivity","title":"1.2.3: Check connectivity","text":"<p>You can try to ping each workder node from the master and vice-versa</p> <pre><code># from master\nping k8s-worker1\nping k8s-worker2\n\n# from the worker\nping k8s-master\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#124-enable-ipv4-packet-forwarding","title":"1.2.4 Enable IPv4 packet forwarding","text":"<p>By default, the <code>Linux kernel does not allow IPv4 packets to be routed between interfaces</code>. Most Kubernetes  cluster networking implementations will change this setting (if needed), but some might expect the administrator  to do it for them. (Some might also expect other sysctl parameters to be set, kernel modules to be loaded, etc;  consult the documentation for your specific network implementation.</p> <p>We will provide the complete procedure on how to set up this in docs/Container/Containerd/02.Install_config_containerd.md</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#125-check-your-cgroup-drivers","title":"1.2.5 Check your cgroup drivers","text":"<p>On Linux, control groups are used to constrain resources that are allocated to processes.</p> <p>Both the <code>kubelet and the underlying container runtime</code> need to interface with <code>control groups</code> to enforce resource  management for pods and containers and set resources such as cpu/memory requests and limits.  <code>To interface with control groups, the kubelet and the container runtime need to use a cgroup driver</code>.  It's critical that the kubelet and the container runtime use the same cgroup driver and are configured the same.</p> <p>There are two cgroup drivers available:</p> <ul> <li>cgroupfs</li> <li>systemd</li> </ul> <p>In our case, as we use debian 11, the default cgroup is cgroup v2(Uses a unified hierarchy, improving resource  delegation and security), and the default cgroup driver is systemd. You can check the cgroup value with below  command</p> <pre><code>mount | grep cgroup\n\n# expected output\ncgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#126-disable-firewalls","title":"1.2.6 Disable firewalls","text":"<p>If you have firewalls installed on your severs, the easiest way is to disable them.</p> <pre><code>ufw disable\n\nsudo systemctl stop apparmor\nsudo systemctl disable apparmor\n</code></pre> <p>Or you can follow the below commands to set up specific rules</p> <pre><code># On Master node, run\n$ sudo ufw allow 6443/tcp\n$ sudo ufw allow 2379/tcp\n$ sudo ufw allow 2380/tcp\n$ sudo ufw allow 10250/tcp\n$ sudo ufw allow 10251/tcp\n$ sudo ufw allow 10252/tcp\n$ sudo ufw allow 10255/tcp\n$ sudo ufw reload\n</code></pre> <pre><code># On Worker Nodes,\n$ sudo ufw allow 10250/tcp\n$ sudo ufw allow 30000:32767/tcp\n$ sudo ufw reload\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#2-install-container-runtime","title":"2. Install container runtime","text":"<p>Containers require a <code>container runtime</code> to run on the host machine. As a result, we must install a container runtime  before deploying a k8s cluster.</p> <p>For now, <code>Containerd is the industry standard container run time</code>, we must install containerd on all master and worker nodes.</p> <p>Don't use the containerd binary of the native apt repo. Use the version of containerd.io</p> <p>The detailed installation guide is in docs/Container/Containerd/02.Install_config_containerd.md</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#3-diable-swap","title":"3. Diable swap","text":"<p>For kubelet to work smoothly, it is recommended to disable swap. Run following commands to turn off swap. This step can be skipped if your server does not have swap</p> <pre><code>$ sudo swapoff -a\n$ sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#4-install-k8s-packages","title":"4. Install k8s packages","text":"<p>The k8s releases are updated every 6 months. So the below docs maybe obsolete. The official doc can be found here https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management</p> <p>The below docs are tested on debian 11 server with k8s v1.33.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#41-setup-k8s-apt-repository","title":"4.1 Setup k8s apt repository","text":"<p>You need to set up k8s apt repository on all nodes</p> <pre><code># install required dependencies\nsudo apt install gnupg gnupg2 curl software-properties-common apt-transport-https -y\n\n# create the keyrings folder\nsudo mkdir -p /etc/apt/keyrings\n\n# add GPG key\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# allow unprivileged APT programs to read this keyring\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg \n\n# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\n# add v1.33 k8s repo to source.list \necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# helps tools such as command-not-found to work correctly\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   \n</code></pre> <p>In releases older than Debian 12 and Ubuntu 22.04, folder /etc/apt/keyrings does not exist by default, and it should be created before the curl command.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#7-install-kubelet-kubectl-and-kubeadm-on-all-nodes","title":"7. Install kubelet, kubectl and kubeadm on all nodes","text":"<p>Run the following apt commands on all the nodes to install Kubernetes cluster components like kubelet, kubectl and Kubeadm.</p> <pre><code>sudo apt update\n\n# you can check the available version before install\napt-cache madison kubeadm\n\n# expected output\nkubeadm | 1.33.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\nkubeadm | 1.33.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\nkubeadm | 1.33.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n\nsudo apt install kubelet kubeadm kubectl -y\n\n# check the installed version\napt list --installed kubeadm\n\n# hold is used to mark a package as held back, which will prevent the package from being automatically installed, upgraded or removed.\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#8-create-kubernetes-cluster-with-kubeadm","title":"8. Create Kubernetes Cluster with Kubeadm","text":"<p>Now, we need to use kubeadm to create a k8s cluster.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#81-use-custom-root-ca","title":"8.1 Use custom root CA","text":"<p>By default, the k8s cluster will generate a root CA for TLS communication. If you want to use a custom root CA certificate, you need to do the following steps:</p> <pre><code># Step1: put your custom CA files in:\n# The ca.crt is a valid X.509 root certificate\n/etc/kubernetes/pki/ca.crt\n# The ca.key is the corresponding private key (PEM format, unencrypted)\n/etc/kubernetes/pki/ca.key\n\n# Step2: Permissions Check\nchmod 600 /etc/kubernetes/pki/ca.key\nchown root:root /etc/kubernetes/pki/ca.key\n\n# step3: prepare the config (check 8.2)\n\n# step4: init the cluster with the config file\nkubeadm init --config kubeadm-config.yaml --upload-certs\n\n# step5: Optional, if certs were already created with a different CA and you want to re-sign:\nkubeadm certs renew all --config kubeadm-config.yaml\n</code></pre> <p>Do not pass --skip-phases or --certificate-key, unless managing every phase manually</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#82-init-the-k8s-control-plane-master-node","title":"8.2 Init the k8s control plane (master node)","text":"<p>Run the following command only from the master node, it will init the master node as a control plane endpoint</p> <pre><code># short version\nsudo kubeadm init\n\n# long version\nsudo kubeadm init --control-plane-endpoint=k8s-master --kubernetes-version v1.27.0\n\n# with a custom config file, all the configuration is in a file, it's better for versioning\nkubeadm init --config kubeadm-config.yaml --upload-certs\n</code></pre> <p>Below is an example of the <code>kubeadm-config.yaml</code></p> <pre><code>apiVersion: kubeadm.k8s.io/v1beta3\nkind: InitConfiguration\n# Configuration for the local kubelet.\nnodeRegistration:\n  # the hostname used for the node in the cluster.\n  name: onyxia-master\n  # The path to the container runtime interface socket (here: containerd).\n  criSocket: unix:///run/containerd/containerd.sock\n  # Forces kubelet to advertise the correct internal IP. Prevents wrong IP detection on multi-interface hosts.\n  kubeletExtraArgs:\n    node-ip: 10.50.5.67\n\nlocalAPIEndpoint:\n  # The IP address the kube-apiserver binds to on this node.\n  advertiseAddress: 10.50.5.67\n  # Port exposed for control-plane communication \n  bindPort: 6443\n\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\n# Target Kubernetes version to install \nkubernetesVersion: v1.33.2\n# DNS name or IP + port of the cluster\u2019s load balancer or primary API server. In single-node, it's the master's IP.\ncontrolPlaneEndpoint: \"10.50.5.67:6443\"\nnetworking:\n  # CIDR used by the CNI plugin \n  # REQUIRED to match Calico default\n  podSubnet: 192.168.0.0/16       \n  serviceSubnet: 10.96.0.0/12\n  dnsDomain: cluster.local\napiServer:\n  # Additional Subject Alternative Names added to the kube-apiserver certificate. Allows API to be reached via IP or DNS.\n  certSANs:\n    - \"10.50.5.67\"\n    - \"onyxia-master\"\n  # Configures the authorization model \n  extraArgs:\n    authorization-mode: Node,RBAC\n\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: iptables\n</code></pre> <p>You can notice the config file has three main parts: - InitConfiguration: It controls the local node's behavior - ClusterConfiguration: It defines the global cluster settings - KubeProxyConfiguration: It defines kube-proxy behavior</p> <p>I had a warning, because the api <code>v1beta3</code> is too old, kubeadm has a command to convert it to <code>v1beta4</code></p> <pre><code># Run this command to convert the config\nkubeadm config migrate --old-config kubeadm-config.yaml --new-config new-config.yaml\n</code></pre> <p>Below is the generated new config in <code>v1beta4</code></p> <pre><code>apiVersion: kubeadm.k8s.io/v1beta4\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 56mm95.5r9xhf31zr9r9gpm\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 10.50.5.67\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  imagePullSerial: true\n  kubeletExtraArgs:\n  - name: node-ip\n    value: 10.50.5.67\n  name: onyxia-master\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\ntimeouts:\n  controlPlaneComponentHealthCheck: 4m0s\n  discovery: 5m0s\n  etcdAPICall: 2m0s\n  kubeletHealthCheck: 4m0s\n  kubernetesAPICall: 1m0s\n  tlsBootstrap: 5m0s\n  upgradeManifests: 5m0s\n---\napiServer:\n  certSANs:\n  - 10.50.5.67\n  - onyxia-master\n  extraArgs:\n  - name: authorization-mode\n    value: Node,RBAC\napiVersion: kubeadm.k8s.io/v1beta4\ncaCertificateValidityPeriod: 87600h0m0s\ncertificateValidityPeriod: 8760h0m0s\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: 10.50.5.67:6443\ncontrollerManager: {}\ndns: {}\nencryptionAlgorithm: RSA-2048\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.k8s.io\nkind: ClusterConfiguration\nkubernetesVersion: v1.33.2\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 192.168.0.0/16\n  serviceSubnet: 10.96.0.0/12\nproxy: {}\nscheduler: {}\n</code></pre> <p>Now you have you needed to run <code>kubeadm init --config kubeadm-config.yaml --upload-certs</code> If everything works well, you should see the below output</p> <pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of control-plane nodes by copying certificate authorities\nand service account keys on each node and then running the following as root:\n\n  kubeadm join k8s-master:6443 --token 48elby.xre538l1ytebqe7q \\\n        --discovery-token-ca-cert-hash sha256:e0cccf7851ec76248163359058ea8e9aad478daefe14180c82f881a5e433dbda \\\n        --control-plane\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join k8s-master:6443 --token 48elby.xre538l1ytebqe7q \\\n        --discovery-token-ca-cert-hash sha256:e0cccf7851ec76248163359058ea8e9aad478daefe14180c82f881a5e433dbda\n</code></pre> <p>You can notice there are three commands : - setup kubectl (k8s client) to connect the k8s cluster (regular user and root user) - To join any number of master nodes to control plane - To join any number of worker nodes to the cluster</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#82-setup-kubectl","title":"8.2 Setup kubectl","text":"<p>To start interacting with cluster as a regular user, run following commands on master node</p> <pre><code>$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Run following kubectl command to get nodes and cluster information,</p> <pre><code>$ kubectl get nodes\n$ kubectl cluster-info\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#83-join-worker-node","title":"8.3 Join worker node","text":"<p>If you fogot the token to join the cluster, you can generate a new one with below command:</p> <pre><code># you need to have k8s admin right\nkubeadm token create --print-join-command\n\n# you should see below outputs\nkubeadm join 10.50.5.67:6443 --token 03mxyl.eg12gb36v3ya2bcs --discovery-token-ca-cert-hash sha256:1b19519e76812d286a93413320499bfd7ac1e06f7bd795994e086d0d1d0e6661\n\n# list existing token\nkubeadm token list\n\n# you should see below outputs\nTOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS\n03mxyl.eg12gb36v3ya2bcs   23h         2023-04-18T15:05:50Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token\n</code></pre> <p>You can notice the token has a TTL, so it expires in 23hours</p> <p>The general form of the join command is shown below:</p> <pre><code>kubeadm join &lt;api-server-ip:port&gt; --token &lt;token-value&gt; \\\n--discovery-token-ca-cert-hash sha256:&lt;hash value&gt;\n</code></pre> <p>So we need three information, </p> <ul> <li>k8s Api-server-ip and port</li> <li>a Valid token</li> <li>Token-ca-cert-hash value</li> </ul> <p>You can run below command to get the api server ip and port</p> <pre><code>kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}' &amp;&amp; echo \"\"\n</code></pre> <p>You need to have root privilege to run kubeadm join</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#84-check-your-k8s-status","title":"8.4 Check your k8s status","text":"<pre><code># get available nodes\nNAME            STATUS     ROLES           AGE     VERSION\nonyxia-master   NotReady   control-plane   5m43s   v1.33.2\nonyxia-w01      NotReady   &lt;none&gt;          20s     v1.33.2\nonyxia-w02      NotReady   &lt;none&gt;          9s      v1.33.2\n\n# get pods in name space\nkubectl get pods -n kube-system\n</code></pre> <p>If you see the nodes status are <code>Not-Ready</code>, you can check the pods status in kube-system with the below command.  <code>kubectl get pods -n kube-system</code>. If you see coredns is pending, it's normal. Because it requires a network addon to run. We will install it in the next section</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#9-reset-the-cluster","title":"9. Reset the cluster","text":"<p>https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#10-install-calico-pod-network-addon","title":"10. Install Calico Pod Network Addon","text":"<p>To enable communication between nodes and services in k8s cluster, we need a network addon.  In our case, we use Calico. The project page is here</p> <p>On the master node, run beneath command to install calico. Here I choose the current latest version v3.25.1. You can  visit the project page and get the latest version.</p> <pre><code># general form\nkubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/{calico-versioin}/manifests/calico.yaml\n\n# example\nkubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml\n</code></pre> <p>You need to check the compatibility between your k8s cluster version and the calico version.  For </p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#11-test-your-k8s-cluster","title":"11. Test your k8s cluster","text":"<p>To test Kubernetes cluster installation, let\u2019s try to deploy nginx-based application via deployment. Run beneath commands,</p> <pre><code># get general status of your cluster\nkubectl cluster-info\n\n# create a deployment with nginx image\nkubectl create deployment nginx-app --image=nginx --replicas 2\n\n# create a service which uses the deployment\nkubectl expose deployment nginx-app --name=nginx-web-svc --type NodePort --port 80 --target-port 80\n\n# get the pod info the deployment\nkubectl get pods -o wide\n\nNAME                         READY   STATUS    RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES\nnginx-app-7df7b66fb5-b6lhk   1/1     Running   0          83m   192.168.194.68   k8s-worker1   &lt;none&gt;           &lt;none&gt;\nnginx-app-7df7b66fb5-qtsw7   1/1     Running   0          83m   192.168.194.66   k8s-worker1   &lt;none&gt;           &lt;none&gt;\n\n# get the info of the service, you need to get the node port\nkubectl describe svc nginx-web-svc\n\n# the output\n\nName:                     nginx-web-svc\nNamespace:                default\nLabels:                   app=nginx-app\nAnnotations:              &lt;none&gt;\nSelector:                 app=nginx-app\nType:                     NodePort\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.103.208.99\nIPs:                      10.103.208.99\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               80/TCP\nNodePort:                 &lt;unset&gt;  31169/TCP\nEndpoints:                &lt;none&gt;\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n\n# with the above two commands, you know that the pods runs on `k8s-worker1`, the node port is 31169.\n# You can try to access the nginx service with the below command.\n# you need to modify the url and port based on the svc output\ncurl http://k8s-worker1:31169\n\n# clean the cluster after test\nkubectl delete deployment nginx-app\nkubectl delete service nginx-web-svc\n</code></pre> <p>If you get a HTML response from the nginx server with success, it means your k8s cluster is good.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#appendix1-cgroup-and-systemd","title":"Appendix1: cgroup and systemd","text":"<p><code>Systemd</code> organizes processes using <code>cgroups</code> to track and manage resource usage. Each systemd unit (like a service)  runs in its own cgroup. For example, the <code>nginx.service</code> runs in <code>/sys/fs/cgroup/system.slice/nginx.service/</code> This ensures services are isolated and can have specific resource limits.</p> <p>Systemd provides commands to control resource usage dynamically:</p> <pre><code># Limit CPU usage:\nsystemctl set-property nginx.service CPUQuota=50%\n\n# Limit Memory usage:\nsystemctl set-property nginx.service MemoryMax=500M\n</code></pre> <p>These settings are applied via cgroup controllers in the background.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#cgroupfs-driver","title":"cgroupfs driver","text":"<p>The cgroupfs driver is the default cgroup driver in the kubelet. When the cgroupfs driver is used, the kubelet  and the container runtime directly interface with the cgroup filesystem to configure cgroups.</p> <p>The cgroupfs driver is not recommended when systemd is the init system because systemd expects a single cgroup manager on the system. Additionally, if you use cgroup v2 , use the systemd cgroup driver instead of cgroupfs.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#systemd-cgroup-driver","title":"systemd cgroup driver","text":"<p>When systemd is chosen as the init system for a Linux distribution, the init process generates and consumes a root control group (cgroup) and acts as a cgroup manager.</p> <p>systemd has a tight integration with cgroups and allocates a cgroup per systemd unit. As a result, if you use systemd as the init system with the cgroupfs driver, the system gets two different cgroup managers.</p> <p>Two cgroup managers result in two views of the available and in-use resources in the system. In some cases, nodes that are configured to use cgroupfs for the kubelet and container runtime, but use systemd for the rest of the processes become unstable under resource pressure.</p> <p>The approach to mitigate this instability is to use systemd as the cgroup driver for the kubelet and the container runtime when systemd is the selected init system.</p> <p>To set <code>systemd</code> as the cgroup driver, edit the KubeletConfiguration option of cgroupDriver and set it to systemd. For example: </p> <pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n...\ncgroupDriver: systemd\n</code></pre> <p>If you configure systemd as the cgroup driver for the kubelet, you must also configure systemd as the cgroup driver  for the container runtime. Refer to the documentation for your container runtime for instructions. For example:</p> <ul> <li>containerd</li> <li>CRI-O</li> </ul> <p>Caution: Changing the cgroup driver of a Node that has joined a cluster is a sensitive operation. If the kubelet  has created Pods using the semantics of one cgroup driver, changing the container runtime to another cgroup  driver can cause errors when trying to re-create the Pod sandbox for such existing Pods.  Restarting the kubelet may not solve such errors.</p> <p>If you have automation that makes it feasible, replace the node with another using the updated configuration, or reinstall it using automation.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#appendix-install-containerd-via-default-apt-repo-not-recommended","title":"Appendix : Install containerd via default apt repo (not recommended)","text":"<p>You can find the offical intallation doc of containd here</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#configure-containerd","title":"Configure containerd","text":"<p>If you have problems with containerd, check this 02.Install_config_containerd.md</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/02.Deploy_k8s_cluster_online/#appendix-kubeadm-init-without-internet-access","title":"Appendix: Kubeadm init without internet access","text":"<p>Get all images that you need to pull</p> <pre><code>kubectl get pods --all-namespaces -o jsonpath=\"{.items[*].spec.containers[*].image}\" |\\\ntr -s '[[:space:]]' '\\n' |\\\nsort |\\\nuniq -c\n</code></pre> <p>You need to pull below image into local containerd cache - registry.k8s.io/coredns/coredns:v1.10.1 - registry.k8s.io/etcd:3.5.7-0 - registry.k8s.io/kube-apiserver:v1.27.0 - registry.k8s.io/kube-controller-manager:v1.27.0 - registry.k8s.io/kube-proxy:v1.27.0 - registry.k8s.io/kube-scheduler:v1.27.0</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/","title":"K8s cluster management","text":""},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#control-plane-management","title":"Control plane management","text":""},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#1-get-the-status-of-the-control-plane","title":"1. Get the status of the control plane","text":"<pre><code># use kubectl (deprecated)\nkubectl get componentstatuses\n\n# expected output\nNAME                 STATUS    MESSAGE             ERROR\nscheduler            Healthy   ok\ncontroller-manager   Healthy   ok\netcd-0               Healthy   {\"health\":\"true\"}\n\n# get pods status of the components\nkubectl get pods -n kube-system\n\n# expected output\ncalico-kube-controllers-6dd874f784-cmb99   1/1     Running            5768 (294d ago)     2y285d\ncalico-node-4bclg                          1/1     Running            3 (606d ago)        2y285d\ncalico-node-c2sds                          1/1     Running            5 (606d ago)        2y277d\ncalico-node-ccqqk                          1/1     Running            5 (606d ago)        2y282d\ncalico-node-khcv2                          1/1     Running            4 (606d ago)        2y285d\ncoredns-76b4fb4578-cbsr6                   1/1     Running            35 (572d ago)       2y277d\ncoredns-76b4fb4578-wcrr9                   1/1     Running            31 (572d ago)       2y277d\ndns-autoscaler-7979fb6659-z9gkw            1/1     Running            3 (606d ago)        2y285d\nkube-controller-manager-controlplane1      1/1     Running            63 (276d ago)       606d\nkube-proxy-bc9dw                           1/1     Running            0                   606d\nkube-proxy-hp947                           1/1     Running            0                   606d\nkube-proxy-nfwxv                           1/1     Running            0                   606d\nkube-proxy-xgqjc                           1/1     Running            0                   606d\nkube-scheduler-controlplane1               1/1     Running            62 (276d ago)       606d\nnginx-proxy-worker1                        1/1     Running            7 (606d ago)        606d\nnginx-proxy-worker2                        1/1     Running            3 (606d ago)        606d\nnginx-proxy-worker3                        1/1     Running            5 (606d ago)        606d\nnode-custom-setup-bsxt8                    1/1     Running            2 (606d ago)        2y220d\nnode-custom-setup-gp7tk                    1/1     Running            1 (606d ago)        2y220d\nnode-custom-setup-vkf66                    1/1     Running            1 (606d ago)        2y220d\nnodelocaldns-9sg7w                         1/1     Running            59 (572d ago)       2y281d\nnodelocaldns-dklqj                         1/1     Running            35 (573d ago)       2y277d\nnodelocaldns-lfk5v                         0/1     CrashLoopBackOff   101523 (245d ago)   606d\nnodelocaldns-pz7h6                         1/1     Running            57 (572d ago)       2y281d\n</code></pre> <p>The pod manifests of the control plane components are located in <code>/etc/kubernetes/manifests/</code></p> <pre><code>ls /etc/kubernetes/manifests/\n\n# expected output\nkube-apiserver.yaml\nkube-scheduler.yaml\nkube-controller-manager.yaml\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#11-etcd-status","title":"1.1 ETCD status","text":"<p>The etcd is launched as an external cluster, you should find the config in <code>/etc/kubernetes/kubeadm-config.yaml</code></p> <pre><code>etcd:\n  external:\n    endpoints:\n      - https://&lt;etcd-ip&gt;:2379\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#12-api-server-status","title":"1.2 API server status","text":"<pre><code>curl -k https://localhost:6443/healthz\ncurl -k https://localhost:6443/readyz\ncurl -k https://localhost:6443/livez\n\n# expected output\nok\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#stop-the-k8s-cluster","title":"Stop the k8s cluster","text":"<p>Step 1: Drain and cordon worker nodes (optional but safe)</p> <pre><code># Repeat for all worker nodes.\nkubectl drain &lt;worker-node-name&gt; --ignore-daemonsets --delete-emptydir-data\nkubectl cordon &lt;worker-node-name&gt;\n</code></pre> <p>This ensures that workloads are gracefully evicted and won't get rescheduled.</p> <p>Step 2: Stop control plane components (on the master)</p> <p>Move static pod manifests out of the kubelet watch path, kubelet will decommission the related pods.</p> <p>The below procedure only works on control plane that is deployed via <code>kubeadm</code>.</p> <pre><code>sudo mkdir -p /etc/kubernetes/manifests.bak\nsudo mv /etc/kubernetes/manifests/*.yaml /etc/kubernetes/manifests.bak/\n\n# get pods status of the components, they should be terminated\nkubectl get pods -n kube-system\n\n# if it does not work, you can try to shut the pod down manually, based on your container runtime, the commands are bit \n# different\n# for containerd\ncrictl ps -a | grep kube\n# or for Docker\ndocker ps -a | grep kube\n\n# Manual Kill (if you want to force stop)\n# for containerd\nsudo crictl ps | grep kube | awk '{print $1}' | xargs -r sudo crictl stop\n\n# for docker\nsudo docker ps | grep kube | awk '{print $1}' | xargs -r sudo docker stop\n</code></pre> <p>Kubelet will detect file removal and terminate the related pods: kube-apiserver, kube-controller-manager, kube-scheduler </p> <p>Step 3: Stop kubelet and container runtime (on all nodes)</p> <p>On all control plane and worker nodes:</p> <pre><code># \nsudo systemctl stop kubelet\n# stop container runtime depending on your setup\n# for containerd\nsudo systemctl stop containerd\n# or for Docker\nsudo systemctl stop docker     \n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#restart-the-cluster","title":"Restart the cluster","text":"<p>Step 1: Start kubelet and container runtime (on all nodes)</p> <p>On all control plane and worker nodes:</p> <pre><code># start container runtime depending on your setup\n# for containerd\nsudo systemctl start containerd\n# or for Docker\nsudo systemctl start docker     \n\n# start kubelet \nsudo systemctl start kubelet\n</code></pre> <p>Step 2: Restore control plane</p> <p>On the control plane node, restore static manifests:</p> <pre><code>sudo mv /etc/kubernetes/manifests.bak/*.yaml /etc/kubernetes/manifests/\n</code></pre> <p>Cluster should become available within 30\u201360 seconds.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/03.K8s_cluster_management/#destroy-the-cluster","title":"Destroy the cluster","text":"<p>On the control plane:</p> <pre><code>sudo kubeadm reset -f\n# This command stops and removes Kubernetes state\n# 1. Stops the kubelet process (indirectly, by removing configs)\n# Removes the local etcd data if it was part of the control plane\n# Deletes Kubernetes certificates, kubeconfig files, manifests, and state:\n- /etc/kubernetes/admin.conf\n- /etc/kubernetes/kubelet.conf\n- /etc/kubernetes/controller-manager.conf\n- /etc/kubernetes/scheduler.conf\n- /etc/kubernetes/pki/*\n- /etc/kubernetes/manifests/*\n\n# 2. It tries to revert changes made by kube-proxy and CNI plugins:\niptables -t nat -F\niptables -t mangle -F\niptables -F\niptables -X\n\n# 3. It attempts to remove:\n/var/lib/cni/\n/etc/cni/net.d/\n/var/lib/kubelet/\n\n\n# clean the credentials\nsudo rm -rf ~/.kube\n\nsudo systemctl stop kubelet\nsudo systemctl stop containerd \n\n# clean up the config and bin\nsudo rm -rf /etc/kubernetes\nsudo rm -rf /var/lib/etcd\nsudo rm -rf /var/lib/kubelet\nsudo rm -rf /etc/cni \nsudo rm -rf /var/lib/cni\n</code></pre> <p>On the workers:</p> <pre><code>sudo kubeadm reset -f\nsudo rm -rf /var/lib/kubelet /etc/kubernetes\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/","title":"Post k8s installation config","text":"<p>After k8s installation, you need to install other tools such as:   - helm   - reverse proxy</p> <p>I consider <code>cni plugin (e.g. calico, flannel, etc.)</code> is a part of basic component of k8s cluster.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#1-install-helm","title":"1. Install helm","text":"<p>The official release page of helm can be found here</p> <p>You can use the below bash script to install the latest version.</p> <pre><code>vim install_helm.bash\n\n# put the below content and run it\nbash install_helm.bash\n\n#!/bin/bash\n\nset -euo pipefail\n\nHELM_VERSION=\"v3.18.4\"\nHELM_TAR=\"helm-${HELM_VERSION}-linux-amd64.tar.gz\"\nHELM_URL=\"https://get.helm.sh/${HELM_TAR}\"\nTMP_DIR=\"/tmp/helm-install\"\nHELM_DIR=\"linux-amd64\"\n\nmkdir -p \"$TMP_DIR\"\ncd \"$TMP_DIR\"\n\necho \"Downloading Helm ${HELM_VERSION} to ${TMP_DIR}...\"\nwget -q \"${HELM_URL}\"\n\necho \"Extracting Helm...\"\ntar -xzf \"${HELM_TAR}\"\n\necho \"Setting execution permission...\"\nchmod a+x \"${HELM_DIR}/helm\"\n\necho \"Moving Helm binary to /usr/local/bin...\"\nsudo mv \"${HELM_DIR}/helm\" /usr/local/bin/helm\n\necho \"Cleaning up...\"\nrm -rf \"$TMP_DIR\"\n\necho \"Verifying Helm installation...\"\nif command -v helm &gt;/dev/null 2&gt;&amp;1; then\n    helm version\n    echo \"Helm installed successfully.\"\nelse\n    echo \"Helm installation failed.\"\n    exit 1\nfi\n</code></pre> <p>You should see the helm version and the success message. You need to change version and target system architecture if you use another OS other than debian 11.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#2-set-up-a-reverse-proxy","title":"2. Set up a reverse proxy","text":"<p>A <code>reverse proxy</code> is essential for a k8s cluster. Otherwise, the applications deployed in the cluster are not accessible from  the outside world. There are many possible reverse proxy solutions such as: - Kong (commercial alternative): https://konghq.com/ - Traefik (commercial alternative): https://traefik.io/</p> <p>In this tutorial, we choose ingress-nginx.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#21-the-ingress-nginx-controller-mode","title":"2.1 The ingress nginx controller mode","text":"<p>There are three modes to set up the proxy and reverse proxy for a k8s cluster: - host - load balancer - nodePort</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#211-host-mode","title":"2.1.1 Host mode","text":"<p>In host mode, the <code>Ingress controller</code> it uses the host's network namespace. This means that the Ingress  controller binds directly to the host's network interfaces and ports. </p> <p>The advantage of the host mode is that it can achieve higher performance compared to other modes,  as it eliminates the overhead of the kube-proxy layer.</p> <p>The disadvantage is that you cannot run multiple instances of the Ingress controller on the same host with the  same ports, as there would be port conflicts.</p> <p>To view the detailed configuration of host mode, check the section of <code>hostNetwork: true</code> section in the <code>values.yaml</code>  template.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#212-load-balancer-mode","title":"2.1.2 Load balancer mode","text":"<p>In the load balancer mode, the Ingress controller typically runs as a service, and an <code>external load balancer</code> (normally provided by the cloud provider) is provisioned to distribute incoming traffic to the Ingress controller service.</p> <p>This mode is suitable for cloud environments where a load balancer service can be provisioned dynamically (e.g., AWS ELB, GCP Load Balancer). The external load balancer takes care of distributing traffic to the nodes running the Ingress controller service.</p> <p>To view the detailed configuration, check the section of <code>appProtocol:True</code> section</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#213-nodeport-mode","title":"2.1.3 nodePort mode","text":"<p>In NodePort mode, the Ingress controller service is exposed on a static port on each node in the cluster.  This port is accessible from outside the cluster, and the traffic is then forwarded to the Ingress controller.</p> <p>This mode is often used in on-premises or bare-metal environments where cloud load balancers are not available or  in development/testing scenarios.</p> <p>While it provides external access, it might not be as suitable for production environments due to potential  challenges in scaling and managing external access.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#3-a-real-example","title":"3. A real example","text":"<p>In this example, we choose the host mode. So the <code>ingress-nginx</code> listens to the network interface of the host server. As a result, only one ingress nginx pod can be deployed on each node. And we don't want to have <code>more than one ingress</code>.  So we added a <code>node selector</code> on the <code>ingress nginx controller service is only deployed on a specific node</code>.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#31-select-which-node-to-deploy-the-ingress-nginx-controller","title":"3.1 Select which node to deploy the ingress nginx controller","text":"<p>To deploy <code>ingress nginx controller service on a specific node</code>: - Label a worker node with a specific label (e.g. ingress-node) - Add a <code>node selector</code> on the ingress nginx controller service</p> <p>We label only one node, because we need to set up a dns resolver entry, so <code>all the incoming querier can be redirected to the node which contains the ingress controller</code>. </p> <p>Even we have two pods of Ingress controller. The second one that is not in the DNS will never be used. </p> <pre><code># our k8s cluster \n- Master Node (k8s-master) \u2013 10.50.5.67\n- Worker Node 1 (k8s-worker1) \u2013 10.50.5.68\n- Worker Node 2 (k8s-worker2) \u2013 10.50.5.69\n\n# Here we choose worker 1 to host ingress\n# FQDN for k8s Ingress controller\n10.50.5.68   *.casd.local\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#32-label-the-node","title":"3.2 Label the node","text":"<p>The below commands show you how to label a node with a specific label</p> <pre><code># get all available nodes\nkubectl get nodes\n\n# output example\nNAME            STATUS   ROLES           AGE   VERSION\nonyxia-master   Ready    control-plane   19h   v1.33.2\nonyxia-w01      Ready    &lt;none&gt;          19h   v1.33.2\nonyxia-w02      Ready    &lt;none&gt;          19h   v1.33.2\n\n# general form to label a node, \nkubectl label node &lt;nodename&gt; &lt;label-key&gt;=&lt;label-value&gt;\n\n# example\nkubectl label node onyxia-w01 ingress-node=true\n\n# to un-label a node, you can use the below command\nkubectl label node &lt;nodename&gt; &lt;labelname&gt;-\n\n# example\nkubectl label node worker2 public-\n\n# after labeling, you will see new pod of nginx gets created.\nkubectl get all -n ingress-nginx -w\nkubectl get pods -n ingress-nginx -w\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#33-deploy-the-ingress-nginx-controller-service","title":"3.3 Deploy the ingress nginx controller service","text":"<p>You can use the below commands to deploy the ingress nginx controller service</p> <pre><code># we want it to run is the namespace ingress-nginx, so we create a namespace\nkubectl create namespace ingress-nginx\n\n# add ingress-nginx helm repo\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n\n# update the repo content\nhelm repo update\n\n# list available release \nhelm search repo\n\n# output example\nNAME                            CHART VERSION   APP VERSION     DESCRIPTION\ningress-nginx/ingress-nginx     4.13.0          1.13.0          Ingress controller for Kubernetes using NGINX a\n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#331-configure-the-ingress-nginx-controller","title":"3.3.1 Configure the ingress-nginx controller","text":"<p>With the above ingress-nginx repo, we can install an <code>ingress-nginx controller</code> service in our cluster.</p> <p>You can find the full doc https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/</p> <p>You can find the values.yaml template here https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#332-a-minimum-config-example","title":"3.3.2 A minimum config example","text":"<p>Note the below ingress_values.yaml is an example of how our cluster configures the ingress-nginx controller. </p> <pre><code>controller:\n  watchIngressWithoutClass: true\n  allowSnippetAnnotations: false\n  config:\n    error-log-level: \"info\"\n    ignore-invalid-headers: \"false\"\n    proxy-request-buffering: \"off\"\n    proxy-body-size: \"0\"\n    large-client-header-buffers: \"4 16k\"\n\n  hostNetwork: true\n  extraEnvs:\n    - name: MY_POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n  kind: DaemonSet\n  service:\n    enabled: true\n    type: ClusterIP\n  ingressClassResource:\n    name: nginx\n    enabled: true\n    default: true\n    controllerValue: \"k8s.io/ingress-nginx\"\n\nrbac:\n  create: true\npodSecurityPolicy:\n  enabled: false\n</code></pre> <p>With the above configuration, you can have a minimum running ingress controller</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#333-deploy-the-ingress-service","title":"3.3.3 Deploy the ingress service","text":"<pre><code># we deploy the ingress service with above\n# here the version is the helm chart version.\nhelm install ingress-nginx ingress-nginx/ingress-nginx -f ingress_values.yaml -n ingress-nginx --version v4.13.0\n\n# get all components of the ingress-nginx\nkubectl get all -n ingress-nginx\n\n# output example\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/ingress-nginx-controller-lnd4s   1/1     Running   0          81s\n\nNAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/ingress-nginx-controller             ClusterIP   10.96.43.173     &lt;none&gt;        80/TCP,443/TCP   81s\nservice/ingress-nginx-controller-admission   ClusterIP   10.111.211.231   &lt;none&gt;        443/TCP          81s\n\nNAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                              AGE\ndaemonset.apps/ingress-nginx-controller   1         1         1       1            1           ingress-node=true,kubernetes.io/os=linux   81s\n</code></pre> <p>After the pod of ingress service is created, you can try to send a request to the ip of <code>service/ingress-nginx-controller</code>.</p> <pre><code># In our example, the ip address of the service is 10.96.43.173, you can try below command\ncurl 10.96.43.173 \n\n# if you see below output, it means ingress-nginx is running and answering request\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>ingress nginx <code>cve-2025-1974</code>: https://kubernetes.io/blog/2025/03/24/ingress-nginx-cve-2025-1974/ Avoid the versions which are affected by this CVE.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#334-test-ingress-with-an-application","title":"3.3.4 Test ingress with an application","text":"<p>You can try to deploy the <code>mario app</code> and check the certificate. </p> <p>The full manifest can be found here</p> <pre><code># copy the three yaml files in a folder, then run\nkubectl apply -f .\n</code></pre> <p>If your dns is configured to redirect queries to the ingress service, then you should be able to use the url to access the service.</p> <p>You will notice, ingress assigns a <code>fake certificat</code>. Because we have not configured a valid certificate for ingress.</p> <p>We need to replace this <code>fake certificate</code> with a <code>valid certificate</code>.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#4-configure-nginx-with-a-custom-certificate-for-all-services","title":"4. Configure nginx with a custom certificate for all services","text":"<p>In this tutorial, we suppose you only have: - a <code>self-signed root CA certificate</code>. - a <code>wildcard certificate signed by the root CA</code>.</p> <p>The objectives are: - <code>Ingress controller</code> trusts the root CA, so it can validate certificates signed by it (for TLS termination). - All apps use the <code>wildcard certificate</code> (e.g., *.casd.local) signed by that internal root CA. - TLS is terminated at the ingress level, and the root CA is the trusted anchor.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#41-check-your-certificates","title":"4.1 Check your certificates","text":"<p>You need to have the below certificates: - root ca: - wildcard certificate signed by ca - wildcard certificate <code>private key</code></p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#42-create-a-tls-secret-with-the-wildcard-certificate","title":"4.2 Create a TLS secret with the wildcard certificate","text":"<p>Create a secret to host the certificate and private key.  we name the secret as  <code>casd-wildcard-certificate</code>, you can use the below command</p> <pre><code># general form\nkubectl create secret tls &lt;secret-name&gt; --namespace &lt;namespace-name&gt; --key=pathTo/ingress-tls.key --cert=pathTo/ingress-tls.crt -o yaml\n\n# example\nkubectl create secret tls casd-wildcard-certificate --key=wildcard-casd.key --cert=wildcard-casd.crt -o yaml -n ingress-nginx \n\n# view the content of the secret, the certificate and private value is in base64, you need to decode it to view the\n# value. No encryption at all, so we need to pay attention on who can view this secret.\nkubectl get secret casd-wildcard-certificate -o jsonpath='{.data}' -n ingress-nginx \n\n# you can also edit the value directly\nkubectl edit secret casd-wildcard-certificate -n ingress-nginx \n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#43-create-a-secret-for-root-ca","title":"4.3 Create a secret for root ca","text":"<pre><code># Create a Secret for the root CA\nkubectl create secret generic casd-root-ca \\\n  --from-file=ca.crt \\\n  -n ingress-nginx\n\n# check the created secret \nkubectl get secret casd-root-ca -o jsonpath='{.data}' -n ingress-nginx \n</code></pre>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#44-mount-root-ca-and-wildcard-certificate-into-nginx-controller","title":"4.4 Mount Root CA and wildcard certificate into NGINX controller","text":"<p>To tell the ingress to use the given certificate, you need to use extraArgs.default-ssl-certificate config. Below is a full example. Then you need to update the ingress controller with new configuration</p> <pre><code>controller:\n  # the ingress controller will process all Ingress resources that do not have an ingressClassName field.\n  watchIngressWithoutClass: true\n  allowSnippetAnnotations: false\n\n  # use node selector to install nginx on a specific node\n  # all nodes that have label ingress-node:\"true\" will have a replicas of the nginx \n  nodeSelector:\n    ingress-node: \"true\"\n\n  config:\n    error-log-level: \"info\"\n    ignore-invalid-headers: \"false\"\n    proxy-request-buffering: \"off\"\n    proxy-body-size: \"0\"\n    large-client-header-buffers: \"4 16k\"\n    # This tells NGINX to verify the TLS certificate presented by the upstream (backend) service.\n    # default value is off\n    proxy-ssl-verify: \"on\"\n    # Specifies the CA certificate NGINX should use to verify the backend service\u2019s TLS certificate\n    proxy-ssl-trusted-ca-file: \"/etc/nginx/certs/ca.crt\"\n    # tells NGINX which CA to use when verifying client certificates, i.e., when a client presents a certificate to authenticate itself.\n    ssl-trusted-ca-file: \"/etc/nginx/certs/ca.crt\"\n\n  hostNetwork: true\n\n  extraEnvs:\n    - name: MY_POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n\n  kind: DaemonSet\n\n  service:\n    enabled: true\n    type: ClusterIP\n\n  ingressClassResource:\n    name: nginx\n    enabled: true\n    default: true\n    controllerValue: \"k8s.io/ingress-nginx\"\n    # Parameters is a link to a custom resource containing additional\n    # configuration for the controller. This is optional if the controller\n    # does not require extra parameters.\n    parameters: {}\n\n  # Set global default TLS certificate (wildcard)\n  # no need to use  `- secretName: casd-test-tls-secret` in ingress.yaml\n  # to specify a custom certificate\n  # the default certificate should be a wildcard which covers your domain\n  extraArgs:\n    default-ssl-certificate: \"ingress-nginx/casd-wildcard-certificate\"\n\n  # Mount internal CA certificate\n  extraVolumeMounts:\n    - name: root-ca\n      mountPath: /etc/nginx/certs\n      readOnly: true\n\n  extraVolumes:\n    - name: root-ca\n      secret:\n        secretName: casd-root-ca\n\nrbac:\n  create: true\n\npodSecurityPolicy:\n  enabled: false\n</code></pre> <p>Make sure you have the wildcard and root-ca certificate secret in ingress-nginx name space.</p>"},{"location":"container/k8s/deploy_k8s_with_kubeadm/04.Post_k8s_cluster_installation/#45-update-existing-ingress-nginx-deployment","title":"4.5 Update existing ingress-nginx deployment","text":"<p>The best way to update a deployment (deployed via helm chart) is to modify the <code>values.yaml</code>. Then call the below command</p> <pre><code># general form\nhelm upgrade &lt;deployment-name&gt; &lt;chart-name&gt; -f &lt;config-file&gt; -n &lt;namespace&gt;\n\n# example\nhelm upgrade ingress-nginx ingress-nginx/ingress-nginx -f ingress_values.yaml -n ingress-nginx\n\n# to delete \nhelm delete ingress-nginx -n ingress-nginx\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/01.Install_kubectl_and_helm/","title":"K8s clients","text":"<p>To interact with a k8s cluster, you need at lest two clients: - kubectl - helm</p>"},{"location":"container/k8s/k8s_client_setup/01.Install_kubectl_and_helm/#1-kubectl","title":"1. kubectl","text":"<p>kubectl is a command line tool for communicating with a Kubernetes cluster's control plane, using the Kubernetes API.</p> <p>For configuration, it looks for a file named <code>config</code> in the <code>$HOME/.kube</code> directory. You can specify other  kubeconfig files by setting the <code>KUBECONFIG</code> environment variable or by setting the <code>--kubeconfig</code> flag.</p>"},{"location":"container/k8s/k8s_client_setup/01.Install_kubectl_and_helm/#11-installation","title":"1.1 Installation","text":"<p>You can find the official doc here</p> <p>You can follow the below steps to install it. Below instruction are tested for x86-64 architecture.</p> <pre><code># 1. Download the binary with curl \ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\n# 2. Get the hash of the binary\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\n\n# 3. validate the binary by checking the hash\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\n\n# 4. copy bin to your local bin\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n# 5. check the installation\nkubectl version\n\n# output example\nClient Version: v1.31.2\nKustomize Version: v5.4.2\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre> <p>The default config is pointing to localhost:8080, we need to replace it with the k8s api server url. </p>"},{"location":"container/k8s/k8s_client_setup/01.Install_kubectl_and_helm/#12-configuration","title":"1.2 Configuration","text":"<p>As we mentioned, the default config file is located at ``. Below is an example of the config content. </p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: changeMe\n    server: https://k8s-master:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: changeMe\n    client-key-data: changeMe\n</code></pre> <p>You can notice the access control in the k8s cluster is RBAC.</p> <p>Normally, the admin of the k8s cluster will provide you the login(username), and the credential(e.g. password, token, etc.)</p> <p>You can find the official doc of API access control here</p>"},{"location":"container/k8s/k8s_client_setup/01.Install_kubectl_and_helm/#helm","title":"Helm","text":"<p>You can find the official installation doc here. You need to choose a version which is compatible with your k8s cluster and your local OS(e.g. linux-amd64, windows-amd64, etc.).</p> <p>The available version can be found here.</p> <p>For example, in below example, we choose version <code>3.16.2</code>.</p> <pre><code># get the source\nwget https://get.helm.sh/helm-v3.16.2-linux-amd64.tar.gz\n\n\ntar -xzvf helm-v3.16.2-linux-amd64.tar.gz\n\nchmod a+x linux-amd64/helm\n\nmv linux-amd64/helm /usr/local/bin/helm\n\n# add the bitnami repo\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# show available charts in the bitnami repo\nhelm search repo bitnami\n</code></pre> <p>Check Artifact Hub for all public available Helm chart repositories.</p>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/","title":"Helm introduction","text":"<p>Helm is a package manager for Kubernetes. It provides an easy way to <code>find, share, and manage</code> Kubernetes  configurations. With Helm, you can:</p> <ul> <li>Deploy applications quickly and consistently.</li> <li>Version control deployments, enabling rollbacks to previous versions.</li> <li>Configure applications with different environments or options, using values and templates.</li> <li>Package complex applications that contain multiple components into a single Helm chart. </li> </ul> <p>It has a client server architecture: - helm repo servers: A server which stores and distributes helm charts - helm client</p>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#1-what-is-a-helm-chart","title":"1. What is a helm chart?","text":"<p>Helm uses a packaging format called charts.  It\u2019s essentially a collection of <code>YAML files and templates</code>  that define a Kubernetes application and its dependencies. Charts make it possible to bundle multiple  Kubernetes resources (like Deployments, Services, ConfigMaps, etc.) into a single package.</p> <p>Each Helm chart typically contains:</p> <ul> <li>Chart.yaml - Metadata about the chart, like its name, version, and description.</li> <li>values.yaml - Default configuration values that the chart uses. You can override these values when                       installing the chart, allowing for flexible configuration.</li> <li>Templates - A set of files defining Kubernetes resources (in YAML) with placeholders that get replaced                     based on values in <code>values.yaml</code>. Templates allow for dynamic configuration.</li> <li>README - Documentation explaining the chart, how to configure it, and any dependencies it has.</li> </ul> <p>Finally, <code>a running deployed instance of a chart</code> with a specific config is called a release.</p>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#2-install-helm-client","title":"2. Install helm client","text":"<p>You can find the official installation doc here. You need to choose a version which is compatible with your k8s cluster and your local OS(e.g. linux-amd64, windows-amd64, etc.).</p> <p>The available version can be found here.</p> <p>For example, in below example, we choose version <code>3.16.2</code>.</p> <pre><code># get the source\nwget https://get.helm.sh/helm-v3.16.2-linux-amd64.tar.gz\n\n\ntar -xzvf helm-v3.16.2-linux-amd64.tar.gz\n\nchmod a+x linux-amd64/helm\n\nmv linux-amd64/helm /usr/local/bin/helm\n\n# add the bitnami repo\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>Check Artifact Hub for all public available Helm chart repositories.</p>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#3-creating-your-own-chart","title":"3. Creating your own chart","text":"<p>Now let's create a new chart with name <code>hello-world</code></p> <pre><code># below command creates a chart skeleton\n# the name of the chart provided here (e.g. hello-world) \n# will be the directory's name where the chart is created and stored.\nhelm create hello-world\n\n# check the content of the generated hello-world folder\ntree -L 3 hello-world/\nhello-world/\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hpa.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ingress.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Let's understand the relevance of these created files and folders:</p> <ul> <li>charts: This is an optional directory that may contain sub-charts</li> <li>Chart.yaml: This is the main file that contains the description of our chart</li> <li>templates: This is the directory where Kubernetes resources are defined as templates</li> <li>values.yaml: This is the file that contains the default config values for our chart</li> <li>.helmignore: This is where we can define patterns to ignore when packaging (similar in concept to .gitignore)</li> </ul> <p>You can find the generated chart skeleton in resources/harbor/helm/charts/hello-world.</p> <p>Here, we only modify the <code>values.yaml</code> to deploy it on a k8s cluster</p>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#31-validating-a-chart","title":"3.1 Validating a chart","text":"<p>Before you deploy your chart, it's recommended that you valid your chart first (well-formed). You can use <code>helm lint</code> to do that. Below command is an example</p> <pre><code>helm lint ./hello-world\n</code></pre> <p>You can also render the generated k8s resource with the given default values.yaml</p> <pre><code>helm template ./hello-world\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#32-deploy-a-release-with-helm-chart","title":"3.2 Deploy a release with helm chart","text":"<p>Once we've verified the chart to be fine, finally, we can below command to install the chart into the Kubernetes cluster</p> <pre><code>helm install --name chart-sample ./hello-world\n\n# you can view the deployed release \nhelm ls -all \n\n# upgrade your release with new setup\nhelm upgrade hello-world ./hello-world\n\n# you can notice that after each upgrade, the revision number increase, so you can rollback\n# with any number that inferieur than current. Below example will rollback to 1\nhelm rollback hello-world 1\n\n# delete the release\nhelm uninstall hello-world\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#33-distributing-charts","title":"3.3 Distributing charts","text":"<p>Helm acts as a package manager for the Kubernetes application and makes installing, querying, upgrading, and deleting releases pretty seamless.</p> <p>In addition to this, we can also use <code>Helm to package, publish, and fetch Kubernetes applications as chart archives</code>.  We can also use the Helm CLI for this as it offers several commands to perform these activities. </p> <pre><code># after this command, you should see hello-world-0.1.0.tgz\nhelm package ./hello-world \n</code></pre>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#331-use-github-as-helm-repo","title":"3.3.1 Use github as helm repo","text":"<p>Finally, we need a mechanism to work with shared repositories to collaborate. There are several sub-commands available within this command that we can use to <code>add, remove, update, list, or index chart repositories</code>. Let's see how we can use them.</p> <p>We can create a git repository and use that to function as our chart repository. The only requirement is that it should have an index.yaml file.</p> <p>We can create index.yaml for our chart repo:</p> <pre><code># This generates the index.yaml file, which we should push to the repository along with the chart archives.\nhelm repo index my-repo/ --url https://&lt;username&gt;.github.io/my-repo\n\n# After successfully creating the chart repository, subsequently, we can remotely add this repo:\nhelm repo add my-repo https://my-pages.github.io/my-repo\n\n# Now, we should be able to install the charts from our repo directly:\nhelm install my-repo/hello-world --name=hello-world\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/02.Helm_introduction/#332-use-private-helm-repo","title":"3.3.2 Use private helm repo","text":""},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/","title":"Helm client use private registry","text":"<p>In this tutorial, I will show how to use helm client to interact with a private chart registry. - Add the private repo to the helm client - Create a custom helm chart - publish the chart to the private repo - Deploy an instance by downloading the helm chart from the private repo.</p> <p>In this tutorial, the private registry is built by using the chartmuseum.</p>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#1-add-the-private-registry-to-the-helm-client","title":"1. Add the private registry to the helm client","text":"<p>In this example, the private registry uses the basic auth to authenticate users</p> <pre><code># add the registry to helm client\n# I named the repo as cm\nhelm repo add --username admin --password changeMe cm https://chart.casd.local/\n\n# look up available charts in the repo\nhelm search repo cm\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#2-create-a-custom-helm-chart","title":"2. Create a custom helm chart","text":"<p>Now let's create a new chart with name <code>mario</code>. This chart will deploy the famous game <code>mario</code> as a web page game.</p> <pre><code># below command creates a chart skeleton\n# the name of the chart provided here (e.g. mario) \n# will be the directory's name where the chart is created and stored.\nhelm create mario\n\n# check the content of the generated mario folder\ntree -L 3 mario/\n\n# the architecture of the generated skeleton\nmario/\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hpa.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ingress.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>Let's understand the relevance of these created files and folders:</p> <ul> <li>charts: This is an optional directory that may contain sub-charts</li> <li>Chart.yaml: This is the main file that contains the description of our chart</li> <li>templates: This is the directory where Kubernetes resources are defined as templates</li> <li>values.yaml: This is the file that contains the default config values for our chart</li> <li>.helmignore: This is where we can define patterns to ignore when packaging (similar in concept to .gitignore)</li> </ul> <p>Suppose we have an origin deployment, service and ingress configuration as shown below.</p> <ul> <li>mario_deployment.yaml</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mario\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mario\n  template:\n    metadata:\n      labels:\n        app: mario\n    spec:\n      containers:\n        - name: mario\n          image: reg.casd.local/casd/docker-supermario\n          ports:\n            - name: http\n              containerPort: 8080\n      imagePullSecrets:\n        - name: harbor-auth\n</code></pre> <ul> <li>mario_service.yaml</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mario\nspec:\n  type: ClusterIP\n  ports:\n    - name: http\n      targetPort: 8080\n      port: 80\n  selector:\n    app: mario\n</code></pre> <ul> <li>mario_ingress.yaml</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-mario\nspec:\n  # tls:\n  #   - hosts:\n  #       - mario.kub.sspcloud.fr\n  rules:\n    - host: mario.casd.dev\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: mario\n                port:\n                  number: 80\n</code></pre> <p>In general, you need to at least modify the below files in the generated skeleton:  - <code>Chart.yaml</code>: name, description of the app. Pay attention of the value of <code>appVersion</code>, it will be used in the                   template <code>templates/deployment.yaml</code> as default image tag if the image tag value is empty in <code>values.yaml</code>.  - <code>templates/deployment.yaml</code>: You need to adapt the template based on the origin <code>deployment.yaml</code>. For example, the       <code>containerPort</code> value depends on how the image is build. In general, the default value will not work.  - <code>templates/service.yaml</code>: You need to adapt the template based on the origin <code>service.yaml</code>. For example, the       <code>targetPort</code> value depends on how the <code>deployment.yaml</code> is specified. You can't put a value which does not match.  - <code>templates/ingress.yaml</code>: You need to adapt the template based on the origin <code>ingress.yaml</code>.  - <code>values.yaml</code>: This stores all default value of the chart. If user provide nothing, the deployed instance will use                 the value in this file.</p> <p>You can find the complete chart example in src/k8s/helm/custom_chart/mario.</p> <p>Once, the chart is published, the user only need to modify the <code>values.yaml</code> to deploy it on a k8s cluster. If user need to modify the template, it means the chart is not well-designed.</p>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#31-validating-a-chart","title":"3.1 Validating a chart","text":"<p>Before you deploy your chart, it's recommended that you valid your chart first (well-formed).  You can use helm lint to do that. Below command is an example</p> <pre><code># validate the chart syntax\nhelm lint ./mario\n</code></pre> <p>You can also render the generated k8s resource with the given default values.yaml</p> <pre><code>helm template ./mario\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#32-deploy-a-release-with-helm-chart","title":"3.2 Deploy a release with helm chart","text":"<p>Once we've verified the chart to be fine, finally, we can below command to install the chart into the Kubernetes cluster</p> <pre><code>helm install mario-test ./mario\n\n# you can view the deployed release \nhelm ls -n &lt;name-space&gt;\n\n# upgrade your release with new chart version\nhelm upgrade mario-test ./mario\n\n# you can notice that after each upgrade, the revision number increase, so you can rollback\n# with any number that inferior than current. Below example will rollback to 1\nhelm rollback mario-test 1\n\n# delete the release\nhelm uninstall mario-test\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#4-publish-your-chart-to-private-registry","title":"4. Publish your chart to private registry","text":"<p>The below steps only works for registries built by <code>chartmuseum</code>.  There are two ways to push charts to ChartMuseum: - via the <code>api of chartMuseum</code> - via <code>helm cm-push plugin</code>, the easiest way is to use helm cm-push plugin. You can find the official github page here</p>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#41-publish-via-the-api","title":"4.1 Publish via the api","text":"<pre><code># package your chart source\ncd mario/\nhelm package .\n\n# this command will generate a .tgz file. The version comes from the version value of `Chart.yaml`\nmario-0.1.0.tgz \n\n# upload the binary to the registry\n# if you have setup an auto redirect from http to https, you must call https in the curl command, \n# otherwise you will get a 301 status error due to the redirection.\ncurl --data-binary \"@mario-0.1.0.tgz\" https://chart.casd.local/api/charts\n\n# If you\u2019ve signed your package and generated a provenance file, upload it with:\ncurl --data-binary \"@mario-0.1.0.tgz.prov\" https://chart.casd.local/api/prov\n\n# upload the package and provenance file at same time\ncurl -F \"chart=@mario-0.1.0.tgz\" -F \"prov=@mario-0.1.0.tgz.prov\" https://chart.casd.local/api/charts\n\n# check if a chart exists in the registry or not, this will return a list of all available charts\ncurl https://chart.casd.local/api/charts\n\n# delete a chart\ncurl -X DELETE https://chart.casd.local/api/charts/mario/0.1.0\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#42-publish-via-cm-push-plugin","title":"4.2 Publish via cm-push plugin","text":"<p>The <code>cm-push plugin</code> is not installed by default, you need to install it first.</p> <pre><code># install the plugin\nhelm plugin install https://github.com/chartmuseum/helm-push\n\n# check the installed plugin\nhelm cm-push  --help\n\n# add your private chartmuseum as a repo\nhelm repo add --username admin --password changeMe cm https://chart.casd.local/\n\n# push the chart, with the plugin, you don't need to do helm package anymore\n# you can push the directory directly, the plugin will package the chart, then push\nhelm cm-push mario/ cm\n\n# Push .tgz package is still supported\nhelm cm-push mario-0.1.0.tgz cm\n\n# push with a custom version\nhelm cm-push mario/ --version=\"0.2.0\" cm\n\n# If your ChartMuseum install is configured with ALLOW_OVERWRITE=true, chart versions will be automatically overwritten upon re-upload.\n# Otherwise, the upload will be denied with message file already exist. Unless your install is configured with DISABLE_FORCE_OVERWRITE=true (ChartMuseum &gt; v0.7.1), you can use the --force/-f option to to force an upload to overwrite an existing chart\nhelm cm-push --force mario-0.2.1.tgz chartmuseum\n\n# push without adding chart repo. Below example shows how to push to an repo directly\nhelm cm-push mario-0.2.1.tgz http://chart.casd.local/\n</code></pre> <p>To check if the register is updated or not. You can follow the below commands</p> <pre><code># list all available repo\nhelm repo list\n\n# update the index of a repo\nhelm repo update\n\n# Search a chart on all the added repo with a give keyword\nhelm search repo &lt;repo-name&gt;\n\n# if you want to use regex, you need to use option -r\nhelm search repo -r \"*mario*\"\n\n# to further filter your result, you can add an grep after\nhelm search repo -r \"nginx\" | grep -i \"bitnami\"\n</code></pre>"},{"location":"container/k8s/k8s_client_setup/03.Helm_use_private_repo/#5-remove-the-private-registry","title":"5. Remove the private registry","text":"<pre><code># Remove a repo\nhelm repo remove &lt;repo-name&gt;\n</code></pre>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/","title":"k8s cluster security best practices","text":""},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#1-secure-api-access","title":"1. Secure API Access","text":"<ul> <li>Use RBAC (Role-Based Access Control): Restrict permissions based on the principle of least privilege.</li> <li>Disable Anonymous Access: Ensure all API requests are authenticated.</li> <li>Enable Audit Logging: Monitor and log API activity to detect anomalies.</li> <li>Use Network Policies: Restrict API access to trusted sources.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#2-secure-authentication-authorization","title":"2. Secure Authentication &amp; Authorization","text":"<ul> <li>Use Strong Authentication: Rely on an identity provider (OIDC, Active Directory, etc.).</li> <li>Enable Multi-Factor Authentication (MFA): Strengthens access security.</li> <li>Limit Service Account Permissions: Avoid giving service accounts excessive privileges.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#3-secure-cluster-networking","title":"3. Secure Cluster Networking","text":"<ul> <li>Use Network Policies: Restrict pod-to-pod communication based on necessity.</li> <li>Enable Encryption in Transit: Use TLS for securing API server, ETCD, and service communications.</li> <li>Restrict External Access: Expose only necessary services using Ingress controllers.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#4-protect-etcd-database","title":"4. Protect ETCD Database","text":"<ul> <li>Enable Encryption at Rest: Encrypt sensitive data stored in ETCD.</li> <li>Restrict Access to ETCD: Allow access only to the API server and use strong authentication.</li> <li>Regular Backups: Ensure ETCD is backed up regularly and stored securely.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#5-secure-workloads-pods","title":"5. Secure Workloads &amp; Pods","text":"<ul> <li>Use Pod Security Standards (PSS) or Pod Security Admission (PSA): Restrict privileges for pods.</li> <li>Enable Seccomp and AppArmor: Use security profiles to limit system calls.</li> <li>Avoid Running as Root: Use runAsUser and runAsNonRoot to enforce non-root execution.</li> <li>Restrict Privileged Containers: Prevent containers from running with excessive privileges.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#6-secure-supply-chain-image-security","title":"6. Secure Supply Chain &amp; Image Security","text":"<ul> <li>Use Trusted Container Images: Always pull images from verified sources.</li> <li>Sign and Verify Images: Use tools like Cosign or Docker Content Trust.</li> <li>Scan Images for Vulnerabilities: Utilize tools like Trivy, Clair, or Anchore.</li> <li>Restrict Image Registries: Allow deployments only from trusted registries.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#7-limit-resource-access","title":"7. Limit Resource Access","text":"<ul> <li>Enforce Resource Quotas &amp; Limits: Prevent resource exhaustion by setting CPU/memory limits.</li> <li>Use Admission Controllers: Enforce security policies before workloads are created.</li> <li>Disable Default Service Account Auto-Mounting: Prevent unnecessary service account access.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#8-secure-logging-monitoring","title":"8. Secure Logging &amp; Monitoring","text":"<ul> <li>Use Security Information and Event Management (SIEM) Tools: Aggregate logs for real-time analysis.</li> <li>Enable Kubernetes Audit Logs: Detect unauthorized API actions.</li> <li>Monitor Containers and Nodes: Use Prometheus, Falco, or Sysdig for runtime security.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#9-protect-against-network-attacks","title":"9. Protect Against Network Attacks","text":"<ul> <li>Enable CNI Plugins with Security Features: Calico, Cilium, or Antrea provide additional security.</li> <li>Enforce Egress and Ingress Rules: Restrict network traffic using NetworkPolicies.</li> <li>Use Service Mesh for Encryption: Istio or Linkerd provide mutual TLS for service-to-service communication.</li> </ul>"},{"location":"container/k8s/k8s_security/01.K8s_security_best_practices/#10-keep-kubernetes-and-dependencies-updated","title":"10. Keep Kubernetes and Dependencies Updated","text":"<ul> <li>Regularly Patch and Upgrade Kubernetes: Stay up to date with the latest security patches.</li> <li>Upgrade Third-Party Plugins &amp; Dependencies: Keep Helm charts and other dependencies current.</li> </ul>"},{"location":"container/k8s/k8s_security/06.k8s_security/","title":"k8s network security","text":""},{"location":"container/k8s/k8s_security/06.k8s_security/#network-security","title":"network security","text":"<p><code>Container Network Interface (CNI)</code> is a framework for dynamically configuring networking resources.</p> <p>Check this page https://www.tigera.io/learn/guides/kubernetes-networking/kubernetes-cni/</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/","title":"Integrate kerberos into a hadoop cluster","text":"<p>In this tutorial, we will show how to integrate kerberos into a hadoop cluster. The goal is to use the kerberos tickets to authenticate users, hosts(e.g. namenode, datanode, resourceManager, etc.) and services(e.g. hdfs, yarn). </p> <p>We have different strategy for different kinds of users. For service account, we will generate <code>.keytab</code> files to generate kerberos tickets automatically. For user account, a password maybe required to generate the ticket.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#1-prerequisite","title":"1. Prerequisite","text":"<p>Before we start, we need to clarify the hadoop cluster context. Because the <code>AD/Ldap account</code> and <code>kerberos principal</code> naming conventions strongly depends on the cluster architecture.</p> <p>Suppose we have three servers, in each server we run different services: - spark-m01.casdds.casd: name-node(hdfs), resource-manager(yarn), history-server(spark) - spark-m02.casdds.casd: data-node(hdfs), node-manager(yarn) - spark-m03.casdds.casd: data-node(hdfs), node-manager(yarn)</p> <p>We suppose you already join these machines into the AD/krb realm. For more details, you can check this  doc </p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#11-prepare-service-account-and-their-keytab","title":"1.1 Prepare service account and their keytab","text":"<p>By convention, we recommend you to create <code>a dedicated AD/Ldap account and kerberos principal for each service</code>.  This ensures <code>secure authentication</code> and <code>proper ticket management</code>. It's also easier to monitor access and avoid  unexpected situations. Technically, an AD/Ldap account can be associated with one or more kerberos principals.</p> <p>Below is a list of all AD/Ldap accounts and kerberos principal you need to create:</p> Service Hadoop Role Host Kerberos Principal AD account name HDFS NameNode spark-m01.casdds.casd nn/spark-m01.casdds.casd@CASDDS.CASD hdfs-nn HDFS DataNode spark-m02.casdds.casd dn/spark-m02.casdds.casd@CASDDS.CASD hdfs-dn1 HDFS DataNode spark-m03.casdds.casd dn/spark-m03.casdds.casd@CASDDS.CASD hdfs-dn2 HDFS HTTP Service spark-m01.casdds.casd http/spark-m01.casdds.casd@CASDDS.CASD http-nn HDFS HTTP Service spark-m02.casdds.casd http/spark-m02.casdds.casd@CASDDS.CASD http-dn1 HDFS HTTP Service spark-m03.casdds.casd http/spark-m03.casdds.casd@CASDDS.CASD http-dn2 YARN ResourceManager spark-m01.casdds.casd rm/spark-m01.casdds.casd@CASDDS.CASD yarn-rn YARN NodeManager spark-m02.casdds.casd nm/spark-m02.casdds.casd@CASDDS.CASD yarn-nm1 YARN NodeManager spark-m03.casdds.casd nm/spark-m03.casdds.casd@CASDDS.CASD yarn-nm2 Spark History Server spark-m01.casdds.casd jhs/spark-m01.casdds.casd@CASDDS.CASD spark-jhs HOST None spark-m01.casdds.casd host/spark-m01.casdds.casd@CASDDS.CASD spark-m01 HOST None spark-m02.casdds.casd host/spark-m02.casdds.casd@CASDDS.CASD spark-m02 HOST None spark-m03.casdds.casd host/spark-m03.casdds.casd@CASDDS.CASD spark-m03 <p>The AD account name cannot contain special character such as <code>@</code> and <code>.</code>, so we can't use the principal name as  AD account name. </p> <p>You can create an AD account in windows with the below command</p> <pre><code># create AD account and kerberos principal\nNew-ADUser -Name \"hdfs-nn\" -SamAccountName \"hdfs-nn\" -UserPrincipalName \"nn/spark-m01.casdds.casd@CASDDS.CASD\" -Enabled $true -PasswordNeverExpires $true -CannotChangePassword $true -ChangePasswordAtLogon $false -PassThru | Set-ADAccountControl -PasswordNotRequired $true\n\n# create corresponding keytab\nktpass -princ nn/spark-m01.casdds.casd@CASDDS.CASD -mapuser hdfs-nn -crypto ALL -ptype KRB5_NT_PRINCIPAL -pass Password! -out hdfs-nn.keytab\n</code></pre> <p>After you generate the required keytab files for all principals, you need to copy them to the target server. For example, for server <code>spark-m01.casdds.casd@CASDDS.CASD</code>, you need to copy the keytab file for principals: - nn/spark-m01.casdds.casd@CASDDS.CASD - HTTP/spark-m01.casdds.casd@CASDDS.CASD - rm/spark-m01.casdds.casd@CASDDS.CASD - jhs/spark-m01.casdds.casd@CASDDS.CASD - host/spark-m01.casdds.casd@CASDDS.CASD</p> <p>The general rule is straightforward, you need to check the host fqdn name in the principals </p> <p>You can test the validity of the keytab file by asking a kerberos ticket. Below command is an example</p> <pre><code>kinit -kt /etc/hdfs-nn.keytab nn/spark-m01.casdds.casd@CASDDS.CASD\n</code></pre> <p>To show the details of a keytab file, you can use the below command: </p> <pre><code>klist -e -k -t /etc/yarn.keytab\n\n# expected output\nKeytab name: FILE: /etc/yarn.keytab\n KVNO Timestamp         Principal\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (AES-256 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (AES-128 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (ArcFour with HMAC/md5)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (AES-256 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (AES-128 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (ArcFour with HMAC/md5)\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#12-merge-the-keytab-files","title":"1.2 Merge the keytab files","text":"<p>To avoid managing many keytab files, you can merge the multi keytab files into one by using <code>ktutil</code> tool.</p> <pre><code># start a ktuitl shell with sudo right\nsudo ktutil\n\n# load credentials from the keytab files\nrkt /tmp/yarnm02.keytab\nrkt /tmp/hostm02.keytab\n\n# output the loaded credential to a new keytable file\nwkt /tmp/merged.keytab\n\n# exit the ktuitl shell\nq\n</code></pre> <p>You can test the content of the merged keytab file with the below command</p> <pre><code>sudo klist -k /tmp/merged.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#13-check-if-the-hadoop-servers-are-in-the-ad-dns","title":"1.3. Check if the hadoop servers are in the AD DNS","text":"<p>Normally, when the linux servers have joined the AD/Krb realm, their AD/DNS configuration are done automatically.</p> <p>Just to make sure, you can open the DNS manager on the <code>Domain controller</code> where AD/Krb is located. Below figure is an example of the <code>DNS manager GUI</code></p> <p></p> <p>You need to check the <code>value of FQDN and ip</code> for each server in <code>forward and reverse lookup zones</code>.</p> <p>Below figure is an example for the Forward loopup zone definition of a server</p> <p></p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#14-check-kerberos-client","title":"1.4. Check kerberos client","text":"<p>Normally, you should have a valid krb5 client and config on each hadoop node.</p> <p>Below is an example of the krb5 client conf(<code>/etc/krb5.conf</code>).</p> <pre><code>[libdefaults]\ndefault_realm = CASDDS.CASD\ndefault_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\ndefault_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\npermitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\nkdc_timesync = 1\nccache_type = 4\nforwardable = true\nticket_lifetime = 24h\ndns_lookup_realm = true\ndns_lookup_kdc = true\n\n#allow_weak_crypto = true\n\n[realms]\nCASDDS.CASD = {\n    kdc = 10.50.5.64\n    admin_server = 10.50.5.64\n}\n\n[domain_realm]\n.CASDDS.CASD = CASDDS.CASD\nCASDDS.CASD = CASDDS.CASD\n</code></pre> <p>You can enable allow_weak_crypto = true, if the AD/Krb can't use advance crypto algorithm</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#2-enable-ssl-in-the-hadoop-cluster","title":"2. Enable SSL in the hadoop cluster","text":"<p>To secure communication between services in the hadoop cluster, we can enable SSL.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#21-generate-certificate","title":"2.1 Generate certificate","text":"<p>Generate a key pair and store it in a java key store</p> <pre><code>sudo keytool -genkeypair \\\n  -alias hadoop \\\n  -keyalg RSA \\\n  -keysize 2048 \\\n  -validity 365 \\\n  -keystore /opt/hadoop/keystore.jks \\\n  -storepass changeit\n</code></pre> <p>Check the keystore content </p> <pre><code>sudo keytool -list -keystore /opt/hadoop/keystore.jks -storepass changeit\n</code></pre> <p>Export the certificate</p> <pre><code>sudo keytool -export -alias hadoop -keystore /opt/hadoop/keystore.jks -file /opt/hadoop/hadoop-cert.pem -storepass changeit\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#22-configuration-of-ssl-in-hadoop-cluster","title":"2.2 Configuration of SSL in hadoop cluster","text":"<p>The ssl configuration file in hadoop cluster is <code>$HADOOP_HOME/etc/hadoop/ssl-server.xml</code> and  <code>$HADOOP_HOME/etc/hadoop/ssl-client.xml</code>. In our case, we only need to modify <code>ssl-server.xml</code>.</p> <p>Below is an example of the <code>ssl-server.xml</code></p> <pre><code>sudo vim ssl-server.xml\n</code></pre> <p>Add the below lines</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/keystore.jks&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n    &lt;value&gt;jks&lt;/value&gt;\n    &lt;description&gt;(Optionnel) Format du keystore (par d\u00e9faut \u00ab jks \u00bb).&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.exclude.cipher.list&lt;/name&gt;\n    &lt;value&gt;\n      TLS_ECDHE_RSA_WITH_RC4_128_SHA,\n      SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,\n      SSL_RSA_WITH_DES_CBC_SHA,\n      SSL_DHE_RSA_WITH_DES_CBC_SHA,\n      SSL_RSA_EXPORT_WITH_RC4_40_MD5,\n      SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,\n      SSL_RSA_WITH_RC4_128_MD5\n    &lt;/value&gt;\n    &lt;description&gt;(Optionnel) Liste des suites de chiffrement faibles \u00e0 exclure.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>This needs to be done all nodes of the hadoop cluster</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#2-integrate-kerberos-in-to-hadoop-cluster","title":"2. Integrate kerberos in to Hadoop cluster","text":"<p>Here, we suppose you already have a working hadoop cluster, the below steps only shows how to integrate kerberos into Hadoop. If you want to learn how to deploy a hadoop cluster, you need to follow this  doc</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#21-edit-hadoop-envsh","title":"2.1 Edit hadoop-env.sh","text":"<p>The <code>hadoop-env.sh</code> file specifies all environment variables related to the hadoop cluster</p> <p>Below is a list of variables you need to check </p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext\"\n# use krb client config \nexport HADOOP_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf $HADOOP_OPTS\"\nexport HDFS_NAMENODE_USER=hadoop\nexport HDFS_DATANODE_USER=hadoop\nexport HDFS_SECONDARYNAMENODE_USER=hadoop\nexport JSVC_HOME=$(dirname $(which jsvc))\nexport HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport HADOOP_SECURITY_LOGGER=INFO,RFAS,console\nexport JAVA_TOOL_OPTIONS=\"$JAVA_TOOL_OPTIONS --add-opens=java.base/sun.net.dns=ALL-UNNAMED\"\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#22-updates-the-security-configuration-of-jdk","title":"2.2 Updates the security configuration of JDK","text":"<p>For kerberos interoperate well with JDK, we need to update the default security conf of the JDK. </p> <pre><code>sudo vim $JAVA_HOME/conf/security/java.security\n\n# in our case, we use openjdk in debian 11. We can use the absolute path\nsudo vim /usr/lib/jvm/java-11-openjdk-amd64/conf/security/java.security\n\n# you need to add the below lines\ncrypto.policy=unlimited\nsun.security.krb5.disableReferrals=true\n</code></pre> <p>By default, the <code>RC4</code> encryption algo is disabled, because it's weak. But some Windows server still uses it. You can find the line <code>jdk.jar.disabledAlgorithms</code> and <code>jdk.tls.disabledAlgorith</code>, then remove the <code>RC4</code> from the  disabled algo list.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#23-update-the-hadoop-service-configuration","title":"2.3 Update the hadoop service configuration","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#231-for-name-nodes","title":"2.3.1 For Name nodes","text":"<p>For <code>Name nodes</code>, you need to edit the below config files: - core-site.xml - hdfs-site.xml - yarn-site.xml</p> <pre><code>sudo vim core-site.xml \n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/etc/hadoop/ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;\n    &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/httpm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.security.AuthenticationFilterInitializer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@casdds\\.casd)s/@casdds\\.casd//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers le nom d\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim hdfs-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n    &lt;description&gt;Augmentation de la file d\u2019attente pour g\u00e9rer davantage de connexions clients.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour s\u00e9curiser l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:9870&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim yarn-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#232-for-datanode","title":"2.3.2 For DataNode","text":"<p>For data nodes, you need to edit the below configuration files: - core-site.xml - hdfs-site.xml - yarn-site.xml</p> <pre><code>sudo vim core-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@CASDDS\\.CASD)s/@CASDDS\\.CASD//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers l\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim hdfs-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;ip-10-50-5-203.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode sur le DataNode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n    &lt;value&gt;750&lt;/value&gt;\n    &lt;description&gt;Permissions requises sur les r\u00e9pertoires de donn\u00e9es.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim yarn-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m02.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#refresh-user-to-group-mappings","title":"Refresh User to group mappings","text":"<pre><code>hdfs dfsadmin -refreshUserToGroupsMappings\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#reference","title":"Reference","text":"<ul> <li>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</li> <li>http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.5/security-reference/content/kerberos_nonambari_adding_security_information_to_configuration_files.html</li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#repo-test","title":"Repo test","text":"<p>https://github.com/CASD-EU/admin_sys/tree/test/dev/roles</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/","title":"Integrate kerberos in hadoop cluster fr","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#integration-de-kerberos-dans-un-cluster-hadoop","title":"Int\u00e9gration de Kerberos dans un cluster Hadoop","text":"<p>Cette documentation d\u00e9crit les \u00e9tapes pour s\u00e9curiser l\u2019authentification des composants d\u2019un cluster Hadoop (NameNode, DataNode, ResourceManager, etc.) \u00e0 l\u2019aide de Kerberos et SSL.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#1-contexte-et-architecture","title":"1. Contexte et architecture","text":"<ul> <li> <p>Cluster : 3 n\u0153uds (spark-m01, spark-m02, spark-m03) dans le domaine CASDDS.CASD</p> </li> <li> <p>spark-m01 : NameNode, ResourceManager, HistoryServer</p> </li> <li> <p>spark-m02 &amp; spark-m03 : DataNode, NodeManager</p> </li> <li> <p>But :</p> </li> <li> <p>Authentification forte via Kerberos</p> </li> <li>Chiffrement des communications via SSL/TLS</li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#2-prerequis","title":"2. Pr\u00e9requis","text":"<ol> <li> <p>AD/Kerberos</p> </li> <li> <p>Les serveurs Linux doivent \u00eatre joints au domaine AD/Kerberos <code>CASDDS.CASD</code></p> </li> <li> <p>Un contr\u00f4leur de domaine (KDC + DNS) configur\u00e9 pour forward/reverse lookup</p> </li> <li> <p>Logiciels</p> </li> <li> <p>Java\u00a011 (OpenJDK)</p> </li> <li>Hadoop 3.x</li> <li>Utilitaires Kerberos (<code>kinit</code>, <code>klist</code>, <code>ktpass</code>, <code>ktutil</code>)</li> <li>Keytool (Java)</li> </ol>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#3-configuration-kerberos","title":"3. Configuration Kerberos","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#31-creation-des-comptes-service-et-keytabs","title":"3.1. Cr\u00e9ation des comptes service et keytabs","text":"<p>Pour chaque service, cr\u00e9ez un compte AD d\u00e9di\u00e9 et g\u00e9n\u00e9rez un fichier <code>.keytab</code> :</p> Service R\u00f4le FQDN Principal Kerberos AD User HDFS NameNode NameNode spark-m01.casdds.casd <code>hdfs/spark-m01.casdds.casd@CASDDS.CASD</code> <code>hdfs-m01</code> HDFS DataNode DataNode spark-m02.casdds.casd <code>hdfs/spark-m02.casdds.casd@CASDDS.CASD</code> <code>hdfs-m02</code> HDFS DataNode DataNode spark-m03.casdds.casd <code>hdfs/spark-m03.casdds.casd@CASDDS.CASD</code> <code>hdfs-m03</code> HTTP HTTP Service spark-m01.casdds.casd <code>HTTP/spark-m01.casdds.casd@CASDDS.CASD</code> <code>http-m01</code> YARN RM ResourceManager spark-m01.casdds.casd <code>yarn/spark-m01.casdds.casd@CASDDS.CASD</code> <code>yarn-m01</code> YARN NM NodeManager spark-m0X.casdds.casd <code>yarn/spark-m0X.casdds.casd@CASDDS.CASD</code> <code>yarn-m0x</code> HOST Host principal spark-m0X.casdds.casd <code>host/spark-m0X.casdds.casd@CASDDS.CASD</code> <code>host-m0X</code> <p>Commande Windows (AD) :</p> <pre><code>New-ADUser -Name \"hdfs-m01\" -SamAccountName \"hdfs-m01\" \\\n  -UserPrincipalName \"hdfs/spark-m01.casdds.casd@CASDDS.CASD\" \\\n  -Enabled $true -PasswordNeverExpires $true -CannotChangePassword $true\n\nktpass -princ hdfs/spark-m01.casdds.casd@CASDDS.CASD \\\n  -mapuser hdfs-m01 -crypto ALL -ptype KRB5_NT_PRINCIPAL \\\n  -pass \"Password!\" -out hdfs-m01.keytab\n\nscp hdfs-m01.keytab user@spark-m0x.casdds.casd:/tmp/\n</code></pre> <p>Copiez chaque keytab sous <code>/etc/</code> sur le serveur correspondant, avec permissions <code>root:hadoop</code>, <code>chmod 640</code>.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#32-verification-des-keytabs","title":"3.2. V\u00e9rification des keytabs","text":"<pre><code># Authentication sans mot de passe\nkinit -kt /etc/hdfs-m01.keytab hdfs/spark-m01.casdds.casd@CASDDS.CASD\n# Afficher tickets\nklist\n# Lister contenu d'un keytab\nklist -e -k -t /etc/hdfs-m01.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#33-fusion-de-plusieurs-keytabs","title":"3.3. Fusion de plusieurs keytabs","text":"<pre><code>sudo ktutil\nrkt /tmp/yarn-m02.keytab\nrkt /tmp/host-m02.keytab\nwkt /etc/merged.keytab\nq\nsudo klist -k /etc/merged.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#4-configuration-du-client-kerberos-etckrb5conf","title":"4. Configuration du client Kerberos (/etc/krb5.conf)","text":"<pre><code>[libdefaults]\n  default_realm = CASDDS.CASD\n  default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  permitted_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  kdc_timesync = 1\n  ticket_lifetime = 24h\n  forwardable = true\n  dns_lookup_realm = true\n  dns_lookup_kdc = true\n  rdns = false\n  #allow_weak_crypto = true\n\n[realms]\n  CASDDS.CASD = {\n    kdc = @ip\n    admin_server = @ip\n  }\n\n[domain_realm]\n  .casdds.casd = CASDDS.CASD\n  casdds.casd = CASDDS.CASD\n</code></pre> <p>D\u00e9commentez <code>allow_weak_crypto = true</code> si n\u00e9cessaire pour RC4.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#5-securisation-ssltls","title":"5. S\u00e9curisation SSL/TLS","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#51-generation-du-keystore-java","title":"5.1. G\u00e9n\u00e9ration du keystore Java","text":"<pre><code>sudo keytool -genkeypair \\\n  -alias hadoop -keyalg RSA -keysize 2048 -validity 365 \\\n  -keystore /opt/hadoop/keystore.jks -storepass changeit\nsudo keytool -list -keystore /opt/hadoop/keystore.jks -storepass changeit\nsudo keytool -export -alias hadoop \\\n  -file /opt/hadoop/hadoop-cert.pem -keystore /opt/hadoop/keystore.jks \\\n  -storepass changeit\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#52-configuration-ssl-serverxml","title":"5.2. Configuration <code>ssl-server.xml</code>","text":"<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/keystore.jks&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n        &lt;value&gt;jks&lt;/value&gt;\n        &lt;description&gt;Optional. The keystore file format, default value is \"jks\".&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.exclude.cipher.list&lt;/name&gt;\n        &lt;value&gt;TLS_ECDHE_RSA_WITH_RC4_128_SHA,SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,\n        SSL_RSA_WITH_DES_CBC_SHA,SSL_DHE_RSA_WITH_DES_CBC_SHA,\n        SSL_RSA_EXPORT_WITH_RC4_40_MD5,SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,\n        SSL_RSA_WITH_RC4_128_MD5&lt;/value&gt;\n        &lt;description&gt;Optional. The weak security cipher suites that you want excludedfrom SSL communication.&lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>R\u00e9p\u00e9tez l\u2019op\u00e9ration sur tous les n\u0153uds.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#6-configuration-hadoop","title":"6. Configuration Hadoop","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#61-hadoop-envsh","title":"6.1. <code>hadoop-env.sh</code>","text":"<pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext\"\nexport HADOOP_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf $HADOOP_OPTS\"\nexport HDFS_NAMENODE_USER=hadoop\nexport HDFS_DATANODE_USER=hadoop\nexport HDFS_SECONDARYNAMENODE_USER=hadoop\nexport JSVC_HOME=$(dirname $(which jsvc))\nexport HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport HADOOP_SECURITY_LOGGER=INFO,RFAS,console\nexport JAVA_TOOL_OPTIONS=\"$JAVA_TOOL_OPTIONS --add-opens=java.base/sun.net.dns=ALL-UNNAMED\"\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#62-politiques-de-securite-java","title":"6.2. Politiques de s\u00e9curit\u00e9 Java","text":"<pre><code># $JAVA_HOME/conf/security/java.security\ncrypto.policy = unlimited\nsun.security.krb5.disableReferrals = true\n# Retirer RC4 de jdk.jar.disabledAlgorithms / jdk.tls.disabledAlgorithms si besoin\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#63-configuration-des-services","title":"6.3. Configuration des services","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#631-namenode-core-sitexml-hdfs-sitexml-yarn-sitexml","title":"6.3.1. NameNode (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code>)","text":"<pre><code>&lt;!-- core-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/etc/hadoop/ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;\n    &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/http-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.security.AuthenticationFilterInitializer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@casdds\\.casd)s/@casdds\\.casd//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers le nom d\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- hdfs-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n    &lt;description&gt;Augmentation de la file d\u2019attente pour g\u00e9rer davantage de connexions clients.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour s\u00e9curiser l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:9870&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- yarn-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#632-datanode-core-sitexml-hdfs-sitexml-yarn-sitexml","title":"6.3.2. DataNode (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code>)","text":"<pre><code>&lt;!-- core-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@CASDDS\\.CASD)s/@CASDDS\\.CASD//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers l\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- hdfs-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;ip-x.x.x.x.casdds.casd:50470&lt;/value&gt;  &lt;!-- @ip namdenode --&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode sur le DataNode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n    &lt;value&gt;750&lt;/value&gt;\n    &lt;description&gt;Permissions requises sur les r\u00e9pertoires de donn\u00e9es.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- yarn-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m0x.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#7-validation-et-tests","title":"7. Validation et tests","text":"<ol> <li>Tester kinit sur chaque n\u0153ud/service</li> <li>D\u00e9marrer Hadoop en mode s\u00e9curis\u00e9 </li> <li>V\u00e9rifier que les services \u00e9coutent en HTTPS et que <code>klist</code> montre les tickets actifs</li> <li>Rafra\u00eechir les mappings utilisateurs-groupes :</li> </ol> <p><code>bash    hdfs dfsadmin -refreshUserToGroupsMappings</code></p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#8-references","title":"8. R\u00e9f\u00e9rences","text":"<ul> <li> <p>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</p> </li> <li> <p>http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.5/security-reference/content/kerberos_nonambari_adding_security_information_to_configuration_files.html</p> </li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#9-repo-ansible","title":"9. Repo Ansible","text":"<ul> <li>https://github.com/CASD-EU/admin_sys/tree/test/dev/roles</li> </ul>"},{"location":"python/01.Pyenv-offline-config/","title":"Pyenv offline config","text":"<p>Pyenv is a greate tool to install python with internet. In this tutorial, we will show how to set up pyenv for offline usage.</p> <p>The official github page is here: https://github.com/pyenv/pyenv</p>"},{"location":"python/01.Pyenv-offline-config/#1-static-python-source-server","title":"1. Static python source server","text":"<p>Nginx config</p> <pre><code>server {\n  listen        80;\n  server_name   python-src.casd.local;\n  error_log     /var/logs/python-src.error.log;\n\n  location / {\n    autoindex on;\n    root  /data/python;\n  } \n\n}\n</code></pre>"},{"location":"python/01.Pyenv-offline-config/#2-install-pyenv-offline","title":"2. Install pyenv offline","text":"<p>There is an official offline installation script. You can find it here</p> <pre><code>#!/usr/bin/env bash\n\nset -e\n[ -n \"$PYENV_DEBUG\" ] &amp;&amp; set -x\n\nif [ -z \"$PYENV_ROOT\" ]; then\n  if [ -z \"$HOME\" ]; then\n    printf \"$0: %s\\n\" \\\n      \"Either \\$PYENV_ROOT or \\$HOME must be set to determine the install location.\" \\\n      &gt;&amp;2\n    exit 1\n  fi\n  PYENV_ROOT=\"${HOME}/.pyenv\"\nfi\n\ncolorize() {\n  if [ -t 1 ]; then printf \"\\e[%sm%s\\e[m\" \"$1\" \"$2\"\n  else echo -n \"$2\"\n  fi\n}\n\n# Checks for `.pyenv` file, and suggests to remove it for installing\nif [ -d \"${PYENV_ROOT}\" ]; then\n  { echo\n    colorize 1 \"WARNING\"\n    echo \": Can not proceed with installation. Kindly remove '.pyenv' from ${HOME} first.\"\n    echo\n  } &gt;&amp;2\n    exit 1\nfi\n\nconditional_mv() {\n  [ -d \"$2\" ] || mkdir -p \"$2\" &amp;&amp; mv \"$1\"/* \"$2\"\n}\n\nif ! command -v git 1&gt;/dev/null 2&gt;&amp;1; then\n  echo \"pyenv: Git is not installed, can't continue.\" &gt;&amp;2\n  exit 1\nfi\n\n# PYENV_PACKAGE_ARCHIVE is the path of pyenv compressed archive file.\nif [ -z \"$PYENV_PACKAGE_ARCHIVE\" ]; then\n  PYENV_PACKAGE_ARCHIVE=\"$(cd $(dirname \"$0\") &amp;&amp; pwd)/pyenv-package.tar.gz\"\nfi\n\nif [ ! -e \"$PYENV_PACKAGE_ARCHIVE\" ]; then\n  { echo\n    colorize 1 \"ERROR\"\n    echo \": file $PYENV_PACKAGE_ARCHIVE not exists.\"\n    echo\n  } &gt;&amp;2\n  exit 1\nfi\n\n# Decompress archive.\nTMP_DIR=$(mktemp -d)\n\ntar -xf \"$PYENV_PACKAGE_ARCHIVE\" -C \"$TMP_DIR\"\n\nconditional_mv \"$TMP_DIR/pyenv\"            \"${PYENV_ROOT}\"\nconditional_mv \"$TMP_DIR/pyenv-doctor\"     \"${PYENV_ROOT}/plugins/pyenv-doctor\"\nconditional_mv \"$TMP_DIR/pyenv-update\"     \"${PYENV_ROOT}/plugins/pyenv-update\"\nconditional_mv \"$TMP_DIR/pyenv-virtualenv\" \"${PYENV_ROOT}/plugins/pyenv-virtualenv\"\n\nrm -rf $TMP_DIR\n\n\nif ! command -v pyenv 1&gt;/dev/null; then\n  { echo\n    colorize 1 \"WARNING\"\n    echo \": seems you still have not added 'pyenv' to the load path.\"\n    echo\n  } &gt;&amp;2\n\n  { # Without args, `init` commands print installation help\n    \"${PYENV_ROOT}/bin/pyenv\" init || true\n    \"${PYENV_ROOT}/bin/pyenv\" virtualenv-init || true\n  } &gt;&amp;2\nfi\n</code></pre>"},{"location":"python/01.Pyenv-offline-config/#21-add-pyenv-to-your-bashrc","title":"2.1 Add pyenv to your bashrc","text":"<p>To add pyenv to your PATH, you need to add the following lines into your <code>.bashrc</code> or <code>.profile</code> </p> <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\n[[ -d $PYENV_ROOT/bin ]] &amp;&amp; export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n\n# after adding the lines, you need to reload the bashrc file\n\nsource ~/.bashrc\n</code></pre>"},{"location":"python/01.Pyenv-offline-config/#22-remove-pyenv","title":"2.2 Remove pyenv","text":"<pre><code># remove the pyenv source\nrm -rf ~/.pyenv/\n\n# Remove pyenv initialization lines from your shell configuration file (.bashrc, .zshrc, etc.).\nexport PYENV_ROOT=\"$HOME/.pyenv\"\n[[ -d $PYENV_ROOT/bin ]] &amp;&amp; export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n\n# Reload the shell.\nsource ~/.bashrc\n\n# remove the python build dependencies if you no longer need them\nsudo yum remove gcc zlib-devel bzip2-devel readline-devel sqlite-devel openssl-devel\n</code></pre>"},{"location":"python/01.Pyenv-offline-config/#3-use-pyenv-to-install-python","title":"3. Use pyenv to install python","text":"<p>Once the pyenv is installed, you can check which python version is available for installation.</p> <pre><code># this will show the full list\npyenv install --list\n</code></pre> <p>The configuration of these python source is under {pyenv-root}/.pyenv/plugins/python-build/share/python-build</p> <p>For example, for python 3.12.7, you have the following config file</p> <pre><code>prefer_openssl3\nexport PYTHON_BUILD_CONFIGURE_WITH_OPENSSL=1\ninstall_package \"openssl-3.3.2\" \"https://github.com/openssl/openssl/releases/download/openssl-3.3.2/openssl-3.3.2.tar.gz#2e8a40b01979afe8be0bbfb3de5dc1c6709fedb46d6c89c10da114ab5fc3d281\" mac_openssl --if has_broken_mac_openssl\ninstall_package \"readline-8.2\" \"https://ftpmirror.gnu.org/readline/readline-8.2.tar.gz#3feb7171f16a84ee82ca18a36d7b9be109a52c04f492a053331d7d1095007c35\" mac_readline --if has_broken_mac_readline\nif has_tar_xz_support; then\n    install_package \"Python-3.12.7\" \"https://www.python.org/ftp/python/3.12.7/Python-3.12.7.tar.xz#24887b92e2afd4a2ac602419ad4b596372f67ac9b077190f459aba390faf5550\" standard verify_py312 copy_python_gdb ensurepip\nelse\n    install_package \"Python-3.12.7\" \"https://www.python.org/ftp/python/3.12.7/Python-3.12.7.tgz#73ac8fe780227bf371add8373c3079f42a0dc62deff8d612cd15a618082ab623\" standard verify_py312 copy_python_gdb ensurepip\nfi\n</code></pre> <p>You can notice the script downloads the source from <code>www.python.org</code>, and compile it locally. </p> <p>If you want to change the source, you need to modify this config file. Below is an example.</p> <p>Here we suppose that we have a private python source server under <code>python-src.casd.local</code>.</p> <pre><code>install_package \"Python-3.12.7\" \"http://python-src.casd.local/Python-3.12.7.tgz#73ac8fe780227bf371add8373c3079f42a0dc62deff8d612cd15a618082ab623\" standard verify_py312 copy_python_gdb ensurepip\n</code></pre> <p>The number after # in <code>Python-3.12.7.tgz#73ac8fe780227bf371add8373c3079f42a0dc62deff8d612cd15a618082ab623</code> is the checksum value of the python source with <code>SHA256</code>. You can find the corresponding value in this page</p> <p>Below is an extraction of the origin json file</p> <pre><code>\"SPDXID\": \"SPDXRef-PACKAGE-cpython\",\n      \"checksums\": [\n        {\n          \"algorithm\": \"SHA256\",\n          \"checksumValue\": \"73ac8fe780227bf371add8373c3079f42a0dc62deff8d612cd15a618082ab623\"\n        }\n      ],\n      \"downloadLocation\": \"https://www.python.org/ftp/python/3.12.7/Python-3.12.7.tgz\",\n</code></pre>"},{"location":"python/01.Pyenv-offline-config/#31-add-new-python-version","title":"3.1 Add new python version","text":"<p>There are two steps: 1. Download the python source file in the <code>python-src.casd.local</code> server 2. Update the pyenv available python build list.</p>"},{"location":"python/01.Pyenv-offline-config/#311-download-the-source","title":"3.1.1 Download the source","text":"<p>You can find all the official release here</p> <p>In our current setup. The server <code>python-src.casd.local</code> is located at <code>10.50.5.65</code>. </p> <p>The source is located at <code>/data/python</code>. And the <code>/data</code> is a mounted disk which is not in <code>fstab</code>. Need to mount after restart</p>"},{"location":"python/01.Pyenv-offline-config/#312-update-the-pyenv-python-build-list","title":"3.1.2 Update the pyenv python build list","text":"<p>All the available build configuration file are located at {pyenv-root}/.pyenv/plugins/python-build/share/python-build</p> <p>The easiest way to add new source is to modify the origin pyenv conf, just change the official python source url to your  private python source url.</p> <p>If the python version is new, and the pyenv does not have an origin config file. You can use the below template to add a new version</p> <pre><code>install_package \"Python-3.x.y\" \"http://python-src.casd.local/Python-3.x.y.tgz#&lt;checksum of Python-3.x.y&gt;\" standard verify_py3x copy_python_gdb ensurepip\n</code></pre> <p>To get the check sum, you can goto the spdx page https://www.python.org/ftp/python/3.x.y/Python-3.x.y.tgz.spdx.json</p> <p>Then search text such as <code>https://www.python.org/ftp/python/3.x.y/Python-3.x.y.tgz</code></p> <p>For example, for <code>3.13.0</code>, when you search <code>https://www.python.org/ftp/python/3.13.0/Python-3.13.0.tgz</code>, you should find the below lines. Take the checksum and put it behind the <code>#</code></p> <pre><code>\"SPDXID\": \"SPDXRef-PACKAGE-cpython\",\n      \"checksums\": [\n        {\n          \"algorithm\": \"SHA256\",\n          \"checksumValue\": \"12445c7b3db3126c41190bfdc1c8239c39c719404e844babbd015a1bc3fafcd4\"\n        }\n      ],\n      \"downloadLocation\": \"https://www.python.org/ftp/python/3.13.0/Python-3.13.0.tgz\",\n</code></pre>"},{"location":"python/01.Pyenv-offline-config/#32-missing-system-dependencies","title":"3.2 Missing system dependencies","text":"<p>As we mentioned before, the source will be compiled locally, so it requires some system dependencies </p> <pre><code>sudo apt install libsqlite3-dev\nsudo apt-get install libffi-dev\nsudo apt-get build-dep python-tk\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/","title":"Use Bandersnatch to build a private pypi repository","text":"<p>Bandersnatch is a PyPI mirror client according to <code>PEP 381 + PEP 503 + PEP 691</code>. - PEP 381: Mirroring infrastructure for PyPI - PEP 691: JSON-based Simple API for Python Package Indexes Version features:   * bandersnatch &gt;=6.0 implements PEP691   * bandersnatch &gt;=4.0 supports Linux, MacOSX + Windows</p> <p>In this tutorial, we will use bandersnatch to mirror some python packages and server them as a <code>private pypi server</code>. The official GitHub repo of bandersnatch. The official doc is here</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#1-install-bandersnatch","title":"1. Install bandersnatch","text":"<p>It's recommended to install bandersnatch on a virtual env with a reserved uid</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#11-prepare-a-python-venv","title":"1.1 Prepare a Python venv","text":"<p>To Run below command, the user need to have sudoer right</p> <pre><code>sudo apt update\n\n# install python interpreter\nsudo apt-get install python3 -y\n\n# install pip\nsudo apt-get install python3-pip\n\n# install venv\nsudo apt-get install python3-venv\n\n# create a user bandersnatch\nsudo useradd -m bandersnatch -s /bin/bash\n\n# change current user to bandersnatch\nsudo su bandersnatch\n\n# go to the folder which you want to create the venv.\n# in this tutorial we choose ~/pypi/venv, to combine with our cron job script\nsudo mkdir -p ~/pypi/venv\n\n# change owner if you need, but optional\nsudo chown -R bandersnatch:root ~/pypi/venv\n\n# goto the venv folder\ncd ~/pypi/venv\n\n# create  a virtual env called bandersnatch\n# don't use sudo here\npython3 -m venv bandersnatch\n\n# test the virtual env\n# activate the venv\nsource bandersnatch/bin/activate\n\n# to exit the venv, just type \ndeactivate\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#12-install-the-package","title":"1.2  Install the package","text":"<pre><code># install the package\npip install bandersnatch\n\n# test the package\nbandersnatch --help\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#2-configure-bandersnatch","title":"2. Configure bandersnatch","text":"<p>When we run the command <code>bandersnatch mirror</code>, it will call a conf file that controls the mirror's behavior. By default, this file is located at /etc/bandersnatch.conf. And the default user may not have the right to create or edit this file.</p> <p>To avoid the access rights problem, we recommend you use the below config file path to host the config</p> <p>To run the below commands, we suppose your uid is bandersnatch</p> <pre><code># create a folder to host bandersnatch conf\nmkdir -p ~/pypi/conf\n\n# create a folder to host the mirrored packages\nmkdir -p ~/pypi/data\n\n# create a folder to host the log\nmkdir -p ~/pypi/log\n\n# create a folder to host the releases\nmkdir -p ~/pypi/export\n\n# create the config file\nvim ~/pypi/conf/bandersnatch.conf\n</code></pre> <p>Run <code>bandersnatch mirror</code> for the first time - it will create an empty configuration file for you in /etc/bandersnatch.conf. If you add some packages in it and run second time, It will populate your mirror with the current status of all PyPI packages. It takes many disk storage and long time to finish if you take all pypi packages. </p> <p>This page https://pypi.org/stats/ will give you an idea how much storage you need to complete the mirror. For mirror the top 100 projects, it requires 23.6TB.</p> <p>Now, put the below config file content in it.</p> <p>The default location should be <code>/etc/bandersnatch.conf</code>. But you can put this config file where you like. For example, in this tutorial, I put it in here <code>/home/bandersnatch/pypi/conf/bandersnatch.conf</code></p> <pre><code>[mirror]\njson = true\ndirectory = /home/bandersnatch/pypi/data\nmaster = https://pypi.org\nworkers = 4\ntimeout = 40\nstop-on-error= false\nhash-index = false\n\n[plugins]\nenabled =\n  exclude_platform\n  allowlist_project\n  allowlist_release\n\n[allowlist]\npackages =\n    pip\n    pandas\n    folium\n\n[blocklist]\nplatforms =\n     macos\n     freebsd\n     py2.4\n     py2.5\n     py2.6\n     py2.7\n</code></pre> <p>You will notice if you only mirror the above packages, you can not do the pip install correctly. Because packages also has dependencies. For example pandas require numpy to work, if you have pandas, but not numpy, pip install will fail. So you need to sync all the dependent packages of the required packages. To determine the complete package list is another challenge which we will address in another doc.</p> <p>Now we can start the mirroring of the packages</p> <pre><code># don't forget to activate the virtual env which contains the bandersnatch binary\n\n# in our case, we provide the path of the config file \nbandersnatch -c /home/bandersnatch/pypi/conf/bandersnatch.conf mirror\n</code></pre> <p>After running the above command, you will notice bandersnatch start to download the packages. It will create a  subdirectory under <code>/home/bandersnatch/pypi/data</code> called <code>web/</code>.  </p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#3-serve-the-package-as-a-web-server","title":"3. Serve the package as a web server","text":"<p>After the <code>bandersnatch mirror</code> command finished, you should see the following file and directory generated in the target folder which are defined in the <code>bandersnatch.conf</code>(directory = /home/bandersnatch/pypi/data).  - generation (file) - status (file) - web (directory which contains all the packages and index)</p> <p>All the packages are located in the <code>web</code> folder.</p> <p>To server the packages, we need to set up a web server. The configuration of the web server must respect <code>PEP691 support</code>. In this tutorial, we only show a nginx config example. Below is an example of the config. Here we suppose that pypi.casd.local is the server url.</p> <p>```nginx configuration server {     listen 80;     server_name pypi.casd.local;</p> <pre><code># redirect http request to https\nreturn 301 https://$host$request_uri;\n\n}\n</code></pre> <p>server {     listen 443 ssl;     server_name pypi.casd.local;     ssl_certificate /etc/ssl/certs/casd_k8s_wildcard.pem;     ssl_certificate_key /etc/ssl/private/wildcard_key.pem;</p> <pre><code>root /data/pypi/data/web;\nautoindex off;\ncharset utf-8;\nautoindex_exact_size off;\n\nlocation / {\n    try_files $uri $uri/ =404;\n}\n\nlocation /simple {\n    # Required for simple index files (like /simple/package-name)\n    try_files $uri $uri/ =404;\n}\n\n# Add caching headers (optional, helps with performance)\nlocation ~* \\.(whl|tar\\.gz|zip)$ {\n    expires 30d;\n    add_header Cache-Control \"public\";\n}\n\n# Enable gzip compression for faster responses\ngzip on;\ngzip_types text/plain application/xml application/json;\ngzip_proxied any;\n\n# Logging (optional)\naccess_log /var/log/nginx/pypi_access.log;\nerror_log /var/log/nginx/pypi_error.log;\n</code></pre> <p>}</p> <pre><code>Note that it is a good idea to have your webserver publish the HTML index files correctly with `UTF-8` as the charset. \nThe index pages will work without it but if humans look at the pages the characters will end up looking funny.\n\nMake sure that the webserver uses UTF-8 to look up Unicode path names. `nginx gets this right by default - not sure about others`.\n\n\n&gt; By default, pip only accept URL with **https**,  if your URL is in HTTP, you will have many problems. \n&gt; So we highly recommend you to add a certificate to your nginx config\n\n## 4. Test the Bandersnatch installation\n\n```shell\ncurl https://pypi.casd.local/simple\n</code></pre> <p>If you see the responce, then the server is up and running</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#5-configure-pip-to-use-the-private-repo","title":"5 Configure pip to use the private repo","text":"<p>pip uses a conf file to list all the hosts which it can download the packages, if your private repo is not in this list pip will consider it as dangerous.</p> <p>The configuration of pip is different based on the pip version and OS.</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#51-temporary-config","title":"5.1 Temporary Config","text":"<p>To test your pypi server, you can use the below command </p> <pre><code># general form, --index-url specifies where to lookup packages index\n# trusted-host: Marks your custom mirror as trusted. If your pypi server runs on http or the certificate is self-signed. It's mandatory to have this\npip install &lt;package-name&gt; --index-url &lt;repo-url&gt; --trusted-host &lt;repo-domain&gt;\n\n# for example, if the repo domain is pypi.casd.local and the repo URL is https://pypi.casd.local/simple\npip install pandas --index-url https://pypi.casd.local/simple --trusted-host pypi.casd.local\npip install ipykernel --index-url https://pypi.casd.local/simple --trusted-host pypi.casd.local\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#52-permanent-config","title":"5.2 Permanent Config","text":"<p>You can find the official pip config documentation here</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#521-use-the-pipconf-recommended","title":"5.2.1 Use the pip.conf (Recommended)","text":"<p>pip has 3 \u201clevels\u201d of configuration files:</p> <ul> <li><code>global</code>: system-wide configuration file, shared across users. The path is <code>/etc/pip.conf</code></li> <li><code>user</code>: per-user configuration file. </li> <li><code>site</code>: per-environment configuration file; i.e. per-virtualenv.</li> </ul> <p>Below is an example of pip.conf. You need to place it in different path based on your requirements(e.g. global, user,)</p> <pre><code>[global]\ntimeout = 60\nindex-url = https://pypi.casd.local/simple\ntrusted-host = pypi.casd.local\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#5211-linux-config","title":"5.2.1.1 Linux config","text":"<p>The path of the pip config file in linux os:  - <code>global</code>: <code>/etc/pip.conf</code>  - <code>user</code>: <code>$HOME/.config/pip/pip.conf</code>  - <code>site</code>: <code>path/to/venv/pip.conf</code></p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#5212-windows-path","title":"5.2.1.2 windows path","text":"<p>The path of the pip config file in Windows os:  - <code>global</code>: <code>C:\\ProgramData\\pip\\pip.ini</code> (win7 and later, hidden but writeable)  - <code>user</code>: <code>%APPDATA%\\pip\\pip.ini</code>  - <code>site</code>: <code>%VIRTUAL_ENV%\\pip.ini</code></p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#522-use-the-pip-cli","title":"5.2.2 Use the pip CLI","text":"<p><code>pip</code> provides command line client which allows you to config pip. Base on the <code>pip</code> version, the command is a little different.</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#5221-for-pip-100","title":"5.2.2.1  For pip &gt;= 10.0","text":"<pre><code># set the index server url\npip config set global.index-url http://pypi.casd.local/simple \n\n# If the server is in HTTP, you need to add the below line to force pip to accept the domain\npip config set global.trusted-host pypi.casd.local\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#5222-for-older-version","title":"5.2.2.2 For older version","text":"<pre><code>pip install --upgrade pip --index-url http://pypi.casd.local/simple --trusted-host pypi.casd.local\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#523-use-the-env-var","title":"5.2.3 Use the env var","text":"<p>For bash:</p> <pre><code>export PIP_INDEX_URL=http://pypi.yourdomain.com/simple\nexport PIP_TRUSTED_HOST=pypi.yourdomain.com\n</code></pre> <p>For cmd windows:</p> <pre><code>set PIP_INDEX_URL=http://pypi.yourdomain.com/simple\nset PIP_TRUSTED_HOST=pypi.yourdomain.com\n</code></pre> <p>For powershell windows:</p> <pre><code>$env:PIP_INDEX_URL = \"http://pypi.yourdomain.com/simple\"\n$env:PIP_TRUSTED_HOST = \"pypi.yourdomain.com\"\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#53-test-your-pip-config","title":"5.3 Test your pip config","text":"<p>After the configuration, you can check if your pip configuration</p> <pre><code># show all pip config\npip config list\n</code></pre> <p>note the pip config may be different from one venv to another.  the best practice is to activate the venv then check the pip config</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#54-for-conda-virtual-env","title":"5.4 For conda virtual env","text":"<p>As conda also uses pip to install python packages, you can use the above pip config. </p> <p>To locate where is the conda env, you can use the below command</p> <pre><code># step 1: find the virtual env root path\nconda info --envs\n## You should see below output\n# conda environments:\n#\nbase                  *  /home/pengfei/anaconda3\nPyQT-CRUD-App            /home/pengfei/anaconda3/envs/PyQT-CRUD-App\n\n# step 2: create the pip.conf file\nvim /home/pengfei/anaconda3/envs/PyQT-CRUD-App/pip.conf\n\n# step3: add content to pip.conf\n[global]\nindex-url = https://pengfei.org/simple\ntrusted-host = pengfei.org\n\n# step4: check the conf\nconda activate PyQT-CRUD-App\n\npip config list\n# you should see below output\nglobal.index-url='https://pengfei.org/simple'\nglobal.trusted-host='pengfei.org'\n</code></pre>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#6-troubles-in-bandersnatch","title":"6. Troubles in bandersnatch","text":""},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#61-mirrored-files","title":"6.1 mirrored-files","text":"<p>Each time when you run bandersnatch mirror command, it will create a file called mirrored-files in the current directory.</p> <p>If you changed directory and run bandersnatch mirror command again, it will not be able to locate previous mirrored-files.  And it will not start the mirror process. As a result, no change will make.</p> <p>You can use the command bandersnatch mirror --force-check to overcome this. But sometimes, it does not work,  you need to remove the wrongly generated mirrored-files file, then rerun bandersnatch mirror --force-check</p>"},{"location":"python/02.Use_bandersnatch_to_build_private_pypi_repo/#62-no-auto-dependencies-checks","title":"6.2 No auto dependencies checks","text":"<p>There is no automatic dependencies checks. For example in the allow_list, if I put pandas, bandersnatch  will only mirror the package source or wheel file of pandas. All the pandas dependencies (e.g. python-dateutil, numpy, six)  will not be mirrored. When you installed, pip will show the error that it can find numpy, etc.</p> <p>So we need to determine the dependencies manually and add all dependencies to the allow_list.  </p> <p>The maintainer of the bandersnatch said, this will not be fixed. https://github.com/pypa/bandersnatch/issues/1472</p>"},{"location":"python/04.Install_python_debian/","title":"Install python on debian","text":"<p>Unlike ubuntu, debian does not provide a python interpreter by default. So we need to install it by ourself.</p> <p>In this tutorial, we will build the python from source</p>"},{"location":"python/04.Install_python_debian/#get-the-python-source","title":"Get the python source","text":"<p>You can download the python source from their official site.</p> <p>For example, I download the 3.10.7 with below command</p> <pre><code>wget https://www.python.org/ftp/python/3.10.7/Python-3.10.7.tgz\n\ntar -xzvf Python-3.10.7.tgz\n</code></pre>"},{"location":"python/04.Install_python_debian/#build-from-source","title":"Build from source","text":"<pre><code>cd Python-3.10.7\n\n# setup configure\n./configure --enable-optimizations\n\n# start the build\n# The value  -j 2 represents the number of CPU cores that can be used  \nsudo make -j 2\n\n# Check it with nproc command.\nnproc\n# it should returns 2\n\n# install the binary\nsudo make altinstall\n\n# if everythin works well. You should be able to run\npython3 -V\nPython 3.10.7\n</code></pre> <p>The <code>-\u2013enable-optimizations</code> flag optimizes the Python binary and executes multiple tests.</p>"},{"location":"python/04.Install_python_debian/#upgrade-pip-and-install-package","title":"Upgrade pip and install package","text":"<pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install &lt;module_name&gt;\n</code></pre>"},{"location":"python/pypi_server/01.introduction/","title":"Build a private pypi server","text":"<p>To run onyxia without internet, we need to build a private pypi server which can respond all the service which need to run <code>pip install</code>.</p> <p>To build a private pypi server, we need three modules: 1. download(mirror) the required python packages from official pypi server, and build the local package index 2. Serve the local index and the packages behind it. 3. A cron job which sync the private server with the official server.</p>"},{"location":"python/pypi_server/01.introduction/#existing-solutions","title":"Existing solutions:","text":"<p>For now, the below list show the available tools   * pypi-server: https://github.com/pypiserver/pypiserver   * devpi: https://github.com/devpi/devpi (focus on releasing private python package)   * bandersnatch: https://github.com/pypa/bandersnatch (focus on mirroring the official repo )   * localshop: https://github.com/jazzband/localshop (too old, no release since 2015)   * JFog Artifactory: https://jfrog.com/artifactory (commercial license, the free version is too broken to meet our requirements)   * Warehouse: https://warehouse.pypa.io/application.html (The official codebase of pypi server)</p>"},{"location":"python/pypi_server/01.introduction/#https-certificates","title":"Https certificates","text":"<p>The pip client does not use the system trust store. So to make pip to accept a not trusted certificate is not simple.</p> <p>The official doc can be found  https://pip.pypa.io/en/stable/topics/https-certificates/</p>"},{"location":"python/pypi_server/01.introduction/#dependency-resolution-of-pip","title":"Dependency resolution of pip","text":"<p>https://pip.pypa.io/en/latest/topics/dependency-resolution/</p>"},{"location":"python/pypi_server/01.introduction/#our-solution","title":"Our solution","text":"<p>For now we choose Bandersnatch to build our private pypi repository. Because it offers a mirror client and can build the package index for the private pypi server.  Bandersnatch also implments the <code>PEP 381 + PEP 503 + PEP 691</code> and PEP381.</p> <p>Version features:   * bandersnatch &gt;=6.0 implements PEP691   * bandersnatch &gt;=4.0 supports Linux, MacOSX + Windows</p> <p>The official documentation can be foun here</p>"},{"location":"python/pypi_server/02.bandersnatch_install/","title":"Install and configure bandersnatch","text":"<p>It's recommended to install bandersnatch on a virtual env with a reserved uid </p>"},{"location":"python/pypi_server/02.bandersnatch_install/#0-prepare-a-python-venv","title":"0. Prepare a python venv","text":"<p>To Run below command, the user need to have sudoer right</p> <pre><code>sudo apt update\n\n# install python interpreter\nsudo apt-get install python3 -y\n\n# install pip\nsudo apt-get install python3-pip\n\n# install venv\nsudo apt-get install python3-venv\n\n# create a user bandersnatch\nsudo useradd -m bandersnatch -s /bin/bash\n\n# change current user to bandersnatch\nsudo su bandersnatch\n\n# go to the folder which you want to create the venv.\n# in this tutorial we choose ~/pypi/venv, to combine with our cron job script\nsudo mkdir -p ~/pypi/venv\n\n# change owner if you need, but optional\nsudo chown -R bandersnatch:root ~/pypi/venv\n\n# goto the venv folder\ncd ~/pypi/venv\n\n# create  a virtual env called bandersnatch\n# don't use sudo here\npython3 -m venv bandersnatch\n\n# test the virtual env\n# activate the venv\nsource bandersnatch/bin/activate\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#2-install-the-package","title":"2. Install the package","text":"<pre><code># install the package\npip install bandersnatch\n\n# test the package\nbandersnatch --help\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#3-config-bandersnatch","title":"3. Config bandersnatch","text":"<p>When we run the command <code>bandersnatch mirror</code>, it will call a conf file which controls the behavior of the mirror. By default, this file is located at /etc/bandersnatch.conf. And the default user may not have the right to create or edit this file.</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#31-config-file-introduction","title":"3.1 Config file introduction","text":"<p>In general, we can divide it into three parts, such as:   * The main mirror config   * The filter plugin config   * The log config</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#311-the-main-mirror-config","title":"3.1.1 The main mirror config","text":"<p>The official doc can be found Here</p> <p>In this section, we define the main function of the mirror, such as:   * json :  json setting is a boolean (true/false) setting that indicates that the json packaging metadata should be mirrored in addition to the packages (mandatory)   * workers: The workers value is an integer from <code>1-10</code> that indicates the number of concurrent downloads. (mandatory)   * timeout: The timeout value is an integer that indicates the maximum number of seconds for web requests. The default value for this setting is 10 seconds. (mandatory)   * hash-index:  The hash-index is a boolean (true/false) to determine if package hashing should be used. The Recommended value is  false for full pip/pypi compatibility. (mandatory)   * stop-on-error: The stop-on-error setting is a boolean (true/false) setting that indicates if bandersnatch should stop immediately if it encounters an error. (mandatory)   * directory: it indicates  the directory path to store the mirror files   * master: set the upstream of the mirror. The master url string must use https   * log-config: The log-config setting is a string containing the filename of a python logging configuration file.</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#322-the-filtering-plugin-config","title":"3.2.2 The filtering plugin config","text":"<p>The filtering plugin config allows you to define a blocklist and allowlist to filter python package with:   * project name   * release    * platforms   * size</p> <p>For filtering platforms,   * available os values are: windows, macos, freebsd, linux   * available python versions are: py2.4 ~ py2.7, py3.1 ~ py3.10</p> <pre><code>[plugins]\nenabled =\n    exclude_platform\n    allowlist_project\n    allowlist_release\n\n[allowlist]\npackages =\n    pandas==2.0.2\n\n[blocklist]\nplatforms = \n       macos\n       freebsd\n       py2.4\n       py2.5\n       py2.6\n       py2.7\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#323-the-log-config","title":"3.2.3 The log config","text":"<p>The log config defines where to put the output log</p> <pre><code>[logging]\ndirectory = /path/to/logs\n</code></pre> <p>The log format config file is located at /etc/bandersnatch-log.conf</p> <pre><code>[loggers]\nkeys=root,file\n[handlers]\nkeys=root,file\n[formatters]\nkeys=common\n[logger_root]\nlevel=NOTSET\nhandlers=root\n[logger_file]\nlevel=INFO\nhandlers=file\npropagate=1\nqualname=bandersnatch\n[formatter_common]\nformat=%(asctime)s %(name)-12s: %(levelname)s %(message)s\n[handler_root]\nclass=StreamHandler\nlevel=DEBUG\nformatter=common\nargs=(sys.stdout,)\n[handler_file]\nclass=handlers.TimedRotatingFileHandler\nlevel=DEBUG\nformatter=common\ndelay=False\nargs=('/repo/bandersnatch/banderlogfile.log', 'D', 1, 0)\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#32-a-example-of-a-simple-conf","title":"3.2  A example of a simple conf","text":"<p>In the below config example /etc/bandersnatch.conf, we will mirror <code>pandas,folium</code> package from pypi.org for the windows and linux OS and for python 3.8, 3.9, 3.10</p> <pre><code>[mirror]\njson = true\ndirectory = /home/bandersnatch/pypi/data\nmaster = https://pypi.org\nworkers = 4\ntimeout = 40\nstop-on-error= false\nhash-index = false\n\n[plugins]\nenabled =\n  exclude_platform\n  allowlist_project\n  allowlist_release\n\n[allowlist]\npackages =\n    pip\n    pandas\n    folium\n\n[blocklist]\nplatforms =\n     macos\n     freebsd\n     py2.4\n     py2.5\n     py2.6\n     py2.7\n\n\n[logging]\ndirectory = /home/bandersnatch/pypi/log\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#33-standard-config-for-the-cron-job","title":"3.3 Standard config for the cron job","text":"<p>To avoid access rights problem, we recommend you use the below config file path to host the config</p> <pre><code>mkdir -p ~/pypi/conf\nvim ~/pypi/conf/bandersnatch.conf\n</code></pre> <p>And put the above config file content in it.</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#34-test-your-installation","title":"3.4 Test your installation","text":"<pre><code># activate the venv if it's not activated\nsource ~/pypi/venv/bandersnatch/bin/activate\n\n# start the mirror process\nbandersnatch -c ~/pypi/conf/bandersnatch.conf mirror\n</code></pre> <p>After the above command finished, you should see the following file and directory - generation (file) - status (file) - web (directory which contains all the packages and index)</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#4-serve-the-package","title":"4. Serve the package","text":"<p>Once the <code>bandersnatch mirror</code> command succeeds, you\u2019re now ready to serve your mirror. Any webserver can do this, as long as it can serve the simple HTML and packages directory that the HTML links to.</p> <p>In a production environment, we recommand you to use nginx as the web server.</p> <p>In a dev environment, you can use <code>python simple http server</code></p>"},{"location":"python/pypi_server/02.bandersnatch_install/#41-use-nginx-to-serve-the-package","title":"4.1 Use nginx to serve the package","text":"<pre><code># install\nsudo apt install nginx\n\n# Start Nginx: Once the installation is complete, Nginx should start automatically. If not, you can start it manually by executing the following command\nsudo systemctl start nginx\n</code></pre> <p>Create a nginx server conf for to serve the package. We name the conf file as pypi-repo.conf. Put the below file in <code>/etc/nginx/sites-available</code></p> <pre><code>server {\n    listen 80;\n    server_name pypi.casd.local;\n\n    # redirect http request to https\n    return 301 https://$host$request_uri;\n\n}\n\n\nserver {\n    listen 443 ssl;\n    ssl_certificate /etc/ssl/certs/casd_k8s_wildcard.pem;\n    ssl_certificate_key /etc/ssl/private/casd_k8s_wildcard_key.pem;\n    server_name pypi.casd.local;\n\n    # root /package-repo/pypi/data/web;\n\n    location / {\n        root /package-repo/pypi/data/web;\n        autoindex on;\n        charset utf-8;\n        autoindex_exact_size off;\n        try_files $uri $uri/ =404;\n    }\n}\n</code></pre> <p>Now let's enable the conf</p> <pre><code># create a soft link \nsudo ln -s /etc/nginx/sites-available/pypi-repo.conf /etc/nginx/sites-enabled/pypi-repo.conf\n\n# check the validity\nsudo nginx -t\n\n# restart the server\nsudo systemctl restart nginx\n</code></pre> <p>Now you should be able to open  <code>http://pypi.casd.local/simple/pandas/</code> to check all available pandas sources</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#42-use-python-simple-http-server","title":"4.2 Use python simple http server","text":""},{"location":"python/pypi_server/02.bandersnatch_install/#5-configure-pip-to-use-the-private-repo","title":"5. Configure pip to use the private repo","text":"<p>By default, pip only accept URL with https,  if your URL is in HTTP, you will have many problems. So we highly recommend you to add a certificate</p> <p>The configuration of pip is different based on the pip version</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#51-temporary-config","title":"5.1 Temporary Config","text":"<pre><code>pip install &lt;package-name&gt; --index-url &lt;repo-url&gt; --trusted-host &lt;repo-domain&gt;\n\n# for example, if the repo domain is pypi.casd.local and the repo URL is http://pypi.casd.local/simple\npip install pandas --index-url http://pypi.casd.local/simple --trusted-host pypi.casd.local\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#52-permenent-config","title":"5.2 Permenent Config","text":"<p>You can use the <code>pip config</code> command to configure permenently the pip behaviour. The official doc of the pip config.</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#521-for-pip-100","title":"5.2.1 For pip &gt;= 10.0","text":"<pre><code># set the index server url\npip config set global.index-url http://pypi.casd.local/simple \n\n# If the server is in HTTP, you need to add the below line to force pip to accept the domain\npip config set global.trusted-host pypi.casd.local\n\n# check the current config file content\npip config list\n</code></pre> <p>This command will create a file <code>pip.conf</code> in the current user home (e.g. <code>$HOME/.config/pip/pip.conf</code>).</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#522-for-older-version","title":"5.2.2 For older version","text":"<pre><code>pip install --upgrade pip --index-url http://pypi.casd.local/simple --trusted-host pypi.casd.local\n</code></pre>"},{"location":"python/pypi_server/02.bandersnatch_install/#523-edit-the-config-file-directly","title":"5.2.3 Edit the config file directly","text":"<p>You can find the official doc here</p> <p>pip has 3 \u201clevels\u201d of configuration files:</p> <ul> <li> <p>global: system-wide configuration file, shared across users.</p> </li> <li> <p>user: per-user configuration file.</p> </li> <li> <p>site: per-environment configuration file; i.e. per-virtualenv</p> </li> </ul> <pre><code># open the pip.conf file, find the global section \n[global]\nindex-url = http://pypi.casd.local/simple\n</code></pre> <p>The path of the per-user configuration file can be different, below are some path example of the global configuration file:</p> <ul> <li>Linux: <code>/etc/pip.conf</code></li> <li>macOS: <code>/Library/Application Support/pip/pip.conf</code></li> <li>Windows: On Windows 7 and later: <code>C:\\ProgramData\\pip\\pip.ini</code> (hidden but writeable)</li> </ul>"},{"location":"python/pypi_server/02.bandersnatch_install/#6things-that-i-dont-like-in-bandersnatch","title":"6.Things that I don't like in bandersnatch","text":""},{"location":"python/pypi_server/02.bandersnatch_install/#61-mirrored-files","title":"6.1 mirrored-files","text":"<p>Each time when you run bandersnatch mirror command, it will create a file called mirrored-files in the current directory.</p> <p>If you changed directory and run bandersnatch mirror command again, it will not be able to locate previous mirrored-files. And it will not start the  mirror process. As a result, no change will make.</p> <p>You can use command bandersnatch mirror --force-check to overcome this. But sometimes, it does not work, you need to remove the wrongly generated mirrored-files file, then rerun bandersnatch mirror --force-check</p>"},{"location":"python/pypi_server/02.bandersnatch_install/#62-no-auto-dependencies-checks","title":"6.2 No auto dependencies checks","text":"<p>There is no automatic dependencies checks. For example in the allow_list, if I put pandas, bandersnatch will only mirror the package source or wheel file of pandas. All the pandas dependencies (e.g. python-dateutil, numpy, six) will not be mirrored. When you installed, pip will show the error that it can find numpy, etc.</p> <p>So we need to determine the dependencies manually and add all dependencies to the allow_list.  I just develop a litte python client to generate the dependencies automatically.</p> <p>You can find the repo here</p>"},{"location":"python/pypi_server/03.bandersnatch_cron/","title":"The bandersnatch cron job","text":"<p>There are three step in the cron job scripts: 1. Activate the virtual env that runs the <code>bandersnatch</code> and the <code>dependencies checker</code> 2. Run the dependencies checker to get the complete list of the packages with dependencies 3. Start the bandersnatch mirror process 4. zip the downloaded packages and index build.</p>"},{"location":"python/pypi_server/03.bandersnatch_cron/#1-the-cron-job-requirement","title":"1 The cron job requirement","text":"<p>The cron job requires the complete installation of the bandersnatch and the conf file is provided. Below shows the requirements which the installation needs to respect. </p> <ul> <li>has a valid venv that contains all the dependencies</li> <li>has the required folders</li> <li>has the dependencies checker script</li> </ul>"},{"location":"python/pypi_server/03.bandersnatch_cron/#11-the-required-folders","title":"1.1 The required folders","text":"<p>If you followed the previous tutorial to install the bandersnatch, your directory should be organized as below:</p> <pre><code>pypi\n    \u251c\u2500\u2500 bin: contains the dependencies checker script\n    \u251c\u2500\u2500 conf: contains the bandersnatch conf\n    \u251c\u2500\u2500 data: contains the mirrored packages and index \n    \u251c\u2500\u2500 export: the zipped release tagged with release date\n    \u251c\u2500\u2500 log: corn job log\n    \u2514\u2500\u2500 venv: contains the venv that allows the scripts to run\n</code></pre>"},{"location":"python/pypi_server/03.bandersnatch_cron/#12-the-required-system-dependencies","title":"1.2 The required system dependencies","text":"<p>The corn job requires zip to build the release, so you need to install it</p> <pre><code>sudo apt install zip -y\n</code></pre>"},{"location":"python/pypi_server/03.bandersnatch_cron/#13-the-dependencies-checker","title":"1.3 The dependencies checker ====","text":"<p>As I explained before, the Bandersnatch does not check the required dependencies for the mirrored package, so we write a script that can calculate the dependencies and update the Bandersnatch config file automatically. </p> <p>The repo git is here: https://github.com/CASD-EU/PyPiDepDetective</p> <p>This script requires below package to run, so you need to install it in the bandersnatch venv</p> <pre><code># change uid to bandersnatch\nsudo su bandersnatch\n\n# activate the venv if it's not activated\nsource ~/pypi/venv/bandersnatch/bin/activate\n\n# install the packages\npip install requests johnnydep\n\n# copy the above script in to below file\nvim ~/pypi/bin/detective.py\n\n# Test the script\npython3 ~/pypi/bin/detective.py -h\n\n# If you see the help page of the script, it means the installation is successful\n</code></pre>"},{"location":"python/pypi_server/03.bandersnatch_cron/#2-the-corn-job-script","title":"2. The corn job script","text":"<p>Now you have everything you need to run the cron job script. </p> <p>You can find the script here</p>"},{"location":"python/pypi_server/03.bandersnatch_cron/#3-test-the-corn-job-script","title":"3. Test the corn job script","text":""},{"location":"python/pypi_server/03.bandersnatch_cron/#31-to-test-the-script-without-crontab","title":"3.1 To test the script without crontab","text":"<pre><code>sudo su bandersnatch\n\nbash /path/to/pypi_build_release.sh\n</code></pre>"},{"location":"python/pypi_server/03.bandersnatch_cron/#32-to-test-the-script-with-crontab","title":"3.2 To test the script with crontab","text":"<p>There are two possible way to set up the cron job:   - set up cron job as root   - set up cron job as bandersnatch</p>"},{"location":"python/pypi_server/03.bandersnatch_cron/#321-as-root","title":"3.2.1 As root","text":"<p>If you run the cron job in the root crontab, you need to add the uid bandersnatch to specify that the script need to run with uid bandersnatch</p> <pre><code># the script will run at 6:00 am every Saturday\n0 6 * * 6 bandersnatch /path/to/pypi_build_release.sh\n</code></pre>"},{"location":"python/pypi_server/03.bandersnatch_cron/#322-as-bandersnatch","title":"3.2.2 As bandersnatch","text":"<p>If you run the cron job in the bandersnatch crontab, you don't need to add the uid bandersnatch</p> <pre><code># the script will run at 6:00 am every Saturday\n0 6 * * 6 /path/to/pypi_build_release.sh\n</code></pre>"},{"location":"security/kerberos/01.Introduction/","title":"Introduction of Kerberos server","text":"<p>Kerberos is a secure authentication protocol that enables users and services to <code>authenticate without transmitting  passwords</code> over the network. It is built around a <code>trusted third-party authentication system</code>, using  <code>tickets and encryption keys</code> to ensure authentication.</p> <p>The Kerberos server knows \"secrets\" (encrypted passwords) for all clients and servers under its control.  These \"secrets\" are used to encrypt all the messages exchanged during authentication.</p>"},{"location":"security/kerberos/01.Introduction/#1-key-components-and-terms-of-kerberos","title":"1. Key components and terms of Kerberos","text":"<p>Below is a list of important components and terms of Kerberos: - Key Distribution Center(KDC): is responsible for checking user credentials and issuing Tickets - Realm: defines a administrative domain to restrict a security scope. It includes a <code>KDC</code> and a <code>list of Clients(e.g. users, hosts, services)</code> - Principal: is a unique identity in the Kerberos system, which can be a <code>user, host, or service</code>. - Ticket: a credential issued by KDC. - Keytab: A file that includes one or more principals and their keys.   </p>"},{"location":"security/kerberos/01.Introduction/#11-key-distribution-centerkdc","title":"1.1 Key Distribution Center(KDC)","text":"<p>Key Distribution Center, or KDC is the heart of the Kerberos authentication system. At a high level, it has three parts:</p> <ul> <li>A database: of the users, hosts and services (known as principals) that it knows about and their respective Kerberos passwords </li> <li>An authentication server (AS): which performs the initial authentication and issues a Ticket Granting Ticket (TGT)</li> <li>A Ticket Granting Server (TGS): that issues <code>subsequent service tickets</code> based on the initial TGT for <code>accessing specific hosts or services</code></li> </ul> <p>The KDC is typically hosted on a <code>Domain Controller (DC) in Active Directory (AD)</code> or a <code>Kerberos server</code> in <code>MIT/Heimdal implementations</code>.</p>"},{"location":"security/kerberos/01.Introduction/#12-realm","title":"1.2 Realm","text":"<p>A Kerberos realm defines the administrative domain. It includes a <code>KDC</code> and a <code>list of Clients(e.g. users, hosts, services)</code> The <code>realm name</code> usually matches the <code>organization\u2019s domain name, written in uppercase</code>. For example, if the organization domain name is <code>casd.eu</code>, the realm name should be <code>CASD.EU</code>.</p> <p>A realm can trust another realm (cross-realm authentication) for authentication across different domains.</p>"},{"location":"security/kerberos/01.Introduction/#13-principal","title":"1.3 Principal","text":"<p>A Principal is a unique identity in the Kerberos system, which can be:</p> <ul> <li>a user (e.g., user@CASD.EU).</li> <li>a host (e.g., host/server01.casd.eu@CASD.EU).</li> <li>a service (e.g., hdfs/namenode.casd.eu@CASD.EU).</li> </ul> <p>We must distinguish the difference between an account in AD/Ldap and a principal in Kerberos. An account in AD can be associated with multiple principals in Kerberos.</p>"},{"location":"security/kerberos/01.Introduction/#131-naming-convention-of-the-principal","title":"1.3.1 Naming convention of the principal","text":"<p>In theory, A principal in Kerberos 5 is of the following type: <code>component1/component2/.../componentN@REALM</code></p> <p>But, in practice a <code>maximum of two components</code> are used. So we recommend the below naming convention for principals.</p> <p>Principal for users, we use the uid of the user and roles of the user to build the principal. In below example, user1 has two principals, <code>user1@CASD.EU</code> has normal privileges, <code>user1/admin@CASD.EU</code> has admin privileges</p> <pre><code># general form for user principal\n&lt;uid&gt;/&lt;role&gt;@REALM\n\n# examples\nuser1@CASD.EU\nuser1/admin@CASD.EU\n</code></pre> <p>Principal for hosts(servers), we use the keyword <code>host</code> to indicate this principal provides a special service which give generic access to the machine (e.g. telnet, rsh, ssh). The second component is the <code>complete hostname (FQDN)</code>  of the machine.</p> <pre><code># general form for host principal, \nhost/&lt;host-fqdn&gt;@REALM\n\n# examples\nhost/server.casd.eu@CASD.EU\n</code></pre> <p>It is important that the <code>host-fqdn</code> exactly matches (in lower case letters) the DNS reverse resolution  of the application server's IP address</p> <pre><code># general form for service principal\n&lt;service-name&gt;/&lt;host-fqdn&gt;@REALM\n\n# examples\nimap/mbox.casd.eu@CASD.EU\nyarn/deb11_h01.casd.eu@CASD.EU\n</code></pre>"},{"location":"security/kerberos/01.Introduction/#14-ticket","title":"1.4 Ticket","text":"<p>A <code>kerberos ticket</code> is something a client presents to an application server to demonstrate the authenticity of its identity.  Tickets are issued by the <code>authentication server</code> and are encrypted using the secret key of the service they are  intended for. Since this key is a secret shared only between the <code>authentication server</code> and the <code>server providing  the service</code>, not even the client which requested the ticket can know it or change its contents.  The main information contained in a ticket includes:  - The requesting user's principal (generally the username);   - The principal of the service it is intended for;   - The IP address of the client machine from which the ticket can be used. In Kerberos 5 this field is optional and may       also be multiple to be able to run clients under NAT or multi-home.   - The date and time (in timestamp format) when the tickets validity commences;   - The ticket's maximum lifetime   - The session key (this has a fundamental role which is described below);</p>"},{"location":"security/kerberos/01.Introduction/#141-tickets-maximum-lifetime","title":"1.4.1 Ticket's maximum lifetime","text":"<p>Each ticket has an expiration (generally 10 hours). This is essential since the authentication server no longer  has any control over an already issued ticket. Even though the realm administrator can prevent the issuing of  new tickets for a certain user at any time, it cannot prevent users from using the tickets they already possess.  This is the reason for limiting the lifetime of the tickets to limit any abuse over time.</p>"},{"location":"security/kerberos/01.Introduction/#142-ticket-granting-tickettgt-vs-service-ticketst","title":"1.4.2 Ticket Granting Ticket(TGT) VS Service Ticket(ST)","text":"<p>Kerberos provides two types of tickets: - Ticket Granting Ticket(TGT): A ticket that proves the user identity to request other tickets; issued once per login session. - Service Ticket(ST): A ticket that grants access to a specific service; issued per service request.</p> <p>A <code>user principal</code> requests authentication from the <code>AS</code>. The AS returns a <code>TGT</code> that is <code>encrypted using the user  principal's Kerberos password</code>, which is known only to the user principal and the AS. The user principal decrypts the  TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can  use the TGT to get <code>service tickets(ST)</code> from the <code>TGS</code>. </p>"},{"location":"security/kerberos/01.Introduction/#ticket-granting-ticket-tgt","title":"Ticket Granting Ticket (TGT)","text":"<ul> <li>Issued by the Authentication Service (AS) after a successful login.</li> <li>Encrypted with the user\u2019s password.</li> <li>Allows the user to request service tickets without entering credentials again.</li> <li>Has a validity period (e.g., 10 hours) and may be renewable.</li> </ul>"},{"location":"security/kerberos/01.Introduction/#service-ticket-st","title":"Service Ticket (ST)","text":"<ul> <li>Issued by the Ticket Granting Service (TGS) upon request.</li> <li>Allows access to a specific service (e.g., HDFS, Spark, SSH).</li> <li>Each service requires its own ticket.</li> <li>In general, the validity period of ST should be less than TGT </li> </ul>"},{"location":"security/kerberos/01.Introduction/#difference-between-a-ticket-granting-ticket-tgt-and-a-service-ticket-st","title":"Difference Between a Ticket Granting Ticket (TGT) and a Service Ticket (ST)","text":"Feature Ticket Granting Ticket (TGT) Service Ticket (ST) Purpose Proves the user\u2019s identity and allows them to request service tickets Grants access to a specific service (e.g., HDFS, Spark, SSH) Issued By Authentication Service (AS) within the KDC Ticket Granting Service (TGS) within the KDC Used For Requesting service tickets (STs) from the KDC Accessing a specific service Validity Period Typically valid for several hours (e.g., 10 hours) Shorter lifespan (usually same as or shorter than TGT) Encrypted With KDC\u2019s secret key Service\u2019s secret key Stored In User\u2019s credential cache (klist command shows it) Also stored in the credential cache but used for service authentication Authentication Flow First step in Kerberos authentication Second step after obtaining a TGT <p>Below is an example of viewing tickets in linux</p> <pre><code># example of viewing TGT and ST\nklist\n\n# example output\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: user@CASD.EU\n\nValid starting     Expires            Service principal\n03/25/25 12:00:00  03/25/25 22:00:00  krbtgt/CASD.EU@CASD.EU  (TGT)\n03/25/25 12:05:00  03/25/25 22:00:00  hdfs/namenode.casd.eu@CASD.EU (Service Ticket)\n</code></pre> <p>By default, in linux all krb tickets of a user are stored in a file /tmp/krb5cc_1000, where 1000 is the uid number  of the user.</p> <p>Below is an example of viewing tickets in windows</p> <pre><code># example of viewing TGT and ST\nklist\n\n# example output\nCredential cache: C:\\Users\\user\\krb5cc_user\nDefault principal: user@CASD.EU\n\nValid starting     Expires            Service principal\n03/25/25 12:00:00  03/25/25 22:00:00  krbtgt/CASD.EU@CASD.EU  (TGT)\n03/25/25 12:05:00  03/25/25 22:00:00  hdfs/namenode.casd.eu@CASD.EU (Service Ticket)\n</code></pre> <p>By default, in windows, all krb tickets of a user are stored in a file C:\\Users\\user\\krb5cc_user. </p> <ul> <li>The user principal is <code>user@CASD.EU</code></li> <li>The first ticket with service principal <code>(krbtgt/CASD.EU@CASD.EU)</code> is the TGT. </li> <li>The second ticket with service principal <code>(hdfs/namenode.casd.eu@CASD.EU)</code> is the Service Ticket for accessing HDFS.</li> </ul>"},{"location":"security/kerberos/01.Introduction/#15-keytab","title":"1.5 Keytab","text":"<p>A <code>.keytab</code> file stores the <code>encrypted credentials</code> for <code>non-interactive authentication</code>.</p> <p>It's often used by services and automated scripts to authenticate to Kerberos and request a ticket without user intervention.</p> <p>A <code>.keytab</code> file can contain <code>one or more encryption keys</code> linked to a Kerberos principal. </p>"},{"location":"security/kerberos/01.Introduction/#16-encryption-algos-and-keys","title":"1.6 Encryption algos and keys","text":"<p>As we explained before, Kerberos uses <code>symmetric encryption algorithms</code> to secure authentication (e.g. tickets and keytabs). It supports many algorithms to fits different hardware and OS requirements. Below are some supported algo - AES256-CTS-HMAC-SHA1-96 (Strongest, preferred) - AES128-CTS-HMAC-SHA1-96 - RC4-HMAC (Weaker, legacy)</p> <pre><code># to check the encryption algo of a ticket, you can use -e option\nklist -e \n</code></pre> <p>The algo <code>AES256-CTS-HMAC-SHA1-96</code> can be break down into three parts: - AES256: symmetric encryption algorithm which encrypts the shared secrets between user and KDC - CTS(Ciphertext Stealing): helps encrypt data of any size without an anomaly. Because encryption works on fixed blocks of data (e.g., 16 bytes at a time) - HMAC-SHA1-96: ensures the integrity of the ticket or credentials in the keytab file. HMAC (Hash-Based Message Authentication Code)</p>"},{"location":"security/kerberos/01.Introduction/#17-summary","title":"1.7 Summary","text":"<p>Below is a summary of import terms in Kerberos</p> Term Description Key Distribution Center(KDC) checking user credentials and issuing Tickets Realm The administrative domain that includes a KDC and a number of Clients. Principal The unique name of a user or service that authenticates against the KDC Ticket Granting Ticket(TGT) A ticket that proves the user identity to request other tickets Service Ticket(ST) A ticket that grants access to a specific service Keytab A file that includes one or more principals and their associated keys."},{"location":"security/kerberos/01.Introduction/#2-introduction-to-kerberos-authentication-protocol","title":"2. Introduction to Kerberos Authentication protocol","text":"<p>Kerberos is a network authentication protocol. It is designed to provide strong authentication for client/server  applications by using secret-key cryptography. It has the following characteristics:</p> <ul> <li> <p>It is secure: it never sends a password unless it is encrypted.</p> </li> <li> <p>Only a single login is required per session. Credentials defined at login are then passed between resources without the need for additional logins.</p> </li> <li> <p>The concept depends on a trusted third party  a Key Distribution Center (KDC). The KDC is aware of all systems in the network and is trusted by all of them.</p> </li> <li> <p>It performs mutual authentication, where a client proves its identity to a server, and a server proves its identity to the client.</p> </li> </ul> <p>Kerberos introduces the concept of a Ticket-Granting Server (TGS). A client that wishes to use a service has to  receive a ticket a time-limited cryptographic message giving it access to the server.  Kerberos also requires an Authentication Server (AS) to verify clients. The two servers combined make up a KDC. </p> <p>The following figure shows the sequence of events required for a client to gain access to a service using Kerberos  authentication. Each step is shown with the Kerberos message associated with it, as defined in RFC  4120 \u201cThe Kerberos Network Authorization Service (V5)\u201d.</p> <p></p> <ul> <li> <p>Step 1: The user logs on to the workstation and requests service on the host. The workstation sends a message to            the Authorization Server requesting a <code>ticket granting ticket (TGT)</code>.</p> </li> <li> <p>Step 2: The Authorization Server verifies the user\u2019s access rights in the user database and creates a TGT             and session key. The Authorization Sever encrypts the results using a key derived from the user\u2019s             password and sends a message back to the user workstation. The workstation prompts the user for a             password and uses the password to decrypt the incoming message. When decryption succeeds, the user will             be able to use the TGT to request a <code>service ticket</code>.</p> </li> <li> <p>Step 3: When the user wants access to a service, the workstation client application sends a request to the Ticket            Granting Service containing the client name, realm name and a timestamp. The user proves his identity by            sending an authenticator encrypted with the session key received in Step 2.</p> </li> <li> <p>Step 4: The TGS decrypts the ticket and authenticator, verifies the request, and creates a ticket for the             requested server. The ticket contains the client name and optionally the client IP address. It also              contains the realm name and ticket lifespan. The TGS returns the ticket to the user workstation.             The returned message contains two copies of a server session key  one encrypted with the client password,             and one encrypted by the service password.</p> </li> <li> <p>Step 5: The client application now sends a service request to the server containing the ticket received in Step            4 and an authenticator. The service authenticates the request by decrypting the session key. The server           verifies that the ticket and authenticator match, and then grants access to the service. This step as           described does not include the authorization performed by the Intel AMT device, as described later.</p> </li> <li> <p>Step 6: If mutual authentication is required, then the server will reply with a server authentication message.</p> </li> </ul>"},{"location":"security/kerberos/01.Introduction/#3-time-synchronization","title":"3. Time synchronization","text":"<p>To prevent <code>replay attacks</code>, Kerberos uses <code>timestamps</code> as part of its protocol definition. For timestamps to  work properly, the <code>clocks of the client and the server need to be in synch as much as possible</code>. Since the  clocks of two computers are often out of synch, administrators can establish a policy to establish the  maximum acceptable difference to Kerberos between a client's clock and server's clock. If the difference between  a client's clock and the server's clock is less than the maximum time difference specified in this policy, any timestamp used in a session between the two computers will be  considered authentic. The maximum difference is usually set to five minutes.</p> <p>Note that if a client application wishes to use a service that is \"Kerberized\" (the service is configured to perform    Kerberos authentication), the client must also be Kerberized so that it expects to support the necessary message responses. </p>"},{"location":"security/kerberos/01.Introduction/#reference","title":"Reference","text":"<ul> <li>https://software.intel.com/sites/manageability/AMT_Implementation_and_Reference_Guide/default.htm?turl=WordDocuments%2Fintroductiontokerberosauthentication.htm</li> <li>http://web.mit.edu/kerberos/www/.</li> </ul>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/","title":"Install Kerberos server and client on debian","text":"<p>In this tutorial, we will install MIT Kerberos server and client on a debian 11 server.</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#1-prerequisite","title":"1. Prerequisite","text":""},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#11-setup-ntp-server","title":"1.1 Setup NTP server","text":"<p>As we mentioned above, kerberos is time sensitive, we need to use a Network Time Protocol (NTP) server to synchronize  the time. You can follow this page https://vitux.com/how-to-setup-ntp-server-and-client-on-debian-11/</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#12-dns-server-setup","title":"1.2 DNS server setup","text":"<p>Before installing the Kerberos server, a properly configured DNS server is needed for your domain.  Since the Kerberos Realm by convention matches the domain name.</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#13-some-basic-krb-information","title":"1.3 Some basic krb information","text":"<p>Realm: CASD.LOCAL KDC server url: kdc01.casd.local (10.50.5.57) backup KDC server url: kdc02.casd.local (not deployed for now) admin principal: pliu/admin user principal: hadoop</p> <p>It is strongly recommended that your network-authenticated users have their uid in a different range (say, starting at 5000) than that of your local users.</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#2-server-installation","title":"2. Server Installation","text":"<p>For the server side, we will install an <code>MIT kerberos V5</code>. As we mentioned above, a KDC requires an Authentication Server (AS) and Ticket-Granting Server (TGS) To manage the KDC server, we need some admin tools which will be provided by the krb5-admin-server</p> <pre><code># step 1:  install the krb5-kdc and krb5-admin-server packages.\nsudo apt install krb5-kdc krb5-admin-server\n\n# 1st prompt will ask you the default REALM name, enter CASD.LOCAL\n# 2nd prompt will ask you the url/ip of the kdc server url, enter kdc01.casd.local or ip, if you have two kdc server, you can use space to separate them.\n# 3rd prompt will ask you the url/ip of the admin server url, enter kdc01.casd.local or ip(we install it on the same). \n\n# step2: create a new realm\n# The default realm in step 1 is the configuration for connection. We need to create the realm\nsudo krb5_newrealm\n# suppose the reaml name is CASD.LOCAL\n\n# step3: create principals(user account) by using admin tools\n# login to admin tools \nsudo kadmin.local\n\n# create new principals (user accounts)\n# note the / in pliu/admin here is only for human, nothing special for kdc.\n# as we did not specify the realm, the default realm will be added to the principals\nkadmin.local: addprinc pliu/admin\nkadmin.local: addprinc hadoop\n\n# step4: Add the appropriate Access Control List (ACL) permissions for the new admin user.\nsudo vim /etc/krb5kdc/kadm5.acl\n# add below lines\npliu/admin@CASD.LOCAL  *\n# The above line grants pliu/admin the ability to perform any operation on all principals in the realm. You can \n# configure principals with more restrictive privileges.\n\n# step5: restart the krb5-admin-server for the new ACL to take affect\nsudo systemctl restart krb5-admin-server.service\n</code></pre> <p>In this setup, the KDC uses a local database to store user login/password; we can connect kerberos to a ldap server.</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#3-server-configuration","title":"3. Server configuration","text":"<p>There are three main config files you need to pay attention: - /etc/krb5.conf - /etc/krb5kdc/kdc.conf - /etc/krb5kdc/kadm5.acl</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#31-etckrb5conf-kerberos-client-general-configuration","title":"3.1 /etc/krb5.conf (Kerberos Client &amp; General Configuration)","text":"<p>This configuration file is <code>required</code> by both <code>krb clients</code> and the <code>KDC server</code> to specify general <code>Kerberos settings</code>. It has three key sections: - <code>[libdefaults]</code>: General Kerberos settings (default realm, ticket options). - <code>[realms]</code>: Defines realms and their corresponding KDC and admin server. - <code>[domain_realm]</code>: Maps domain names to Kerberos realms.</p> <p>Below is an example of the <code>krb5.conf</code></p> <pre><code>[libdefaults]\n    default_realm = CASD.EU\n    ticket_lifetime = 24h\n    renew_lifetime = 7d\n    forwardable = true\n    dns_lookup_kdc = false\n\n[realms]\n    CASD.EU = {\n        kdc = kdc.casd.eu\n        admin_server = kdc.casd.eu\n        default_domain = casd.eu\n    }\n\n[domain_realm]\n    .casd.eu = CASD.EU\n    casd.eu = CASD.EU\n</code></pre>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#32-etckrb5kdckdcconf-kdc-specific-configuration","title":"3.2 /etc/krb5kdc/kdc.conf (KDC-Specific Configuration)","text":"<p>This configuration file is only used by the KDC (Key Distribution Center). It controls <code>how the KDC manages  authentication, tickets, and encryption</code>.</p> <p>It has three key section: - <code>[kdcdefaults]</code>: Defines which ports the KDC listens on. - <code>[realms]</code>: KDC-specific settings for managing tickets, database, and encryption of a domain.</p> <pre><code>[kdcdefaults]\n    kdc_ports = 88\n    kdc_tcp_ports = 88\n\n[realms]\n    CASD.EU = {\n        database_name = /var/lib/krb5kdc/principal\n        admin_keytab = /etc/krb5kdc/kadm5.keytab\n        acl_file = /etc/krb5kdc/kadm5.acl\n        dict_file = /usr/share/dict/words\n        key_stash_file = /etc/krb5kdc/stash\n        max_life = 24h\n        max_renewable_life = 7d\n        supported_enctypes = aes256-cts-hmac-sha1-96:normal aes128-cts-hmac-sha1-96:normal\n    }\n</code></pre>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#33-etckrb5kdckadm5acl-acl-for-kdc","title":"3.3 /etc/krb5kdc/kadm5.acl (ACL for KDC)","text":"<p>The <code>/etc/krb5kdc/kadm5.acl</code> file controls who can manage Kerberos principals (users, services, and hosts).</p> <pre><code># general form \nprincipal  privilege\n\n# some example\n# the admin principal has full admin access  (can add/delete principal, reset passwords, etc.)\nadmin@CASD.EU  *\n# user1 can change their own password but nothing else\nuser1@CASD.EU  x\n# service/admin can add and change principals, but not delete them\nservice/admin@CASD.EU  ac\n</code></pre> Symbol Privilege * Full access a Add principal d Delete principal m Modify principal c Change password x Change own password"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#4-client-installation","title":"4. Client installation","text":"<p>You will now need to configure a Linux system as a Kerberos client. This will allow access to any kerberized services  once a user has successfully logged into the system.</p> <pre><code># step1: install packages\nsudo apt install krb5-user \n\n# step2: Configure the krb client config\nsudo dpkg-reconfigure krb5-config\n# The dpkg-reconfigure adds entries to the /etc/krb5.conf file for your Realm. You should have entries similar to the following:\n[libdefaults]\n        default_realm = CASD.LOCAL\n...\n[realms]\n        CASD.LOCAL = {\n                kdc = 10.50.5.57\n                admin_server = 10.50.5.57\n        }\n\n# step3: Test the configuration by requesting a ticket using the kinit utility\nkinit pliu@CASD.LOCAL\n\n# step4: Check the generated ticket\nklist \n</code></pre> <p>if you want to use krb for ssh authentication, you need to install libpam-krb5 libpam-ccreds auth-client-config <code>sudo auth-client-config -a -p kerberos_example</code> use the <code>auth-client-config</code> to configure the <code>libpam-krb5</code> module  to request a ticket during login</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#41-kadmin","title":"4.1 kadmin","text":"<p>kadmin and kadmin.local are command-line interfaces to the Kerberos V5 administration system. They provide  nearly identical functionalities; the difference is that <code>kadmin.local directly accesses the KDC database</code>, while  <code>kadmin performs operations using kadmind</code>.</p> <p>With <code>sudo kadmin.local</code> you have the admin console in the krb server locally. You can also access the admin console  remotely. You can find the complete doc here.</p>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#42-kutil","title":"4.2 kutil","text":"<p>The ktutil command invokes a command interface from which an administrator can read, write, or edit  entries in a keytab or Kerberos V4 srvtab file. You can find the detailed documentation of ktutil</p> <p>Create a new keytab with existing principal and password. We can choose different encryption algorithm</p> <pre><code>ktutil:  add_entry -password -p pengfei@CASD.LOCAL -k 1 -e\n    aes128-cts-hmac-sha1-96\nPassword for pengfei@CASD.LOCAL:\nktutil:  add_entry -password -p pengfei@CASD.LOCAL -k 1 -e\n    aes256-cts-hmac-sha1-96\nPassword for pengfei@CASD.LOCAL:\nktutil:  write_kt keytab\nktutil:\n</code></pre> <p>Read an existing keytab</p> <pre><code>ktutil:  read_kt /etc/krb5.keytab \nktutil:  list\nslot KVNO Principal\n---- ---- ---------------------------------------------------------------------\n   1    2     host/sssd-test.casd.local@CASD.LOCAL\n   2    2     host/sssd-test.casd.local@CASD.LOCAL\n   3    2 auth-agent/sssd-test.casd.local@CASD.LOCAL\n   4    2 auth-agent/sssd-test.casd.local@CASD.LOCAL\n</code></pre>"},{"location":"security/kerberos/02.Install_kerberos_server_client_for_debian_server/#reference","title":"Reference","text":"<p>https://www.easyredmine.com/documentation-of-easy-redmine/article/how-to-set-up-kerberos-authentication https://blog.csdn.net/qq_43536701/article/details/109854270</p>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/","title":"Set up kerberos to use OpenLdap as backend","text":"<p>kerberos can store user login and password. But often we already have OpenLDAP set up for other things,  such as storing users and groups, adding the <code>Kerberos attributes</code> can be beneficial, providing an <code>integrated story</code>.</p> <ul> <li> <p>Pros:</p> <ul> <li>OpenLDAP replication is faster and more robust than the native Kerberos one, based on a cron job</li> <li>Single source for user account management(e.g. pwd, groups, etc.)</li> </ul> </li> <li> <p>Cons:         - Setting up the LDAP backend isn\u2019t a trivial task and shouldn\u2019t be attempted by administrators without prior knowledge of OpenLDAP.         - since krb5kdc is single-threaded there may be <code>higher latency in servicing requests</code> when using the OpenLDAP backend</p> </li> </ul>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#prerequisite","title":"Prerequisite","text":"<p>Here, we suppose you already have  - openldap server  - kerberos server(e.g. kdc, admin-server)</p> <p>For the openldap server, we suppose  - the base cn: dc=casd,dc=local - the admin account: cn=admin,dc=casd,dc=local</p>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#1-configure-openldap-to-work-with-kerberos","title":"1. Configure Openldap to work with Kerberos","text":"<p>We need to install the below packages</p> <pre><code>sudo apt install krb5-kdc-ldap krb5-admin-server\n</code></pre> <ul> <li>krb5-kdc-ldap: This package provides the LDAP backend plugin for the Kerberos Key Distribution Center (KDC). It               allows Kerberos to use OpenLDAP or another LDAP server to store</li> <li>krb5-admin-server: This package provides the Kerberos admin service (kadmind), which allows administrators               to manage the Kerberos database remotely.</li> </ul>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#11-install-kerberos-schema-in-openldap-server","title":"1.1 Install kerberos schema in openldap server","text":"<p>Openldap needs a <code>Kerberos schema</code> to be able to store the necessary object classes and attributes that  allow LDAP to store Kerberos principals and related data, such as encryption keys, expiration dates, and policy information. </p> <p>Kerberos provides an integrated ldap backend as a database if you are in this situation. You don't need this step. </p> <p>After installing <code>krb5-kdc-ldap</code>, you should find the kerberos schema in <code>/usr/share/doc/krb5-kdc-ldap/kerberos.schema.gz</code></p> <pre><code># copy and unzip the schema\nsudo cp /usr/share/doc/krb5-kdc-ldap/kerberos.schema.gz /etc/ldap/schema/\nsudo gunzip /etc/ldap/schema/kerberos.schema.gz\n\n# convert the schema into ldif format, then load the schema to ldap server\nsudo apt install schema2ldif\nsudo ldap-schema-manager -i kerberos.schema\n# now you should see below output\n# SASL/EXTERNAL authentication started\n# SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\n# SASL SSF: 0\n# executing 'ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/kerberos.ldif'\n# SASL/EXTERNAL authentication started\n# SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\n# SASL SSF: 0\n# adding new entry \"cn=kerberos,cn=schema,cn=config\"\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#12-optimize-the-backend-db-index-of-ldap-server","title":"1.2 Optimize the backend db index of ldap server","text":"<p>With the new schema loaded, let\u2019s index an attribute often used in searches:</p> <pre><code>$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// &lt;&lt;EOF\ndn: olcDatabase={1}mdb,cn=config\nadd: olcDbIndex\nolcDbIndex: krbPrincipalName eq,pres,sub\nEOF\n\n# output example\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#13-create-kerberos-service-account-in-openldap","title":"1.3 Create kerberos service account in Openldap","text":"<p>As kerberos need to contact the Openldap server to perfrom operations, we need to create two service accounts:</p> <ul> <li>kdc-service: needs to have <code>read rights on the realm container, principal container and realm sub-trees</code>.                 If disable_last_success and disable_lockout are not set, however, then <code>kdc-service needs write                  access to the Kerberos container</code> just like the admin DN below.</li> <li>kadmind-service: needs to have read and write rights on the realm container, principal container and realm sub-trees</li> </ul> <p>You can use the below command to add the two service accounts.</p> <pre><code># we use the admin account to run the add script\nldapadd -x -D cn=admin,dc=casd,dc=local -W &lt;&lt;EOF\ndn: uid=kdc-service,dc=casd,dc=local\nuid: kdc-service\nobjectClass: account\nobjectClass: simpleSecurityObject\nuserPassword: {CRYPT}x\ndescription: Account used for the Kerberos KDC\n\ndn: uid=kadmin-service,dc=casd,dc=local\nuid: kadmin-service\nobjectClass: account\nobjectClass: simpleSecurityObject\nuserPassword: {CRYPT}x\ndescription: Account used for the Kerberos Admin server\nEOF\n\n# you will be prompted to enter the password of the two accounts\nEnter LDAP Password: \nadding new entry \"uid=kdc-service,dc=casd,dc=local\"\n\nadding new entry \"uid=kadmin-service,dc=casd,dc=local\"\n</code></pre> <p>You can always reset the password by using the below command</p> <pre><code>ldappasswd -x -D cn=admin,dc=casd,dc=local -W -S uid=kdc-service,dc=casd,dc=local\n# you will be prompted to enter the password\nNew password:   \nRe-enter new password: \nEnter LDAP Password:  \n</code></pre> <p>You can test these accounts</p> <pre><code>ldapwhoami -x -D uid=kdc-service,dc=casd,dc=local -W\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#14-update-the-access-control-lists-acl-of-openldap-server","title":"1.4 update the Access Control Lists (ACL) of Openldap server","text":"<p>This step can be tricky, as it highly depends on what you have defined already.  By default, the slapd package configures your database with the following ACLs:</p> <pre><code>olcAccess: {0}to attrs=userPassword by self write by anonymous auth by * none\nolcAccess: {1}to attrs=shadowLastChange by self write by * read\nolcAccess: {2}to * by * read\n</code></pre> <p>We need to insert new rules before the final to * by * read one, to control access to the Kerberos related entries and attributes:</p> <pre><code># add acl for krb service accounts\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// &lt;&lt;EOF\ndn: olcDatabase={1}mdb,cn=config\nadd: olcAccess\nolcAccess: {2}to attrs=krbPrincipalKey\n  by anonymous auth\n  by dn.exact=\"uid=kdc-service,dc=casd,dc=local\" read\n  by dn.exact=\"uid=kadmin-service,dc=casd,dc=local\" write\n  by self write\n  by * none\n-\nadd: olcAccess\nolcAccess: {3}to dn.subtree=\"cn=kerberos,dc=casd,dc=local\"\n  by dn.exact=\"uid=kdc-service,dc=casd,dc=local\" read\n  by dn.exact=\"uid=kadmin-service,dc=casd,dc=local\" write\n  by * none\nEOF\n\n# output example\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\n</code></pre> <p>Here, we define the dn of kerberos container as <code>cn=kerberos,dc=casd,dc=local</code>, all the entries related to kerberos which are generated automatically should be stored in the kerberos container.</p> <p>After the modification, the existing {2} rule become {4}.:</p> <pre><code>$ sudo slapcat -b cn=config\n\n# the output below was reformatted a bit for clarity\nolcAccess: {0}to attrs=userPassword\n    by self write\n    by anonymous auth\n    by * none\nolcAccess: {1}to attrs=shadowLastChange\n    by self write\n    by * read\nolcAccess: {2}to attrs=krbPrincipalKey by anonymous auth\n    by dn.exact=\"uid=kdc-service,dc=casd,dc=local\" read\n    by dn.exact=\"uid=kadmin-service,dc=casd,dc=local\" write\n    by self write\n    by * none\nolcAccess: {3}to dn.subtree=\"cn=kerberos,dc=casd,dc=local\"\n    by dn.exact=\"uid=kdc-service,dc=casd,dc=local\" read\n    by dn.exact=\"uid=kadmin-service,dc=casd,dc=local\" write\n    by * none\nolcAccess: {4}to * by * read\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#2-configure-kerberos-to-use-openldap","title":"2. Configure kerberos to use openldap","text":""},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#21-install-or-reconfigure-kerberos","title":"2.1 Install or reconfigure kerberos","text":"<pre><code># to get a good starting point with /etc/krb5.conf, you can reconfigure kerberos\nsudo dpkg-reconfigure krb5-config\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#22-configure-kerberos-to-use-openldap-as-backend","title":"2.2 Configure kerberos to use openldap as backend","text":"<p>Now edit /etc/krb5.conf by adding the <code>database_module</code> option to the <code>CASD.LOCAL</code> realm section:</p> <pre><code>[realms]\n    CASD.LOCAL = {\n        kdc = 10.50.5.57\n        admin_server = 10.50.5.57\n                default_domain = casd.local\n                database_module = openldap_ldapconf\n    }\n</code></pre> <p>Then add these two sections to complete the definition of <code>database_module = openldap_ldapconf</code>:</p> <pre><code>[dbdefaults]\n        ldap_kerberos_container_dn = cn=kerberos,dc=casd,dc=local\n\n[dbmodules]\n        openldap_ldapconf = {\n                db_library = kldap\n\n                # if either of these is false, then the ldap_kdc_dn needs to\n                # have write access\n                disable_last_success = true\n                disable_lockout  = true\n\n                # this object needs to have read rights on\n                # the realm container, principal container and realm sub-trees\n                ldap_kdc_dn = \"uid=kdc-service,dc=casd,dc=local\"\n\n                # this object needs to have read and write rights on\n                # the realm container, principal container and realm sub-trees\n                ldap_kadmind_dn = \"uid=kadmin-service,dc=casd,dc=local\"\n\n                ldap_service_password_file = /etc/krb5kdc/service.keyfile\n                ldap_servers = ldapi:///\n                ldap_conns_per_server = 5\n        }\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#23-create-the-realm-in-openldap-server","title":"2.3 Create the realm in openldap server","text":"<p>Use the <code>kdb5_ldap_util</code> utility to create the realm in openldap server</p> <pre><code>sudo kdb5_ldap_util -D cn=admin,dc=casd,dc=local create -subtrees dc=casd,dc=local -r CASD.LOCAL -s -H ldapi:///\n\n# output example\nPassword for \"cn=admin,dc=casd,dc=local\": \nInitializing database for realm 'CASD.LOCAL'\nYou will be prompted for the database Master Password.\nIt is important that you NOT FORGET this password.\nEnter KDC database master key: \nRe-enter KDC database master key to verify: \n</code></pre> <p>after this command, you should see <code>cn=CASD.LOCAL,cn=kerberos,dc=casd,dc=local</code> in the openldap server.</p>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#24-create-a-stash-of-the-password-used-to-bind-to-the-ldap-server","title":"2.4 Create a stash of the password used to bind to the LDAP server.","text":"<pre><code>sudo kdb5_ldap_util -D cn=admin,dc=casd,dc=local stashsrvpw -f /etc/krb5kdc/service.keyfile uid=kdc-service,dc=casd,dc=local\nsudo kdb5_ldap_util -D cn=admin,dc=casd,dc=local stashsrvpw -f /etc/krb5kdc/service.keyfile uid=kadmin-service,dc=casd,dc=local\nsudo kdb5_ldap_util -D cn=admin,dc=casd,dc=local stashsrvpw -f /etc/krb5kdc/service.keyfile cn=admin,dc=casd,dc=local\n</code></pre> <p>The /etc/krb5kdc/service.keyfile file now contains clear text versions of the passwords used by the KDC to contact the LDAP server!</p>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#25-create-acl-file-for-admin-server","title":"2.5 create acl file for admin server","text":"<pre><code>$ sudo vim /etc/krb5kdc/kadm5.acl\n\n# add the below line\n*/admin@CASD.LOCAL       *\n\n# start or restart the krb services\nsudo systemctl start krb5-kdc.service krb5-admin-server.service\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#26-test-the-binding","title":"2.6 Test the binding","text":"<p>Login to the krb admin </p> <pre><code># login to admin server\nsudo kadmin.local\n# now you will have a krb admin server prompt, you can use the kerberos admin command now\n# Below are some command examples, you can type ? to have all the command list\n\n# list the existing principals(user account)\nlist_principals\n\n# add new principals\naddprinc test\n</code></pre> <p>The above command will create an <code>test</code> principal with a DN of krbPrincipalName=test@CASD.LOCAL,cn=CASD.LOCAL,cn=krbContainer,dc=casd,dc=local.</p>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#27-use-the-existing-user-account-of-openldap-server","title":"2.7 Use the existing user account of openldap server","text":"<p>Let\u2019s say, however, that you already have a user in your directory, and it\u2019s in uid=pengfei,ou=people,dc=casd,dc=local.  How can you add the Kerberos attributes to it? You use the -x parameter to specify the location.  For the ldap_kadmin_dn to be able to write to it, we first need to update the ACLs:</p> <pre><code>sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// &lt;&lt;EOF\ndn: olcDatabase={1}mdb,cn=config\nadd: olcAccess\nolcAccess: {4}to dn.subtree=\u201cou=people,dc=casd,dc=local\u201d\n    by dn.exact=\u201duid=kdc-service,dc=casd,dc=local\u201d read\n    by dn.exact=\u201duid=kadmin-service,dc=casd,dc=local\u201d write\n    by * break\nEOF\n</code></pre> <p>Now you can create a principal which matches with an existing openldap user account. Suppose the dn of the user account is <code>uid=pengfei,ou=people,dc=casd,dc=local</code>.</p> <pre><code># login to admin server\nsudo kadmin.local\n\n# the kadmin-service must have write access on the target dn, otherwise you will get an Insufficient access error\naddprinc -x dn=uid=pengfei,ou=people,dc=casd,dc=local pengfei/admin\n\naddprinc -x dn=uid=test,ou=people,dc=casd,dc=local test\n\n# you can check the details of a principal by \ngetprinc pengfei/admin\n</code></pre> <p>if the <code>dn</code> exists, <code>kadmin.local</code> will just add the required Kerberos attributes to this existing entry. If it  didn\u2019t exist, <code>it would be created from scratch, with only the Kerberos attributes</code>.</p>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#3-test-the-principal-via-a-kerberos-client","title":"3. Test the principal via a kerberos client","text":"<p>Goto another server</p> <pre><code># step1: install packages\nsudo apt install krb5-user\n\n# if it's the first time, you will see three prompt\n# 1. enter default Realm: CASD.LOCAL\n# 2. enter kdc url: kdc01.casd.local or 10.50.5.57\n# 3. enter admin server url: kdc01.casd.local or 10.50.5.57\n\n# if you already have the client, you can re-configure the krb client, you should see the three prompt.\nsudo dpkg-reconfigure krb5-config\n\n# After the prompt, the config file `/etc/krb5.conf` file for your Realm. You should have entries similar to the following:\n[libdefaults]\n        default_realm = CASD.LOCAL\n...\n[realms]\n        CASD.LOCAL = {\n                kdc = 10.50.5.57\n                admin_server = 10.50.5.57\n        }\n\n# step3: Test the configuration by requesting a ticket using the kinit utility\nkinit pengfei/admin@CASD.LOCAL\n\n# step4: setup a default principal\nvim ~/.k5identity\npengfei/admin@CASD.LOCAL\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#4-gssapi-for","title":"4. GSSAPI for","text":"<p>The below <code>olc_enable_gssapi.ldif</code> enables GSSAPI authentication in the openldap server</p> <pre><code>dn: cn=config\nchangetype: modify\nadd: olcAuthzRegexp\nolcAuthzRegexp: uid=([^,]+),cn=casd.local,cn=gssapi,cn=auth\n  uid=$1,ou=people,dc=casd,dc=local\n-\n#2. Configurer the domain SASL (Simple Authentication and Security Layer) of kerberos\nadd: olcSaslRealm\nolcSaslRealm: CASD.LOCAL\n</code></pre> <pre><code>$ldapmodify -QY EXTERNAL -H ldapi:/// -f olc_enable_gssapi.ldif\n</code></pre> <pre><code># login to admin server\nsudo kadmin.local\n\n# create a principal kerberos to connect to ldap\naddprinc -randkey ldap/ldap.casd.local@CASD.LOCAL\n\n# Update the Keytab: Export the principal to the keytab file so the LDAP server can use it:\nktadd -k /etc/krb5.keytab ldap/ldap.casd.local@CASD.LOCAL\n</code></pre> <p>Try to use your kerberos token to access the openldap server</p> <pre><code>ldapsearch -H ldap://ldap.casd.local -Y GSSAPI -b \"dc=casd,dc=local\"\n\nssh -o GSSAPIAuthentication=yes -o GSSAPIDelegateCredentials=yes test@10.50.5.92\n</code></pre>"},{"location":"security/kerberos/03.Configure_kerberos_to_use_ldap/#bkp","title":"bkp","text":"<pre><code>sudo apt install  krb5-pkinit libsasl2-modules-gssapi-mit\n</code></pre> <ul> <li> <p>krb5-pkinit: This package enables <code>PKINIT (Public Key Cryptography for Initial Authentication)</code> in Kerberos.            PKINIT allows Kerberos authentication <code>using X.509 certificates instead of passwords</code>, which is useful for            environments that require <code>smart card login</code> or <code>certificate-based authentication</code>.</p> </li> <li> <p>libsasl2-modules-gssapi-mit: This package provides the GSSAPI (Generic Security Services Application Program Interface)       module for SASL (Simple Authentication and Security Layer) using the MIT Kerberos implementation.</p> </li> </ul>"},{"location":"security/kerberos/04.Install_kerberos_server_client_for_windows_server/","title":"Install Kerberos server and client on windows","text":"<p>In windows, the <code>ActiveDirectory(AD)</code> implements the feature of both <code>Ldap and kerberos</code>. So if you enabled an AD in your Windows server, you automatically have a <code>KDC</code>.</p> <p>So you only need to follow the tutorial on how to install AD.</p>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/","title":"Configure sssd to use static uid, gid","text":"<p>POSIX (Portable Operating System Interface) is <code>UNIX/Linux standards for identity and access control</code>. A <code>POSIX</code>  account use specific attributes such as - uidNumber \u2013 Unique User ID (UID) - gidNumber \u2013 Primary Group ID (GID) - homeDirectory \u2013 User's home directory path  - loginShell \u2013 The default shell (e.g., /bin/bash)</p> <p>These attributes allow UNIX/Linux systems to recognize, authenticate users, and create user workspace.</p> <p>By default, AD does not use POSIX attributes for user and group.  Instead, AD relies on:</p> <ul> <li>Security Identifiers (SIDs): Every user and group has a <code>SID</code>, which is a unique identifier in Windows.</li> <li><code>sAMAccountName</code>: pliu (This is legacy login, )</li> <li>UserPrincipalName (UPN): pliu@casd.eu (authentication in modern windows server)</li> </ul>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#1-default-behavior-when-sssd-uses-ad-as-authentication-backend","title":"1. Default behavior when sssd uses AD as authentication backend","text":"<p>The default behavior in <code>SSSD</code> and <code>Winbind</code> is to use <code>auto id mapping</code>. SSSD will dynamically generate UIDs and GIDs from the <code>AD objects's ObjectSID</code>. This will lead to inconsistent UIDs and GIDs across machines.</p> <p>In certain scenarios, it will create conflicting ACL in user home. For example, if a user with AD account with name <code>test</code> login to a linux server, a user home will be created <code>/home/test</code>. If the user account is deleted, and a new account <code>test</code> is created, when the new <code>test</code> user login to the linux server, it will user /home/test as home dir too. But the new and old <code>test</code> will have different uid. So the old files in /home/test will have old uid as owner, the new  <code>test</code> user can't access it. </p> <p>To avoid inconsistent UIDs and GIDs, we recommend you to use static uidNumber and gidNumber.</p>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#2-configure-sssd-to-user-static-uidnumber-and-gidnumber","title":"2. Configure sssd to user static uidNumber and gidNumber.","text":"<p>To configure sssd to user static uidNumber and gidNumber, follow the below steps 1. Add posix attributes in AD 2. Configure sssd to read posix attributes</p>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#21-adds-posix-attributes-in-ad","title":"2.1 Adds posix attributes in AD","text":"<p>If SSSD wants to use static uidNumber and gidNumber, the AD server must have those attributes. Before Windows server 2016. The AD server can use <code>rfc2307</code> schema, which allows us to create posix compatible user  accounts and groups. This feature has been removed since <code>Windows server 2016</code>. But you can still add attributes such as <code>uidNumber</code>, <code>gidNumber</code> to a user account or group. </p> <p>Open <code>AD users and groups gui</code>-&gt; on the toolbar, click on <code>view</code>-&gt; Select <code>advance features</code> -&gt; now when you double-click on  a user account, you will see a tab called </p> <p></p> <p></p> <p>The official doc can be found here.</p>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#22-configure-sssd-to-read-posix-attributes","title":"2.2 Configure sssd to read posix attributes","text":"<p>Before changing your sssd configuration, make sure <code>AD Objects have gidNumber and uidNumber Attributes</code>.</p>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#221-for-ad-windows-server-2016","title":"2.2.1 For AD &lt; Windows server 2016.","text":"<p>Configure the AD server to use <code>rfc2307</code> schema, and create posix compatible accounts and groups. Then add the below  conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False\nldap_schema = rfc2307\n</code></pre>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#221-for-ad-windows-server-2016_1","title":"2.2.1 For AD &gt;= Windows server 2016.","text":"<p>Add the below conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False  # Important: Forces usage of uidNumber/gidNumber\nldap_user_uid_number = uidNumber\nldap_user_gid_number = gidNumber\nldap_group_gid_number = gidNumber\nenumerate = True  # Optional: Lists all users and groups\n</code></pre>"},{"location":"security/kerberos/05.Configure_sssd_to_use_static_uid_gid/#23-restart-sssd-and-check-uid-gid","title":"2.3 Restart sssd and check uid, gid","text":"<pre><code># restart sssd\nsystemctl restart sssd\n# clear sssd cache\nsss_cache -E\n\n# check user id and groups\nid &lt;uid&gt;\n\n# you should see the output id value matches the value which you deined in AD\n\n# check group id\ngetent group &lt;groupname&gt;\n</code></pre>"},{"location":"security/kerberos/Keytab_introduction/","title":"Keytab files","text":"<p>A .keytab file is a binary file that stores <code>Kerberos principal credentials in an encrypted format</code>.  It allows services and users to authenticate with a Kerberos Key Distribution Center (KDC) without  manually entering a password.</p>"},{"location":"security/kerberos/Keytab_introduction/#1-use-cases","title":"1. Use cases","text":"<p>We will use a keytab file for getting a kerberos ticket in the below scenarios:</p> <ul> <li> <p>Service-to-Service Authentication: The authentication between services must be automatically without manual input             of password. For example, Hadoop YARN ResourceManager or Spark job needs to authenticate to HDFS, it               uses a keytab to prove its identity</p> </li> <li> <p>Passwordless Login: A keytab file securely stores credentials and eliminates the need to hardcode                  passwords in scripts or configurations.</p> </li> </ul>"},{"location":"security/kerberos/Keytab_introduction/#2-lifecycle-of-a-keytab-file","title":"2. Lifecycle of a .keytab File","text":"<p>A .keytab file <code>does not expire on its own</code>, but the credentials inside it (Kerberos keys) can become invalid due to  password changes, key rotations, or policy settings in Active Directory (AD) or the Kerberos Key Distribution Center (KDC).  Below is the typical lifecycle: 1. Creation of the Keytab: The .keytab file is generated using <code>ktpass (Windows)</code> or <code>kadmin (Linux)</code>.       <code>It contains encrypted credentials for a Kerberos principal.</code> 2. Usage in passwordless Authentication: <code>kinit -kt &lt;keytab&gt; &lt;principal&gt;</code> command fetches a Kerberos ticket from the KDC. 3. Expiry, renew of Tickets: <code>TGTs expire</code> after a certain time (e.g., 10 hours). If the ticket is renewable, it Can be          renewed using <code>kinit -R</code>. Otherwise, a new ticket must be obtained using <code>kinit -kt &lt;keytab&gt; &lt;principal&gt;</code>. 4. Keytab becomes invalid, ask a new keytab:</p> <p>Below is a list of when you need to renew a .keytab file: 1. Users change the password: 2. Password/key rotation policies: For service accounts, the services do not change the password. But password/key                     rotation policies may be enforced. AD may change the password every 90 days. 3. Revocation: If a security breach has been discovered, the admin users will revoke existing credentials.</p>"},{"location":"security/kerberos/Keytab_introduction/#3-keytab-files-generation","title":"3. Keytab files generation","text":"<p>There are two ways to generate keytab files: - from the server side(KDC) - from the client side</p>"},{"location":"security/kerberos/Keytab_introduction/#31-from-the-kdc-side","title":"3.1 From the KDC side","text":""},{"location":"security/kerberos/Keytab_introduction/#311-for-adkrb-userswindows","title":"3.1.1 For AD/krb users(Windows):","text":"<pre><code># for a user account\nktpass -princ pliu@CASDDS.CASD -mapuser pliu -crypto ALL -ptype KRB5_NT_PRINCIPAL -pass * -out pliu-user.keytab\n\n# for a service account\nktpass -princ host/hadoop-client.casdds.casd@CASDDS.CASD -mapuser HADOOP-CLIENT$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 -out hadoop-client.keytab\n</code></pre> <ul> <li>In <code>-mapuser HADOOP-CLIENT$</code>, The $ indicates it's a service account (not a user). </li> <li><code>-pass *</code>: You will be prompted to enter a password, this password must match the actual AD account password.                 ktpass doesn\u2019t have the ability to fetch a user's existing password from Active Directory automatically.</li> </ul> <p>To automate the keytab generation, you can write a powershell which reset the AD account password at the same time.</p> <pre><code># get a new password from prompt\n$pass = Read-Host -AsSecureString \"Enter AD Password\"\n$plain = [Runtime.InteropServices.Marshal]::PtrToStringAuto(\n    [Runtime.InteropServices.Marshal]::SecureStringToBSTR($pass)\n)\n# reset AD account password with the new password\nSet-ADAccountPassword -Identity pliu -Reset -NewPassword (ConvertTo-SecureString $plain -AsPlainText -Force)\n\n# generate keytab with the new password\nktpass -princ pliu@CASDDS.CASD -mapuser pliu -crypto ALL -ptype KRB5_NT_PRINCIPAL -pass $plain -out pliu-user.keytab\n</code></pre>"},{"location":"security/kerberos/Keytab_introduction/#312-for-mit-kerberos-userslinux","title":"3.1.2 For MIT kerberos users(Linux):","text":"<pre><code># get a kadmin shell\nsudo kadmin.local\n\n# create a keytab for one principal\nktadd -k /tmp/pliu-user.keytab pliu@CASDDS.CASD\n\n# if the principal does not exist, you can create a new principal with\naddprinc -randkey pliu@CASDDS.CASD\n</code></pre>"},{"location":"security/kerberos/Keytab_introduction/#32-from-the-client-side","title":"3.2 From the client side","text":""},{"location":"security/kerberos/Keytab_introduction/#321-for-adkrb-userswindows","title":"3.2.1 For AD/krb users(Windows):","text":"<p>For windows user, we can use the <code>ktpass</code> command</p> <pre><code>ktpass -princ HTTP/client.example.com@EXAMPLE.COM ^\n       -mapuser clientuser ^\n       -crypto AES256-SHA1 ^\n       -ptype KRB5_NT_PRINCIPAL ^\n       -pass ActualUserPassword123! ^\n       -out C:\\Users\\YourName\\Desktop\\client.keytab ^\n       -kvno 0\n</code></pre> <ul> <li>-princ: The Kerberos principal (usually service/FQDN@REALM)</li> <li>-mapuser: AD username (can be service or regular user)</li> <li>-crypto: Encryption type</li> <li>-out: Path to save the keytab</li> <li>-pass: You must know the user's actual password</li> <li>-kvno 0: Lets the KDC auto-select the current Key Version Number</li> </ul> <p>Ktpass still needs to contact the KDC, you need to make sure DNS and time sync with the domain are correct.</p>"},{"location":"security/kerberos/Keytab_introduction/#322-for-mit-kerberos-userslinux","title":"3.2.2 For MIT kerberos users(Linux):","text":"<p>For linux kerberos client, you can use the ktutil tool. Below is an example</p> <pre><code># open ktutil shell\nktutil\n\n# add an kerberos entry(a principal, password pair)\naddent -password -p pliu@CASDDS.CASD -k 1 -e aes256-cts-hmac-sha1-96\n\n# output the kerberos entry to a keytab file\nwkt /home/pliu/pliu-user.keytab\n\n# you can read the kerberos entry in a keytab file with read_kt command\nread_kt /home/pliu/pliu-user.keytab\n\n# the above command will add the content of the keytab in the ktutil cache. When you write the cache to a keytab file\n# the content of all kerberos entry will be copied in the keytab file.\n\n# quit the ktutil shell \nq\n\n# test the keytab files\nklist -k /home/pliu/pliu-user.keytab\n\n# generate a ticket with the keytab\nkinit -kt pliu-user.keytab pliu@CASDDS.CASD\n</code></pre> <ul> <li>-password: prompt to ask user password</li> <li>-p: specify user principal</li> <li>-k: specify the key version number.</li> <li>-e: specify the encryption algorithm.</li> </ul>"},{"location":"security/keycloak/Install_keycloak/","title":"Install keycloak on debian 11","text":"<p>This tutorial shows how to install and config a keycloak server in a scripted manner. The objective is: </p> <ul> <li>enable installation automation without K8s cluster</li> <li>enable the usage of variables for supporting various environments</li> <li>enable idempotence (the script can be executed multiple times producing the same results)</li> </ul> <p>You can find the official doc https://www.keycloak.org/documentation.html</p> <p>Keycloak Operator can only be used in a Kubernetes based runtime.</p> <p>We will follow below key Steps  1. Installing and starting the Keycloak server 2. Connecting the Admin CLI 3. Configuring</p>"},{"location":"security/keycloak/Install_keycloak/#step-0-prerequisites","title":"Step 0 : Prerequisites","text":""},{"location":"security/keycloak/Install_keycloak/#create-user-account","title":"Create user account","text":"<p>For good practice, we should run the <code>keycloak.service</code> with a service account(e.g. specific User and Group).  In this tutorial, we decide to use <code>keycloak</code> as the username and group name.</p> <p>You can create the user and group using the <code>groupadd and useradd</code> commands. The following example creates the user, group,  and working directory for keycloak. These commands typically requires root (sudo) permissions.</p> <pre><code>sudo groupadd -r keycloak\n\nsudo useradd -r -g keycloak -d /opt/keycloak -s /sbin/nologin keycloak\n\nsudo mkdir -p /opt/keycloak\n\n# change the owner of the data folder\nsudo chown keycloak:keycloak /opt/keycloak\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#install-jdk","title":"Install jdk","text":"<p>Keycloak requires Java to work. You can check and verify that Java is installed with the following command. Base on the keycloak version, you need to install the required jdk version. In this tutorial, for keycloak 23.0.4, we use jdk-17</p> <pre><code># check if java installed\njava -version\n\n# install jdk 17 (debian 11)\nsudo apt install openjdk-17-jdk\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#step-1-installing-and-starting-the-keycloak-server","title":"Step 1: Installing and starting the Keycloak server","text":"<p>We can install the keycloak en mode: - bare metal - container(e.g. docker, k8s, etc.)</p>"},{"location":"security/keycloak/Install_keycloak/#installation-en-mode-bare-metal","title":"Installation en mode bare metal","text":"<p>Download of the Keycloak distribution.</p> <pre><code># fix the kc version which you want to download\nexport KC_VERSION=23.0.4\ncurl -LO  https://github.com/keycloak/keycloak/releases/download/\"${KC_VERSION}\"/keycloak-\"${KC_VERSION}\".zip\n\n# copy the bin into the working directory\nsudo mv keycloak-${KC_VERSION}.zip /opt/keycloak\n\n# unpacking the archive.\nunzip keycloak-${KC_VERSION}.zip\n\n# \ncd keycloak-${KC_VERSION}\n\n# add execute acl on bin folder\nsudo chmod o+x bin\n\n# This directory contains a Keycloak Quarkus application.\n# When we start the server for the first time, we have to set the admin user and the admin password:\n# You can notice all the config is done by setting env var not in a config file. Because it's designed with cloud native\nKEYCLOAK_ADMIN=admin KEYCLOAK_ADMIN_PASSWORD=YVqs7p4bJaim3rQ2FSI8 ./bin/kc.sh start-dev\n\n# When we start again, it is not necessary to set these variables, again. You can start the server with:\n./bin/kc.sh start-dev\n\n# start-dev runs the quarkus application in DEV-mode. Do not use this for Production.\n# By default, the Keycloak server is using the following ports. They are only served from the localhost loopback address 127.0.0.1:\n# 8080 for Keycloak using HTTP\n# One of the last lines from the log output is:\n# 2023-04-11 13:23:29,545 INFO  [io.quarkus] (main) Keycloak 21.0.2 on JVM (powered by Quarkus 2.13.7.Final) started in 4.418s. Listening on: http://0.0.0.0:8080\n# \n</code></pre> <p>We can now open the Administration Console from localhost and do the login with the just created admin user.</p> <p>The distribution also contains the <code>Admin CLI</code>. This is the shell script ./bin/kcadm.sh.</p> <p>We define the <code>environment variable KCADM for the kcadm.sh</code> script. </p> <pre><code># It must be the absolute path to the \n# kcadm.sh script from the above Keycloak installation.\n# general form\nexport KCADM=\"/path/to/keycloak/bin/kcadm.sh\"\n# in our tutorial\nexport KCADM=\"/opt/keycloak/keycloak-23.0.4/bin/kcadm.sh\"\nexport HOST_FOR_KCADM=localhost\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#installation-en-mode-docker-image","title":"Installation en mode docker image","text":"<p>To install and run Keycloak as a docker container a single command is necessary.</p> <pre><code>#  \nexport KC_VERSION=23.0.4\n\n# create a container\ndocker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:${KC_VERSION} start-dev\n</code></pre> <p>In the next steps we are using the Admin CLI script (kcadm.sh). It is also contained in the Keycloak docker image.  This means every call of the Admin CLI is executing the script from within the docker image.</p> <pre><code>docker run --rm --entrypoint /opt/keycloak/bin/kcadm.sh quay.io/keycloak/keycloak:${KC_VERSION}\n</code></pre> <p>We define the environment variable KCADM for the above command. Additionally, we mount the <code>$HOME/.keycloak</code> folder  from the docker host at <code>/opt/.keycloak</code>.</p> <pre><code>export KCADM=\"docker run --rm --entrypoint /opt/keycloak/bin/kcadm.sh -v ${HOME}/.keycloak:/opt/keycloak/.keycloak quay.io/keycloak/keycloak:${KC_VERSION}\"\n\n# When we executed $KCADM successfully the following output is shown:\nKeycloak Admin CLI\n\nUse 'kcadm.sh config credentials' command with username and password to start a session against a specific\nserver and realm.\n</code></pre> <p>When executing this script with a command (like config, create, get etc.) it connects to the Keycloak instance running  in another docker container. Depending on the docker environment you are using, the host name of the Keycloak instance  must be specified differently. For Docker Desktop environments the host name can be defined as <code>host.docker.internal</code>.</p> <pre><code>export HOST_FOR_KCADM=host.docker.internal\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#step2-starting-the-keycloak-server","title":"Step2: Starting the keycloak server","text":""},{"location":"security/keycloak/Install_keycloak/#in-development-mode","title":"In development mode","text":"<p>This mode offers convenient defaults for developers to get the keycloak server up and running quickly. </p> <pre><code># To start in development mode, enter the following command:\n\nbin/kc.[sh|bat] start-dev\n</code></pre> <p>You can write a little script to run the service on the background. Not recommended for the production usage</p> <pre><code>#!/bin/bash\nexport KEYCLOAK_ADMIN=admin\nexport KEYCLOAK_ADMIN_PASSWORD=YVqs7p4bJaim3rQ2FSI8\n\nnohup path/to/keycloak/bin/kc.sh start-dev --http_server_proxy edge  --hostname-strict=false --hostname=keycloak.casd.local &amp;\n</code></pre> <p>Development mode sets the following default configuration:</p> <ul> <li>HTTP is enabled</li> <li>Strict hostname resolution is disabled</li> <li>Cache is set to local (No distributed cache mechanism used for high availability)</li> <li>Theme-caching and template-caching is disabled</li> </ul>"},{"location":"security/keycloak/Install_keycloak/#in-production-mode","title":"In production mode","text":"<p>This mode follows a secure by default principle.</p> <pre><code>To start in production mode, enter the following command:\n\nbin/kc.[sh|bat] start\n</code></pre> <p>Without further configuration, this command will not start Keycloak and show you an error instead. This response is  done on purpose, because Keycloak follows a secure by default principle. Production mode expects a hostname to be  set up and an HTTPS/TLS setup to be available when started.</p> <p>Production mode sets the following defaults:</p> <ul> <li>HTTP is disabled as transport layer security (HTTPS) is essential </li> <li>Hostname configuration is expected </li> <li>HTTPS/TLS configuration is expected</li> </ul> <p>Before deploying Keycloak in a production environment, make sure to follow the steps outlined in Configuring  Keycloak for production.</p>"},{"location":"security/keycloak/Install_keycloak/#creating-the-initial-admin-user","title":"Creating the initial admin user","text":"<p>You can create the initial admin user by using the <code>web frontend</code>, which you access using a local connection  (localhost). You can instead create this user by using environment variables. Set <code>KEYCLOAK_ADMIN=&lt;username&gt;</code> for  the initial admin username and <code>KEYCLOAK_ADMIN_PASSWORD=&lt;password&gt;</code> for the initial admin password.</p> <p>Keycloak parses these values at first startup to create an initial user with administrative rights. Once the first  user with administrative rights exists, you can use the Admin Console or the command line tool kcadm.[sh|bat] to  create additional users.</p> <p>If the initial administrator already exists and the environment variables are still present at startup, an error  message stating the failed creation of the initial administrator is shown in the logs. Keycloak ignores the  values and starts up correctly.</p>"},{"location":"security/keycloak/Install_keycloak/#optimize-the-keycloak-startup","title":"Optimize the Keycloak startup","text":"<p>We recommend optimizing Keycloak to provide faster startup and better memory consumption before deploying Keycloak  in a production environment. </p> <p>By default, when you use the start or start-dev command, Keycloak runs a build command under the covers for  convenience reasons.</p> <p>You can run the build command explicitly. Below are some examples</p> <pre><code># get all build command options \nbin/kc.[sh|bat] build --help\n\n# Run a build to set the database to PostgreSQL before startup\nbin/kc.[sh|bat] \n\n# below is an example\nsudo ./bin/kc.sh build --db=postgres \n\n# after the build process, you can check the config of the build result\nsudo ./bin/kc.sh show-config\n</code></pre> <p>For example, you can add below conf in the <code>conf/keycloak.conf</code> file</p> <pre><code># Basic settings for running in production. Change accordingly before deploying the server.\n\n# Database\n\n# The database vendor.\ndb=postgres\n\n# The username of the database user.\ndb-username=keycloak\n\n# The password of the database user.\ndb-password=changeMe\n\n# The full database JDBC URL. If not provided, a default URL is set based on the selected database vendor.\ndb-url=jdbc:postgresql://localhost/keycloak\n\n# Observability\n\n# If the server should expose healthcheck endpoints.\n#health-enabled=true\n\n# If the server should expose metrics endpoints.\n#metrics-enabled=true\n\n# HTTP\n\n# The file path to a server certificate or certificate chain in PEM format.\nhttps-certificate-file=/opt/keycloak/keycloak-23.0.4/conf/wildcard-casd.pem\n\n# The file path to a private key in PEM format.\nhttps-certificate-key-file=/opt/keycloak/keycloak-23.0.4/conf/wildcard-casd.key\n\n# The http_server_proxy address forwarding mode if the server is behind a reverse http_server_proxy.\n#http_server_proxy=reencrypt\n\n# Do not attach route to cookies and rely on the session affinity capabilities from reverse http_server_proxy\n#spi-sticky-session-encoder-infinispan-should-attach-route=false\n\n# Hostname for the Keycloak server.\nhostname=keycloak.casd.local\n</code></pre> <p>After a successful build, you can start Keycloak and turn off the default startup behavior by entering the following command:</p> <pre><code>bin/kc.[sh|bat] start --optimized &lt;configuration-options&gt;\n</code></pre> <p>The <code>--optimized</code> parameter tells Keycloak to assume a pre-built, already optimized Keycloak image is used. As  a result, Keycloak avoids checking for and running a build directly at startup, which saves time.</p> <p>For more information you can visit this page. https://www.keycloak.org/server/configuration</p>"},{"location":"security/keycloak/Install_keycloak/#step3-connection-the-admin-cli","title":"Step3 : Connection the admin CLI","text":"<p>Now we connect the <code>Keycloak Admin CLI</code> to the API and authenticate with the user created previously. We use two  environment variables created in Step 1:</p> <ul> <li>$KCADM</li> <li>$HOST_FOR_KCADM</li> </ul> <p>Please make sure they are defined. Their definition is dependent on the runtime you have chosen.</p> <p>We log in to the master realm with the admin user. By using the options config credentials we request and maintain an  authenticated session, which is used for all further calls. Be aware the access and refresh tokens for this session will be stored in the file $HOME/.keycloak/kcadm.config.</p> <pre><code>$KCADM config credentials --server http://$HOST_FOR_KCADM:8080 --user admin --password YVqs7p4bJaim3rQ2FSI8 --realm master\n</code></pre> <p>To check the successful authentication and an authenticated session, we make a first request to the API.</p> <pre><code>$KCADM get serverinfo\n</code></pre> <p>The Keycloak server responds with a dump of information about its state and functionality. The same information is  also available within the Web Admin Console.</p>"},{"location":"security/keycloak/Install_keycloak/#step3-creating-a-systemd-service-file-for-keycloak","title":"Step3: Creating a SystemD Service File for Keycloak","text":"<p>If everythion goes well in step 1 and 2, it means we have a keycloak server ready. To facilitate the keycloak  service management, Now we will create a systemD service config file.</p>"},{"location":"security/keycloak/Install_keycloak/#configuration-directory-for-keycloak","title":"configuration directory for Keycloak","text":"<p>Create a configuration directory for Keycloak under /etc directory by the name keycloak.</p> <pre><code>$ cd /etc/\n$ sudo mkdir keycloak\n</code></pre> <p>The keycloak distribution contains a default config template which locate at <code>/path/to/keycloak/conf/keycloak.conf</code>  We can use it as our start point, so copy it to  <code>/etc/keycloak/</code> and rename it to keycloak.conf</p> <pre><code># in our example, our keycloak path is /opt/keycloak/keycloak-23.0.4\nsudo cp /opt/keycloak/keycloak-23.0.4/conf/keycloak.conf /etc/keycloak/keycloak.conf\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#create-a-systemd-service-for-keycloak","title":"Create a systemd service for keycloak","text":"<p>If you followed the above tutorial, you should have one keycloak server on mode bare-metal ready to run.</p> <p>To do a test run, you can use the below command.</p> <pre><code>sudo ./bin/kc.sh start --http_server_proxy edge --hostname-strict=false --http-port=8080 --log=file --log-file=/var/log/keycloak/keycloak.log\n</code></pre> <p>You can also create a systemd daemon to better control the keycloak service. </p> <pre><code># Create a file\nsudo vim /etc/systemd/system/keycloak.service\n\n# put the below content\n[Unit]\nDescription=Keycloak Server\nAfter=syslog.target network.target\nWants=network.target\n\n[Service]\nType=notify\nAmbientCapabilities=CAP_SYS_ADMIN\nUser=keycloak\nGroup=keycloak\nEnvironment=JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64\n# this needs to be reviewed in production env\nExecStart=/opt/keycloak/current/bin/kc.sh start --proxy edge --hostname-strict=false --http-port=8080 --log=file --log-file=/var/log/keycloak/keycloak.log\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>You can now control the keycloak daemon with systemd</p> <pre><code>sudo systemctl start/status/stop/restart keycloak\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#set-a-reverse-proxy","title":"Set a reverse proxy","text":"<p>In this tutorial, I used nginx, you can use other product. Below is a nginx server config example. This works fine with direct grant request. If you use an app to integrate keycloak, you may encounter the  CORS header 'Access-Control-Allow-Origin' missing issue. To resolve this issue, you need to modify the  below config to add <code>Access-Control-Allow-Origin</code> in the http response header. For more details, check</p> <pre><code># set a backend upstream for keycloak server\nupstream keycloak_backend {\n    server 127.0.0.1:8080 fail_timeout=0;\n}\n\nserver {\n  listen 80;\n  server_name keycloak.casd.local;\n\n  # Redirect all traffic to SSL\n  rewrite ^ https://$host$request_uri? permanent;\n}\n\nserver {\n  listen 443 ssl default_server;\n\n  # enables SSLv3/TLSv1, but not SSLv2 which is weak and should no longer be used.\n  ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;\n\n  # disables all weak ciphers\n  ssl_ciphers ALL:!aNULL:!ADH:!eNULL:!LOW:!EXP:RC4+RSA:+HIGH:+MEDIUM;\n\n  server_name keycloak.casd.local;\n\n  ## Access and error logs.\n  access_log /var/log/nginx/access.log;\n  error_log  /var/log/nginx/error.log info;\n\n  ## Keep alive timeout set to a greater value for SSL/TLS.\n  keepalive_timeout 75 75;\n\n  ## See the keepalive_timeout directive in nginx.conf.\n  ## Server certificate and key.\n  ssl_certificate /opt/keycloak/keycloak-23.0.4/conf/wildcard-casd.pem;\n  ssl_certificate_key /opt/keycloak/keycloak-23.0.4/conf/wildcard-casd.key;\n  ssl_session_timeout  5m;\n\n  ## Strict Transport Security header for enhanced security. See\n  ## http://www.chromium.org/sts. I've set it to 2 hours; set it to\n  ## whichever age you want.\n\n  location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Server $host;\n        proxy_set_header X-Forwarded-Port $mapped_server_port;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_pass http://keycloak_backend;\n    }\n\n}\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#testing-the-keycloak-service","title":"Testing the keycloak service","text":""},{"location":"security/keycloak/Install_keycloak/#set-up-keycloak-client","title":"Set up keycloak client","text":"<ol> <li>Create a new realm(e.g. Data-catalog)</li> <li>Create a client inside the realm which you just created (e.g. open-metadata )</li> <li>In the <code>Capability config/Authentication flow</code> section, select two options <code>standard flow</code>, and <code>direct access grants</code></li> <li>In the <code>Capability config/Client authentication</code> section, enable it. This will set up a password for the <code>open-metadata</code> client</li> <li>Get the password from the <code>credentials</code> Tab.</li> </ol>"},{"location":"security/keycloak/Install_keycloak/#get-an-access-token","title":"Get an access token","text":"<p>This curl command will get a token from the <code>open-metadata</code> client in realm <code>Data-catalog</code>(generated by keycloak server).</p> <pre><code># \ncurl -k -X POST https://keycloak.casd.local/realms/Data-catalog/protocol/openid-connect/token \\\n     -H 'Content-Type: application/x-www-form-urlencoded' \\\n     -H 'Accept: application/json' \\\n     -d 'grant_type=password' \\\n     -d 'client_id=open-metadata' \\\n     -d 'client_secret=HZDISp4LEuLL96ozs6cN2p4HftHKY62P' \\\n     -d 'scope=openid' \\\n     -d 'username=jsnow' \\\n     -d 'password=jsnow'\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#verify-user-access-token","title":"Verify User access token","text":"<pre><code>curl -k -X GET https://keycloak.casd.local/realms/Data-catalog/protocol/openid-connect/userinfo \\\n     -H 'Content-Type: application/x-www-form-urlencoded' \\\n     -H 'Authorization: Bearer &lt;access-token&gt;'\n</code></pre> <p>If your token is not valid, you will not receive the user info. If everything goes well, you will have the below  response</p> <pre><code>{\n    \"sub\": \"d0618826-8b2f-4853-a216-5b342d2c0452\",\n    \"email_verified\": false,\n    \"name\": \"John Snow\",\n    \"preferred_username\": \"jsnow\",\n    \"given_name\": \"John\",\n    \"family_name\": \"Snow\",\n    \"email\": \"jsnow@casd.local\"\n}\n</code></pre> <p>Below are some useful urls</p> <pre><code># you can get the realm configuration with the below url\nhttps://keycloak.casd.local/realms/Data-catalog/.well-known/openid-configuration\n\n# you can get the token endpoint\nhttps://keycloak.casd.local/realms/examples/protocol/openid-connect/token\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#use-postman-to-test-the-above-request","title":"Use postman to test the above request","text":"<ol> <li>Create a new collection <code>keycloak API test</code></li> <li>Create a request <code>Get keycloak access token</code>, this will simulate the first curl request</li> <li>In <code>Get keycloak access token</code>, In http method, choose <code>POST</code>, in url, put <code>https://keycloak.casd.local/realms/Data-catalog/protocol/openid-connect/token</code>    In <code>Headers</code>, add two row <code>Content-Type:application/x-www-form-urlencoded</code>, <code>Accept:application/json</code>. In <code>Body</code>,    add the below rows <code>grant_type:password client_id:open-metadata client_secret:changeMe scope:openid username:jsnow password:jsnow</code></li> <li>Copy the access token in the response body</li> <li>Create a new request <code>Verify keycloak access token</code></li> <li>In http method, choose <code>GET</code>, in url, put <code>https://keycloak.casd.local/realms/Data-catalog/protocol/openid-connect/userinfo</code>    In <code>Authorization</code>, in <code>Type</code> choose <code>Bearer token</code>, Copy the access token in Token input line.</li> <li>After sending the request, you should receive the repose with user profile</li> </ol> <p>The default access token is only valid for 5 mins, so it's normal your token is no longer valid after 5 mins. You can  Change this in Keycloak -&gt; realm settings -&gt; token</p>"},{"location":"security/keycloak/Install_keycloak/#troubleshoot","title":"Troubleshoot","text":""},{"location":"security/keycloak/Install_keycloak/#css-loading-issue-after-hosting-keycloak-on-linux-with-nginx-as-proxy-server","title":"CSS loading issue after hosting keycloak on linux with nginx as proxy server","text":"<p>https://github.com/keycloak/keycloak/issues/12719</p>"},{"location":"security/keycloak/Install_keycloak/#cors-header-access-control-allow-origin-missing","title":"CORS header 'Access-Control-Allow-Origin' missing","text":"<p>You can find more info about this issue here: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors/CORSMissingAllowOrigin</p> <p>You can also check this page https://www.stackhawk.com/blog/react-cors-guide-what-it-is-and-how-to-enable-it/</p> <p>The response to the CORS request is missing the required Access-Control-Allow-Origin header, which is used to determine  whether the resource can be accessed by content operating within the current origin.</p> <p>To correct this issue, you need to add the appropriate <code>Access-Control-Allow-Origin</code> values into the HTTP header.</p> <p>In this tutorial, we used the nginx as our reverse proxy. The below lines show a possible config for nginx. The idea is the nginx server checks the <code>origin value of the http request header</code>. Base on this value, it will set  the <code>Access-Control-Allow-Origin</code> on the http response header.</p> <pre><code># set default values for the cors values \n  set $cors_origin \"\";\n  set $cors_cred   true;\n  set $cors_header \"Content-Type\";\n  set $cors_method \"POST, GET\";\n\n# if requests comes from the allowed domain or subdomain, we assign certain core values \n# nginx create variables for http header, for example the origin header has the equivalent variable $http_origin in nginx \n  if ($http_origin ~* (https?://.*\\.mckinsey\\.com(:[0-9]+)?$)) {\n            set $cors_origin $http_origin;\n            set $cors_cred   true;\n            set $cors_header $http_access_control_request_headers;\n            set $cors_method $http_access_control_request_method;\n  }\n\n# if request comes from null origin, we set another group of core values\n if ($http_origin ~ 'null') {\n            set $cors_origin \"null\";\n            set $cors_cred   true;\n            set $cors_header $http_access_control_request_headers;\n            set $cors_method $http_access_control_request_method;\n  }\n\n# add header to the Http response\n  add_header Access-Control-Allow-Origin      $cors_origin;\n  add_header Access-Control-Allow-Credentials $cors_cred;\n  add_header Access-Control-Allow-Headers     $cors_header;\n  add_header Access-Control-Allow-Methods     $cors_method;\n\n  location / {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $http_host;\n        proxy_pass http://keycloak_backend;\n    }\n</code></pre>"},{"location":"security/keycloak/Install_keycloak/#bash-script-to-test-authorization-code-grant-of-keycloak","title":"Bash script to test Authorization Code grant of Keycloak","text":"<p>This following script expects <code>KEYCLOAK_URL</code>, <code>REDIRECT_URL</code> (it is the client application URL, in this case  it can be any URL), <code>REALM</code> (Keycloak realm), <code>CLIENTID</code> (client application created in Keycloak) and the <code>USERNAME</code>.</p> <pre><code>#!/bin/bash\n\n# This script will perform the following steps:\n#\n# 1. Initialize variables and functions.\n# 2. Prompt for the user's password.\n# 3. Obtain the authentication URL from Keycloak.\n# 4. Send username and password to Keycloak to receive a code URL.\n# 5. Extract the code from the received URL.\n# 6. Send the code to Keycloak to receive the Access Token.\n# 7. Decode and display the Access Token.\n# 8. Clean up the cookie file used for authentication.\n\n# Initialize variables\ninit() {\n    KEYCLOAK_URL=\"https://keycloak.casd.local\"\n    REDIRECT_URL=\"http://localhost:8080\"\n    USERNAME=\"jsnow\"\n    REALM=\"Data-catalog\"\n    CLIENTID=\"open-metadata\"\n}\n\n# Function to decode the access token\ndecode() {\n    jq -R 'split(\".\") | .[1] | @base64d | fromjson' &lt;&lt;&lt; \"$1\"\n}\n\n# Prompt for password\nread -rp \"Password: \" -s PASSWORD\necho \" \"\n\n# Initialize\ninit\n\n# Cookie file path\nCOOKIE=\"$(pwd)/cookie.jar\"\n\n# Step 1: Obtain the authentication URL\nAUTHENTICATE_URL=$(curl -sSL --get --cookie \"$COOKIE\" --cookie-jar \"$COOKIE\" \\\n    --data-urlencode \"client_id=${CLIENTID}\" \\\n    --data-urlencode \"redirect_uri=${REDIRECT_URL}\" \\\n    --data-urlencode \"scope=openid\" \\\n    --data-urlencode \"response_type=code\" \\\n    \"$KEYCLOAK_URL/realms/$REALM/protocol/openid-connect/auth\" | pup \"form#kc-form-login attr{action}\")\n\n# Convert &amp;amp; to &amp;\nAUTHENTICATE_URL=$(echo \"$AUTHENTICATE_URL\" | sed -e 's/\\&amp;amp;/\\&amp;/g')\n\necho \"Sending Username Password to the following authentication URL of Keycloak: $AUTHENTICATE_URL\"\necho \" \"\n\n# Step 2: Obtain the code URL\nCODE_URL=$(curl -sS --cookie \"$COOKIE\" --cookie-jar \"$COOKIE\" \\\n    --data-urlencode \"username=$USERNAME\" \\\n    --data-urlencode \"password=$PASSWORD\" \\\n    --write-out \"%{REDIRECT_URL}\" \\\n    \"$AUTHENTICATE_URL\")\n\necho \"Following URL with code received from Keycloak: $CODE_URL\"\necho \" \"\n\n# Extract code from URL\ncode=$(echo \"$CODE_URL\" | awk -F \"code=\" '{print $2}' | awk '{print $1}')\n\necho \"Extracted code: $code\"\necho \" \"\n\necho \"Sending code=$code to Keycloak to receive Access token\"\necho \" \"\n\n# Step 3: Obtain the Access Token\nACCESS_TOKEN=$(curl -sS --cookie \"$COOKIE\" --cookie-jar \"$COOKIE\" \\\n    --data-urlencode \"client_id=$CLIENTID\" \\\n    --data-urlencode \"redirect_uri=$REDIRECT_URL\" \\\n    --data-urlencode \"code=$code\" \\\n    --data-urlencode \"grant_type=authorization_code\" \\\n    \"$KEYCLOAK_URL/realms/$REALM/protocol/openid-connect/token\" | jq -r \".access_token\")\n\necho \" \"\n\n# Print decoded Access Token\necho \"Decoded Access Token: \"\ndecode \"$ACCESS_TOKEN\"\n\n# Clean up the cookie file\nrm \"$COOKIE\"\n</code></pre> <p>you can find the above bash script in ./src/keycloak/keycloak_auth_code.bash</p>"},{"location":"security/ldap/01.install_ldap/","title":"Install openldap server on debian 11/10","text":""},{"location":"security/ldap/01.install_ldap/#1-prepare-the-server","title":"1. Prepare the server","text":"<p>Before installing the ldap server, you need to prepare it.</p>"},{"location":"security/ldap/01.install_ldap/#11-configure-fqdn-hostname-for-your-server","title":"1.1  Configure FQDN hostname for your server","text":"<p>You need to create a FQDN hostname and add a record to file/etc/hosts.</p> <pre><code>sudo vim /etc/hosts\n10.50.5.57 ldap.casd.local\n\n# Configure hostname\nsudo hostnamectl set-hostname ldap.casd.local --static\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#12-update-the-server","title":"1.2 Update the server","text":"<pre><code># fix the hashicorp repo key\ncurl -fsSL https://apt.releases.hashicorp.com/gpg | sudo gpg --yes --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\n\n echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list &gt; /dev/null\n\nsudo apt -y update &amp;&amp; sudo apt -y upgrade\nsudo reboot\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#2-install-the-openldap-package","title":"2. Install the openldap package","text":"<pre><code>sudo apt -y install slapd ldap-utils\n</code></pre> <ul> <li><code>slapd</code>: is the openldap server.</li> <li><code>ldap-utils</code>: is the ldap cli.</li> </ul> <p>After the above command, you will be prompted to enter the admin password for your LDAP directory</p> <p>The base dn of the ldap server will be generated base on your FQDN hostname. So if your fqdn is not right, don't continue just restart from step 1.</p>"},{"location":"security/ldap/01.install_ldap/#3-set-up-the-base-structure","title":"3. Set up the base structure","text":"<p>You can consider the ldap server as a database of your users and groups. To better organise them, we need to create some basic structures.</p>"},{"location":"security/ldap/01.install_ldap/#31-create-olc-admin-account","title":"3.1 Create olc admin account","text":"<p>Openldap has two admin accounts, the installation guide will help you to set up the <code>front-end</code> admin account(e.g. cn=admin, dc=casd, dc=loacl). There is a backend admin account <code>cn=admin,cn=config</code> which allows you to access the <code>ldap olc</code>(cn=config).</p> <pre><code># sudo su become root user\n# show the content of cn=config\nldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config\n</code></pre> <p>Normally the <code>olcRootDN</code> entry is already created by default, all you need to do is to modify the <code>olcRootPW</code> You can use <code>slappasswd</code> to generate the ssha of your password.</p> <pre><code># we name this conf file as change_pwd_config.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype: modify\nadd: olcRootPW\nolcRootPW: {SSHA}33aeJH8tsp6+NxNg9LIK9VjUtmhYTOnV\n</code></pre> <pre><code>ldapmodify -Y EXTERNAL -H ldapi:/// -f change_pwd_config.ldif\n</code></pre> <pre><code># we name this file as create_account_config.ldif\ndn: olcDatabase={0}config,cn=config\nchangetype: modify\nadd: olcRootDN\nolcRootDN: cn=admin,cn=config\n\ndn: olcDatabase={0}config,cn=config\nchangetype: modify\nadd: olcRootPW\nolcRootPW: {SSHA}33aeJH8tsp6+NxNg9LIK9VjUtmhYTOnV\n</code></pre> <pre><code>ldapadd -Y EXTERNAL -H ldapi:/// -f create_account_config.ldif\n</code></pre> <p>to access cn=config in <code>Apache Directory Studio</code>, the login will be <code>cn=admin,cn=config</code>, the password is the password in olcRootPW, the root tree will be <code>cn=config</code>. </p>"},{"location":"security/ldap/01.install_ldap/#32-enable-ldaps","title":"3.2 Enable ldaps","text":"<p>To set up ldaps, you need three files:   1. CA certificate (location: /etc/ssl/certs/mycacert.pem)   2. service certificate for ldap signed by CA certificate (location: /etc/ldap/ldap_cert.pem)   3. private key of the service certificate (location: /etc/ldap/ldap_pri_key.pem)</p> <p>you need to add the below config into <code>cn=config</code></p> <pre><code># certinfo.ldif\ndn: cn=config\nadd: olcTLSCACertificateFile\nolcTLSCACertificateFile: /etc/ssl/certs/mycacert.pem\n-\nadd: olcTLSCertificateFile\nolcTLSCertificateFile: /etc/ldap/ldap_cert.pem\n-\nadd: olcTLSCertificateKeyFile\nolcTLSCertificateKeyFile: /etc/ldap/ldap_pri_key.pem\n</code></pre> <pre><code>sudo ldapmodify -Y EXTERNAL -H ldapi:/// -f certinfo.ldif\n</code></pre> <p>To use LDAPS (LDAP over SSL), then you need to edit /etc/default/slapd and include <code>ldaps:///</code> in SLAPD_SERVICES like below:</p> <pre><code># open file\nsudo vim /etc/default/slapd\n\n# find the below line and add ldaps as authorized protocol\nSLAPD_SERVICES=\"ldap:/// ldapi:/// ldaps:///\"\n</code></pre> <pre><code># restart slapd\nsudo systemctl restart slapd\n\n# test the ldaps\nldapwhoami -x -H ldaps://ldap.casd.local\n</code></pre> <p>To use apache directory studio, in the network tab, you need to change the port to 636, and encryption method to ldaps.</p>"},{"location":"security/ldap/01.install_ldap/#33-add-some-basic-structure","title":"3.3 Add some basic structure","text":"<p>Below is an example, you can set up something more complex</p> <pre><code>dn: ou=people,dc=casd,dc=local\nobjectClass: organizationalUnit\nou: people\n\ndn: ou=groups,dc=casd,dc=local\nobjectClass: organizationalUnit\nou: groups\n</code></pre> <p>Load the above entry to the ldap server</p> <pre><code>sudo ldapadd -x -D cn=admin,dc=casd,dc=local -W -f basedn.ldif\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#4-add-sample-user-account-and-group","title":"4. Add sample user account and group","text":""},{"location":"security/ldap/01.install_ldap/#41-add-a-new-user-account","title":"4.1 Add a new user account","text":"<ol> <li>Create a password hash for the user account</li> </ol> <pre><code>sudo slappasswd\nNew password:\nRe-enter new password:\n{SSHA}vjbMsVOMBOyB2/oZ1tiFGptF/ArMGwGH\n</code></pre> <ol> <li>Create a <code>user.ldif</code> file</li> </ol> <pre><code>dn: uid=pliuT,ou=people,dc=casd,dc=local\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: shadowAccount\ncn: Pengfei\nsn: Liu\nuserPassword: {SSHA}vjbMsVOMBOyB2/oZ1tiFGptF/ArMGwGH\nloginShell: /bin/bash\nhomeDirectory: /home/users/pliu\nuidNumber: 3000\ngidNumber: 3000\n</code></pre> <ol> <li>Add it to the ldap server</li> </ol> <pre><code>sudo ldapadd -x -D cn=admin,dc=casd,dc=local -W -f user.ldif\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#42-add-a-new-group","title":"4.2 Add a new group","text":"<ol> <li>Create a <code>group.ldif</code></li> </ol> <pre><code>dn: cn=developers,ou=groups,dc=casd,dc=local\nobjectClass: posixGroup\ncn: developers\ngidNumber: 3000\nmemberUid: pliuT\n</code></pre> <ol> <li>Add it to the ldap server</li> </ol> <pre><code>sudo ldapadd -x -D cn=admin,dc=casd,dc=local -W -f group.ldif\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#5-install-a-ldap-client","title":"5. Install a ldap client","text":"<p>We already have a ldap client in CLI which you can use after installing <code>ldap-utils</code>.</p> <pre><code>ldapsearch -x -LLL -b dc=casd,dc=local '(uid=pengfei)' cn gidNumber\n\n# sample output\ndn: uid=pengfei,ou=people,dc=casd,dc=local\ngidNumber: 4000\ncn: pengfei\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#51-ldap-client-with-gui","title":"5.1 Ldap client with GUI","text":"<p>We have many choices for advance ldap client with GUI. But I recommend <code>Apache Directory Studio</code>.</p> <p>After download, unzip it and run the command <code>./ApacheDirectoryStudio</code></p> <p>Inside the GUI, create a new <code>ldap connection</code>, enter the server host name and port.</p> <p>Then enter the admin acount and password e.g. <code>cn=admin,dc=casd,dc=local</code>.</p> <p>If everything works well, you should see the content of the ldap server.</p>"},{"location":"security/ldap/01.install_ldap/#6-enable-saslgssapi-in-openldap","title":"6. Enable SASL/GSSAPI in openldap","text":"<p>GSSAPI (Generic Security Services API) allows OpenLDAP to authenticate users using Kerberos instead of  <code>simple binds with passwords</code>. This is commonly used in <code>Active Directory (AD) or MIT Kerberos</code> environments.</p> <p>To complete this config, you must have one kerberos server(kdc) up and running. Here, we suppose the kerberos server is running on a server with url such as <code>krb.casd.local</code> with a REALM called <code>CASD.LOCAL</code>.</p> <p>the realm name is case-sensitive, by convention, it should be all in upper-case. </p>"},{"location":"security/ldap/01.install_ldap/#61-install-required-packages","title":"6.1 Install required packages","text":"<pre><code>sudo apt update\nsudo apt install krb5-user libsasl2-modules-gssapi-mit\n</code></pre> <p>krb5-user: kerberos client which allows user to do kinit, klist, kdestroy libsasl2-modules-gssapi-mit:  SASL GSSAPI module for OpenLDAP to allow user kerberos ticket bind.</p>"},{"location":"security/ldap/01.install_ldap/#62-configure-kerberos-authentication","title":"6.2 Configure Kerberos Authentication","text":"<p>The kerberos client authentication config is located at <code>/etc/krb5.conf</code>. Below is an example of the basic config of the kerberos client.</p> <pre><code>[libdefaults]\n    default_realm = CASD.LOCAL\n        dns_lookup_realm = false\n        dns_lookup_kdc = true\n        ticket_lifetime = 24h\n        renew_lifetime = 7d\n        forwardable = true\n# The following krb5.conf variables are only for MIT Kerberos.\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        proxiable = true\n\n\n[realms]\n    CASD.LOCAL = {\n        kdc = krb.casd.local\n        admin_server = krb.casd.local\n    }\n\n[domain_realm]\n       casd.local = CASD.LOCAL\n       .casd.local = .CASD.LOCAL\n</code></pre> <p>With the above conf, you should be able to test the connectivity of the kerberos client.</p> <pre><code># obtain a krb ticket from the kdc\nkinit &lt;user-principal&gt;\n\n# for example\nkinit pengfei@CASD.LOCAL\n\n# show the ticket\nklist\n\n# destroy the cached ticket\nkdestroy\n</code></pre>"},{"location":"security/ldap/01.install_ldap/#63-c","title":"6.3 C","text":""},{"location":"security/pki/08.PKI_cfssl/","title":"7. PKI","text":"<p>In this tutorial, we will use cfssl to build a PKI (CA and all that).</p>"},{"location":"security/pki/08.PKI_cfssl/#71-what-is-cfssl","title":"7.1 What is CFSSL?","text":"<p>CFSSL is CloudFlare's PKI/TLS swiss army knife. It is both a command line tool and an HTTP API server for signing, verifying, and bundling TLS certificates. It requires Go 1.16+ to build.</p> <p>You can visit the official repo for more information.</p> <p>CFSSL consists of a set of packages useful for building custom TLS PKI tools - The <code>cfssl</code> program, which is the canonical command line utility using the CFSSL packages. - The <code>multirootca</code> program, which is a certificate authority server that can use multiple signing keys. - The <code>mkbundle</code> program is used to build certificate pool bundles. - The <code>cfssljson</code> program, which takes the JSON output from the cfssl and multirootca programs and writes certificates, keys, CSRs, and bundles to disk.</p>"},{"location":"security/pki/08.PKI_cfssl/#72-installation","title":"7.2 Installation","text":"<pre><code>mkdir -p /tmp/cfssl\n\ncd /tmp/cfssl\n\n# download the bin from https://pkg.cfssl.org\ncurl -s -L -o cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\n\ncurl -s -L -o cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n\ncurl -s -L -o cfssl-certinfo https://pkg.cfssl.org/R1.2 cfssl-certinfo_linux-amd64\n\nchmod +x cfssl*\n\n# move these bin to ~/.local/bin\nmv cfssl* ~/.local/bin\n\n# add ~/.local/bin to your path, \nvim ~/.bashrc\n\n# add below line \nexport PATH=\"/home/pliu/.local/bin:$PATH\"\n\n# now test cfssl\n$ cfssl -h\n\n# You should see below output\nUsage:\nAvailable commands:\n        gencrl\n        ocspsign\n        ocspserve\n        print-defaults\n        serve\n        version\n        ocspdump\n        scan\n        sign\n        genkey\n        gencert\n        info\n        certinfo\n        ocsprefresh\n        selfsign\n        revoke\n        bundle\nTop-level flags:\n  -allow_verification_with_non_compliant_keys\n        Allow a SignatureVerifier to use keys which are technically non-compliant with RFC6962.\n  -loglevel int\n        Log level (0 = DEBUG, 5 = FATAL) (default 1)\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#73-generate-a-certificate-authority","title":"7.3  Generate a certificate authority","text":"<p>We store the files that we create during this tutorial in the <code>creds</code> directory.</p> <pre><code># create working folder\nmkdir -p ~/local_ca/creds\n\ncd ~/local_ca\n\n# generate default config and csr\ncfssl print-defaults config &gt; config.json\ncfssl print-defaults csr &gt; csr.json\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#731-create-root-ca-csr","title":"7.3.1 Create Root CA csr","text":"<p>Now let's check the generated <code>csr.json</code> </p> <pre><code>{\n    \"CN\": \"example.net\",\n    \"hosts\": [\n        \"example.net\",\n        \"www.example.net\"\n    ],\n    \"key\": {\n        \"algo\": \"ecdsa\",\n        \"size\": 256\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"ST\": \"San Francisco\"\n        }\n    ]\n}\n</code></pre> <p>We need to modify it base on your situation. Below is an example for an organization called casd.</p> <pre><code>{\n    \"CN\": \"casd.local\",\n    \"hosts\": [\n        \"casd.local\",\n        \"www.casd.local\"\n    ],\n    \"key\": {\n        \"algo\": \"ecdsa\",\n        \"size\": 256\n    },\n    \"names\": [\n        {\n            \"C\": \"FR\",\n            \"L\": \"Malakoff\",\n            \"O\": \"CASD\"\n        }\n    ]\n}\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#732-generate-root-ca-certificate","title":"7.3.2 Generate Root CA certificate","text":"<p>We use the above <code>csr.json</code> to issue the following certificate authority for <code>casd.local</code> in two files:</p> <ul> <li><code>ca.pem</code>, the certificate authority;</li> <li><code>ca-key.pem</code>, the private key linked to this certificate authority.</li> </ul> <pre><code># this command will generate three files: ca.csr, ca.pem, ca-key.pem\n# note the creds/ca is the prefix of the output file, if you put toto,\n# the generated file will be toto.csr, toto.pem, etc.\ncfssl gencert -initca csr.json | cfssljson -bare creds/ca\n</code></pre> <p>You can check the generated certificate by using below command</p> <pre><code>openssl x509 -in casd-signed.pem -text -noout\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#731-root-ca-profile-config","title":"7.3.1 Root CA profile config","text":"<p>The <code>config.json</code> file parameterizes the subsequent creation of subsidiary certificates for this certificate authority.  It defines the capabilities of the <code>CA</code> for different usage (<code>profile</code>) of the certificates, which change depending on the needs.</p> <p>Check the generated <code>config.json</code></p> <pre><code>{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"168h\"\n        },\n        \"profiles\": {\n            \"www\": {\n                \"expiry\": \"8760h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"8760h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n</code></pre> <p>You will notice, the default expiration time is too short. We can put it to <code>28800</code> = 1200 days. In the <code>profiles</code> section, we define the capabilities of the <code>CA</code> for each profile.</p> <p>Below is an example of config.json, for <code>intermeidate_ca</code> issued by this ca, it has <code>signing, client authentication, server authentication, certificate signing, key encipherment</code> capacities.</p> <pre><code>{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"28800h\"\n    },\n    \"profiles\": {\n      \"intermediate_ca\": {\n        \"usages\": [\n            \"signing\",\n            \"digital signature\",\n            \"key encipherment\",\n            \"cert sign\",\n            \"crl sign\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"8760h\",\n        \"ca_constraint\": {\n            \"is_ca\": true,\n            \"max_path_len\": 0, \n            \"max_path_len_zero\": true\n        }\n      },\n      \"peer\": {\n        \"usages\": [\n            \"signing\",\n            \"digital signature\",\n            \"key encipherment\", \n            \"client auth\",\n            \"server auth\"\n        ],\n        \"expiry\": \"8760h\"\n      },\n      \"server\": {\n        \"usages\": [\n          \"signing\",\n          \"digital signing\",\n          \"key encipherment\",\n          \"server auth\"\n        ],\n        \"expiry\": \"8760h\"\n      },\n      \"client\": {\n        \"usages\": [\n          \"signing\",\n          \"digital signature\",\n          \"key encipherment\", \n          \"client auth\"\n        ],\n        \"expiry\": \"8760h\"\n      }\n    }\n  }\n}\n</code></pre> <p>For our case, we will use a simpler config.json</p>"},{"location":"security/pki/08.PKI_cfssl/#74-use-cases-of-ca","title":"7.4 Use cases of CA","text":""},{"location":"security/pki/08.PKI_cfssl/#741-issue-an-intermediate-certificate","title":"7.4.1 Issue an intermediate certificate","text":"<p>First create a csr of the intermediate certificate. </p> <pre><code>{\n  \"CN\": \"Onyxia Intermediate CA\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\":  \"FR\",\n      \"L\":  \"PARIS\",\n      \"O\":  \"CASD\",\n      \"OU\": \"Servers Intermediate CA\"\n    }\n  ],\n  \"ca\": {\n    \"expiry\": \"42720h\"\n  }\n}\n</code></pre> <p>Generate the public key pair</p> <pre><code>mkdir intermediate; cd intermediate \n\n# This command will generate intermediate.csr  intermediate-key.pem  intermediate.pem\ncfssl gencert -initca intermediate.json | cfssljson -bare intermediate\n</code></pre> <p>Now we need to sign the intermediate with ROOT CA</p> <pre><code># -ca specifies the Root certifcate\n# -ca-key specifies the private key of the Root certificate\n# -config specifies the CA config file\n# -profile specifies the profile name in the config file follow by the csr which needs to be signed\n# cfssljson -bare will translate the json output into the standard .pem format\n# intermediate/intermediate is the prefix of the output file \ncfssl sign -ca creds/ca.pem \\\n  -ca-key creds/ca-key.pem \\\n  -config config.json \\\n  -profile intermediate intermediate/intermediate.csr | cfssljson -bare intermediate/intermediate\n\n# You should see below output\n[INFO] signed certificate with serial number 196691997370633874902371540997745783818537484259\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#742-singing-a-website-certificate","title":"7.4.2 Singing a website certificate","text":"<p>Create a csr with below <code>minio-server.json</code> file.</p> <pre><code>{\n  \"CN\": \"minio.casd.local\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n  {\n    \"C\": \"FR\",\n    \"L\": \"Paris\",\n    \"O\": \"CASD\",\n    \"OU\": \"Computing Webserver1\"\n  }\n  ],\n  \"hosts\": [\n    \"minio.casd.local\",\n    \"localhost\"\n  ]\n}\n</code></pre> <pre><code>cfssl sign -ca creds/ca.pem \\\n  -ca-key creds/ca-key.pem \\\n  -config config.json \\\n  -profile=server minio.json | cfssljson -bare minio-server1\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#743-signing-a-wildcard-certificate","title":"7.4.3 Signing a wildcard certificate","text":"<p>Step1: Create a csr with below <code>wildcard-casd.json</code> file.</p> <pre><code>{\n  \"CN\": \"*.casd.local\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n  {\n    \"C\": \"FR\",\n    \"L\": \"Paris\",\n    \"O\": \"CASD\",\n    \"OU\": \"Computing Web Service\"\n  }\n  ],\n  \"hosts\": [\n    \"*.casd.local\"\n  ]\n}\n</code></pre> <pre><code>mkdir wildcard; cd wildcard\n\nvim wildcard-casd.json\n# this command will generate three files: ca.csr, ca.pem, ca-key.pem\n# note the creds/ca is the prefix of the output file, if you put toto,\n# the generated file will be toto.csr, toto.pem, etc.\ncfssl gencert -initca wildcard-casd.json | cfssljson -bare wildcard-casd\n</code></pre> <p>The above command will generate three file: - wildcard-casd.csr : request - wildcard-casd-key.pem  : private key - wildcard-casd.pem : certificate unsigned</p> <p>Step2: Sign the csr with root CA</p> <pre><code>cfssl sign -ca creds/ca.pem \\\n  -ca-key creds/ca-key.pem \\\n  -config config.json \\\n  -profile=server wildcard/wildcard-casd.csr | cfssljson -bare casd-signed\n</code></pre> <p>The above command will genertate two files: - casd-signed.csr - casd-signed.pem </p>"},{"location":"security/pki/08.PKI_cfssl/#check-output-certificate","title":"Check output certificate","text":"<p>After the generation, we need to test the validity of the certificate and the private key. Most importantly, we need to check if the private key and the certificate matches</p> <pre><code># You can verify the generated certificate content with below command\nopenssl x509 -in casd-signed.pem -text -noout\n\n# verify the validity of the private key in rsa\nopenssl rsa -check -noout -in wildcard-casd.key \n\n# verify the validity of the private key in ecdsa\nopenssl ec -check -noout -in wildcard-casd.key \n\n# normal output\n&gt; RSA key ok\n\n# Get the Modulus Value of the certificate\nopenssl x509 -noout -modulus -in casd-signed.pem\n\n# Get the Modulus Value of the Private Key of rsa\nopenssl rsa -noout -modulus -in wildcard-casd.key \n\n\n# to facilitate the comparison, we can convert the modulus into md5\nopenssl x509 -noout -modulus -in casd-signed.pem | openssl md5\nopenssl rsa -noout -modulus -in wildcard-casd.key | openssl md5 \n</code></pre> <p>For ECDSA, you can not test the conformity by getting the modulus. But we can extract public key from the private key and the signed certificate. If the extracted public key is the same. Then the private key and certificate is a match.</p> <pre><code># general form to get the public key from private key\nopenssl pkey -in wildcard-casd-key.pem -pubout\n\n# get the public key from certificate\nopenssl x509 -in casd-signed.pem -pubkey\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#perform-encryption-with-public-key-from-certificate-and-decryption-with-private-key","title":"Perform Encryption with Public Key from certificate and Decryption with Private Key","text":"<p>You can follow the below step to encrypt and decrypt the data</p> <pre><code># 1. Get the public key from the certificate\nopenssl x509 -in casd-signed.pem -noout -pubkey &gt; pubkey.cer\n\n# 2. Encrypt test.txt file content using the public key\n# Create a new file called test.txt file with the content \"message test\". Perform the following command to \n# create an encrypted message to cipher.txt file.\nopenssl pkeyutl -encrypt -in test.txt -pubin -inkey pubkey.cer -out cipher.txt \n\n# 3. Decrypt from cipher.txt using the private key\nopenssl  pkeyutl  -decrypt -in cipher.txt -inkey wildcard-casd.key\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#744-bundling-certificates","title":"7.4.4 Bundling Certificates","text":"<p>mkbundle is used to build the root and intermediate bundles used in verifying certificates. It basically  chains the end certificate with the intermediate CA and Root CA public keys. </p> <p>It takes a collection of certificates, checks for CRL revocation (OCSP support is planned for the next release)  and expired certificates, and bundles them into one file. It takes directories of certificates and certificate  files (which may contain multiple certificates). For example, if the directory intermediates contain a number of  intermediate certificates, run: <code>mkbundle -f bundle.crt intermediates</code></p> <p>In order to bundle our certificates, ensure that the Root CA public key (ca.pem) and the intermediate public  keys are in the same directory. All of mine will be copied into the bundle directory. Let us follow the convention  above to bundle our generated certificates.</p> <pre><code>mkdir bundle\n\ncp creds/ca.pem bundle/.\ncp intermediate/intermediate.pem bundle/.\nmkbundle -f minio-server1.crt bundle\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#75-addremove-certificate-as-trusted-ca","title":"7.5 Add/Remove certificate as trusted CA","text":"<p>We can add a certificate as a trusted certificate in a Server (CA or not). </p>"},{"location":"security/pki/08.PKI_cfssl/#for-debian","title":"For debian","text":"<p>To add a CA: </p> <pre><code># note you must change the .pem to .crt, otherwise, it won't work\nsudo cp creds/ca.pem /usr/local/share/ca-certificates/ca.crt\nsudo update-ca-certificates\n\n# Remove your CA.\n</code></pre> <p>Another solution, you only need to copy the certificate to the <code>/etc/ssl/certs/</code> folder</p> <pre><code>sudo cp creds/ca.crt /etc/ssl/certs/ca.crt\n</code></pre> <p>To remove a CA</p> <pre><code># remove the ca.crt\n# Update the CA store\nsudo update-ca-certificates --fresh\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#for-centos","title":"For Centos","text":"<pre><code># Install the ca-certificates package: \nyum install ca-certificates\n\n# Enable the dynamic CA configuration feature: \nupdate-ca-trust force-enable\n\n# Add it as a new file to /etc/pki/ca-trust/source/anchors/: \ncp foo.crt /etc/pki/ca-trust/source/anchors/\n\n# Update ca trust store\nupdate-ca-trust extract\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#for-windows","title":"For Windows","text":"<pre><code># Add   root CA\ncertutil -addstore -f \"ROOT\" ca.crt\n\n# Remove root CA    \ncertutil -delstore \"ROOT\" serial-number-hex\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#for-macos","title":"For MacOS","text":"<pre><code># Add root CA   \n\nsudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/ca.crt\n\n# Remove root CA\nsudo security delete-certificate -c \"&lt;name of existing certificate&gt;\"\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#75-certificate-format","title":"7.5 Certificate format","text":"<p>A complete doc can be found here</p>"},{"location":"security/pki/08.PKI_cfssl/#pem-format","title":"PEM format","text":"<p>PEM (Privacy Enhanced Mail) is a widely used format for storing and sharing cryptographic objects such  as <code>X.509 certificates, private keys</code>, and other related information. PEM files are typically  ASCII-encoded, with a header, the data, and a footer. Below is a general template for a PEM-encoded X.509 certificate:</p> <pre><code>-----BEGIN CERTIFICATE-----\n&lt;base64-encoded certificate data&gt;\n-----END CERTIFICATE-----\n</code></pre> <p>The general file extension are: <code>.cer, .crt, .pem</code> for certificate or  <code>.key</code> for private key.</p>"},{"location":"security/pki/08.PKI_cfssl/#der-format","title":"DER format","text":"<p>The X.509 certificate is typically encoded in binary <code>DER (Distinguished Encoding Rules) format</code>. DER is a standard  for encoding data structures defined by the <code>Abstract Syntax Notation One (ASN.1) standard</code>.</p> <p>You can convert a PEM format certificate</p> <pre><code>openssl x509 -in certificate.pem -outform der -out certificate.der\n</code></pre>"},{"location":"security/pki/08.PKI_cfssl/#references","title":"References","text":"<ul> <li>TLS in Kubernetes</li> <li>Chain of certificates</li> <li>clfssl certificate authority guide</li> <li>introducing-cfssl</li> </ul>"}]}