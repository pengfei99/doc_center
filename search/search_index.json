{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Pengfei doc center","text":"<p>This website is build by using <code>mkdocs</code>. For more information about <code>mkdocs</code>, please visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/","title":"Ansible roles","text":"<p>Read this doc  for more details on how to write an ansible role</p> <p>An Ansible Role is a <code>self-contained, portable unit</code> of Ansible automation that serves as the preferred method for  grouping related tasks and associated variables, files, handlers, and other assets in a known file structure.  While automation tasks can be written exclusively in an Ansible Playbook, Ansible Roles allow you to create bundles  of automation content that can be  - run in 1 or more plays,  - reused across playbooks,  - shared with other users in collections.</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#role-organization","title":"role organization","text":"<p><code>Ansible Roles</code> are expressed in YAML files. When a role is included in a task or a play, Ansible looks for  a <code>main.yml</code> file in at least 1 of 8 standard role directories such as : - tasks,  - handlers,  - modules,  - defaults,  - variables,  - files,  - templates, - meta.</p> <pre><code>roles/\n    my_role1/               # this hierarchy represents a \"role\"\n        tasks/            #\n            main.yml      #  &lt;-- tasks file can include smaller files if warranted\n        handlers/         #\n            main.yml      #  &lt;-- handlers file\n        templates/        #  &lt;-- files for use with the template resource\n            ntp.conf.j2   #  &lt;------- templates end in .j2\n        files/            #\n            bar.txt       #  &lt;-- files for use with the copy resource\n            foo.sh        #  &lt;-- script files for use with the script resource\n        vars/             #\n            main.yml      #  &lt;-- variables associated with this role\n        defaults/         #\n            main.yml      #  &lt;-- default lower priority variables for this role\n        meta/             #\n            main.yml      #  &lt;-- role dependencies\n        library/          # roles can also include custom modules\n        module_utils/     # roles can also include custom module_utils\n        lookup_plugins/   # or other types of plugins, like lookup in this case\n\n    my_role2/              # same kind of structure as \"my_role1\" was above, but for another purpose\n    my_role3/              # \"\"\n    my_role4/              # \"\"\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#role-vs-playbook","title":"Role vs Playbook","text":"<p>Why use an Ansible Role instead of an Ansible Playbook? Ansible Roles and Ansible Playbooks are both tools for organizing and executing automation tasks, but each serves a different purpose. Whether you choose to create Ansible Roles or write all of your tasks in an Ansible Playbook depends on your specific use case and your experience with Ansible.</p> <p>Most automation developers and system administrators begin creating automation content with individual playbooks. A playbook is a list of automation tasks that execute for a defined inventory. Tasks can be organized into a play\u2014a grouping of 1 or more tasks mapped to a specific host and executed in order. A playbook can contain 1 or more plays, offering a flexible mechanism for executing Ansible automation in a single file.</p> <p>While playbooks are a powerful method for automating with Ansible, writing all of your tasks in a playbook isn\u2019t always the best approach. In instances where scope and variables are complex and reusability is helpful, creating most of your automation content in Ansible Roles and calling them within a playbook may be the more appropriate choice.</p> <p>The following example illustrates the use of a role, linux-systemr-roles.timesync, within a playbook. In this instance, over 4 tasks would be required to achieve what the single role accomplishes. </p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#creating-a-role","title":"Creating a role","text":"<p>You can create a new role skeleton by using <code>ansible-galaxy</code></p> <pre><code> ansible-galaxy role init role_name\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#sharing-a-role","title":"Sharing a role","text":"<p>There are few ways to share your ansible roles:</p> <ul> <li>Ansible Galaxy: A free repository for sharing roles and other Ansible content with the larger Ansible community.             Roles can be uploaded to Ansible Galaxy via the command-line (CLI), whereas collections can be shared                 from the web interface. Since Ansible Galaxy is a community site, content is not vetted, certified.</li> <li>Ansible automation hub: repo for <code>Red Hat Ansible Automation Platform</code>, which is a central repository for                         finding, downloading, and sharing <code>Ansible Content Collections</code>.</li> <li>Private automation hub: An on-premise repository. You can share roles and other automation content within your                             enterprise, allowing teams to simplify workflows and speed up automation. </li> </ul>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-roles-in-an-ansible-playbook","title":"Use roles in an ansible playbook","text":"<p>There are three ways to integre an <code>ansible role</code> in an <code>ansible playbook</code>. - Use the <code>roles</code> option in playbook - Use the <code>include_role</code> in a task  - Use the <code>import_role</code> in a task</p>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-roles-option-in-playbook","title":"Use the <code>roles</code> option in playbook","text":"<p>Below is an example of a playbook which calls the role <code>configure_sshd_pam_sssd_openldap</code> and <code>intall_nginx</code> before tasks. </p> <p>If you have multiple roles, the order is not guarantied with this approach. The roles are executed before tasks. If you want to order the task and roles, use the <code>include_role</code> or <code>import_role</code></p> <pre><code>---\n- hosts: linux_servers\n  roles:\n    - configure_sshd_pam_sssd_openldap\n    - install_nginx\n  tasks:\n    - name: task1\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-include_role-in-a-task","title":"Use the <code>include_role</code> in a task","text":"<p>The content of the role is parsed during the execution of the task. </p> <pre><code>---\n- hosts: linux_servers\n  tasks:\n    - name: Print a message\n      ansible.builtin.debug:\n        msg: \"this task runs before the role1\"\n\n    - name: Include the role with name role1\n      ansible.builtin.include_role:\n        name: role1\n      vars:\n        dir: '/opt/a'\n        app_port: 5000\n</code></pre>"},{"location":"adminsys/os_setup/auto_config/ansible/ansible_roles/#use-the-import_role-in-a-task","title":"Use the <code>import_role</code> in a task","text":"<p>The content of the role is parsed at the start of the playbook. </p>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/","title":"Configure debian server ssh to use pam ldap","text":"<p>We will use libpam-ldapd as the ldap server client and server authenticator to check user login and password via ldap server. It is a newer alternative to the original <code>libpam-ldap</code>. libpam-ldapd uses the same backend <code>(nslcd)</code> as <code>libnss-ldapd</code>, and thus also shares the same configuration file <code>(/etc/nslcd.conf)</code> for LDAP connection parameters. If you're already using libnss-ldapd for NSS, it may be more convenient to use libpam-ldapd's pam_ldap implementation.</p> <p>The /etc/pam.d/common-* files are managed by pam-auth-update (from libpam-runtime).</p> <p>The libpam-ldapd package includes <code>/usr/share/pam-configs/ldap</code>, and running <code>dpkg-reconfigure libpam-runtime</code> will let you configure the <code>pam_unix/pam_ldap</code> module(s) to use in /etc/pam.d/common-*.</p> <p>The nslcd is the name service LDAP connection daemon.</p> <p>Installing the libpam-ldapd package will automatically select the pam_ldap module for use in /etc/pam.d/common-*.</p>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#61-install-the-required-packages","title":"6.1 Install the required packages","text":"<pre><code>sudo apt-get install libnss-ldapd libpam-ldapd\n</code></pre> <p>After the installation, a pop-up window will require you to enter the <code>ldap uri</code> and the <code>base dn</code> of the ldap server</p> <p>For example</p> <pre><code>ldap_uri: ldap://10.50.5.57/ or ldap://ldap.casd.local/\n\nldap_base_dn: dc=casd,dc=local\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#62-edit-the-config","title":"6.2 Edit the config","text":""},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#621-the-first-config-is-etcnslcdconf","title":"6.2.1 The first config is <code>/etc/nslcd.conf</code>","text":"<p>As you already enter some information during installation. This file is filled with some info.</p> <p>Below is a working example.</p> <pre><code># /etc/nslcd.conf\n# nslcd configuration file. See nslcd.conf(5)\n# for details.\n\n# The user and group nslcd should run as.\nuid nslcd\ngid nslcd\n\n# The location at which the LDAP server(s) should be reachable.\nuri ldap://10.50.5.57/\n\n# The search base that will be used for all queries.\nbase dc=casd,dc=local\n\n# The LDAP protocol version to use.\n#ldap_version 3\n\n# The DN to bind with for normal lookups.\n#binddn cn=annonymous,dc=example,dc=net\n#bindpw secret\n\n# The DN used for password modifications by root.\n#rootpwmoddn cn=admin,dc=example,dc=com\n\n# SSL options\n#ssl off\n#tls_reqcert never\n# tls_cacertfile /etc/ssl/certs/ca-certificates.crt\n\n# The search scope.\n#scope sub\n</code></pre> <p>The good practice is not write the <code>binddn</code> and <code>bindpw</code> with admin privilege. If you leave it empty, <code>pam-ldapd</code> will use the current user login and pwd to bind to the ldap. So it's safer.</p>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#622-etcnsswitchconf","title":"6.2.2 /etc/nsswitch.conf","text":"<p>Change the old version to below version</p> <pre><code># /etc/nsswitch.conf\n#\n# Example configuration of GNU Name Service Switch functionality.\n# If you have the `glibc-doc-reference' and `info' packages installed, try:\n# `info libc \"Name Service Switch\"' for information about this file.\n\npasswd:         files ldap\ngroup:          files ldap\nshadow:         files ldap\ngshadow:        files\n\nhosts:          files dns\nnetworks:       files\n\nprotocols:      db files\nservices:       db files\nethers:         db files\nrpc:            db files\n\nnetgroup:       nis\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#623-etcpamdcommon-","title":"6.2.3  /etc/pam.d/common-*","text":"<p>There are a list of config files for pam which are located at  /etc/pam.d/. In our case, we need to modify: - /etc/pam.d/common-auth - /etc/pam.d/common-account - /etc/pam.d/common-session - /etc/pam.d/common-password</p> <pre><code>sudo vim /etc/pam.d/common-auth\n\n# comment the old content, and add below line\nauth      sufficient  pam_unix.so\nauth      sufficient  pam_ldap.so minimum_uid=1000 use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-account\n# comment the old content, and add below line\naccount   required    pam_unix.so\naccount   sufficient  pam_ldap.so minimum_uid=1000\naccount   required    pam_permit.so\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-session\n# comment the old content, and add below line\nsession   required    pam_unix.so\nsession   optional    pam_ldap.so minimum_uid=1000\n# this line will create the user home for first login\nsession    required   pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre> <pre><code>sudo vim /etc/pam.d/common-password\n# comment the old content, and add below line\npassword  sufficient  pam_unix.so nullok md5 shadow use_authtok\npassword  sufficient  pam_ldap.so minimum_uid=1000 try_first_pass\npassword  required    pam_deny.so\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#624-etcsshsshd_config","title":"6.2.4 /etc/ssh/sshd_config","text":"<p>Normally, you don't need to modify the  /etc/ssh/sshd_config. Because the <code>libpam-ldapd</code> will set UsePAM yes automatically for sshd to use PAM authentication.</p> <p>If you have troubles, don't forget to check </p> <p>The above conf is the minimun for the pam-ldapd works. You need to enrich it if you have special requirements</p>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#63-restart-the-service","title":"6.3 Restart the service","text":"<p>As we metioned before, the</p> <pre><code># check the status of the daemon\nsudo systemctl status nscd\nsudo systemctl status nslcd\n\n# restart the service\nsudo systemctl restart nscd\nsudo systemctl restart nslcd\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#64-test-and-troubleshoot","title":"6.4 Test and troubleshoot","text":"<p>To ensure that everything is working correctly you can run </p> <pre><code># this command prints all user account of the server which also includes the users from LDAP\ngetent passwd\n\n# below is an example of user passwd from ldap\ntrigaud:x:3000:3000:Titouan:/home/trigaud:/bin/bash\n\n# below can show the user shadow form ldap too\ngetent shadow \n</code></pre> <p>To test authentication log in with an LDAP user, you can run below command</p> <pre><code># general form to local login\nsu - &lt;UID&gt;\n\n# for example, run below command and enter the pwd. if it's correct, \nsu - trigaud\n</code></pre> <p>To troubleshoot problems you can run <code>nslcd in debug mode</code> (remember to stop nscd when debugging). Debug mode should return a lot of information about the LDAP queries that are performed and errors that may arise.</p> <pre><code>/etc/init.d/nscd stop\n/etc/init.d/nslcd stop\nnslcd -d\n</code></pre>"},{"location":"adminsys/os_setup/security/01.Configure_ssh_pam_ldap/#for-ad-compatibility","title":"For AD compatibility","text":"<p>To use AD as authentication server, we can't use <code>nslcd</code> anymore. We need to test the <code>sssd</code> and <code>AD</code> connexion.</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/","title":"Guide pour int\u00e9grer une machine Debian 11 \u00e0 un domaine Active Directory et configurer SSH avec GSSAPI/Kerberos","text":"<p>In this tutorial, we show how to join a <code>debian 11</code> server </p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#etape-1-preparation-de-la-machine-debian","title":"\u00c9tape 1 : Pr\u00e9paration de la machine Debian","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#11-renommer-la-machine","title":"1.1 Renommer la machine","text":"<p>Modifier le nom d'h\u00f4te selon la politique de l'entreprise :</p> <pre><code>sudo nano /etc/hostname  # Exemple : debian.casdds.casd\nsudo hostnamectl set-hostname \"nouveau_nom\"\n</code></pre> <p>Metter \u00e0 jour <code>/etc/hosts</code> pour inclure le nom et l'IP statique :</p> <pre><code>sudo nano /etc/hosts  # Ajouter : 10.50.5.X   debian.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#12-mise-a-jour-du-systeme","title":"1.2 Mise \u00e0 jour du syst\u00e8me","text":"<pre><code>sudo apt update \n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#13-configurer-le-dns-sur-debian","title":"1.3 Configurer le DNS sur Debian","text":"<p>D\u00e9finir le serveur DNS AD dans <code>/etc/resolv.conf</code> :</p> <pre><code>search casdds.casd\nnameserver 10.50.5.64\nnameserver 8.8.8.8\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#14-installer-les-paquets-necessaires","title":"1.4 Installer les paquets n\u00e9cessaires","text":"<pre><code>sudo apt install realmd sssd sssd-tools libnss-sss libpam-sss adcli samba-common-bin krb5-user oddjob oddjob-mkhomedir packagekit -y\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#etape-2-joindre-le-domaine-active-directory","title":"\u00c9tape 2 : Joindre le domaine Active Directory","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#21-decouvrir-le-domaine","title":"2.1. D\u00e9couvrir le domaine","text":"<p>V\u00e9rifier la connectivit\u00e9 avec le contr\u00f4leur de domaine :</p> <pre><code>realm discover CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#22-rejoindre-le-domaine","title":"2.2. Rejoindre le domaine","text":"<p>Utiliser un compte administrateur AD :</p> <pre><code>sudo realm join --user=Administrateur CASDDS.CASD\n</code></pre> <p>A ce stade, mon client Debian a bien rejoint mon domaine et appara\u00eet dans la console Utilisateurs et Ordinateurs Active Directory de mon serveur Windows. S\u2019il n\u2019apparait pas, on peut l\u2019ajouter manuellement dans Ordinateurs en s\u00e9lectionne l\u2019@ip static et d\u00e9l\u00e9gation kerberos </p> <p>{{:datascience:admin_system:linux:capture1.png?400|}} {{:datascience:admin_system:linux:capture2.png?400|}}</p>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#etape-3-configuration-dns-et-kerberos","title":"\u00c9tape 3 : Configuration DNS et Kerberos","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#31-ajouter-lenregistrement-dns-sur-windows","title":"3.1. Ajouter l'enregistrement DNS sur Windows","text":"<p>Dans le serveur DNS Windows :</p> <p>Ajouter un enregistrement A pour la machine Debian dans la zone Forward Lookup*. (S'il n'est pas pr\u00e9sent) </p> <ul> <li>Cr\u00e9er un enregistrement PTR dans la zone Reverse Lookup.(S'il n'est pas pr\u00e9sent)  {{:datascience:admin_system:linux:capture1.png?400|}} {{:datascience:admin_system:linux:capture4.png?400|}}</li> </ul>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#32-enregistrer-le-spn-service-principal-name","title":"3.2. Enregistrer le SPN (Service Principal Name)","text":"<p>Pour assurer que l'enregistrement existant, on tape: </p> <p>Sur le contr\u00f4leur de domaine (PowerShell administrateur) :   </p> <pre><code>setspn -L debian\n</code></pre> <p>Si host/debian.casdds.casd n'est pas pr\u00e9sent, on l'ajouter avec Powershell admin :</p> <pre><code>setspn -S host/debian.casdds.casd debian\nktpass -princ host/debian.casdds.casd@CASDDS.CASD -mapuser DEBIAN$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#33-verifier-le-keytab-kerberos","title":"3.3. V\u00e9rifier le keytab Kerberos","text":"<p>Sur Debian :</p> <pre><code>klist -k /etc/krb5.keytab  # V\u00e9rifier la pr\u00e9sence de \"host/debian.casdds.casd\"\n</code></pre> <p>Si absent, quitter et rejoigner le domaine :</p> <pre><code>sudo realm leave CASDDS.CASD\nsudo realm join --user=Administrateur CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#34-generation-et-deploiement-dun-fichier-keytab","title":"3.4. G\u00e9n\u00e9ration et d\u00e9ploiement d\u2019un fichier keytab","text":"<p>G\u00e9n\u00e9ration du keytab sur Windows :</p> <pre><code>ktpass -princ user@CASDDS.CASD -mapuser user -crypto AES256-SHA1 -ptype KRB5_NT_PRINCIPAL -pass * -out user.keytab\n</code></pre> <p>Transfert vers Debian :</p> <pre><code>scp user.keytab user@debian.casdds.casd:/tmp/\n</code></pre> <p>Installation et s\u00e9curisation du keytab :</p> <pre><code>sudo cp /tmp/user.keytab /etc/\nsudo chmod 644 /etc/user.keytab\nsudo chown root:root /etc/user.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#etape-4-configuration-de-sssd-pam-et-kerberos","title":"\u00c9tape 4 : Configuration de SSSD, PAM et Kerberos","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#41-fichier-etcsssdsssdconf","title":"4.1. Fichier <code>/etc/sssd/sssd.conf</code>","text":"<pre><code>[sssd]\nservices = nss, pam\ndomains = casdds.casd\nconfig_file_version = 2\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n\n[domain/casdds.casd]\nldap_sasl_authid = user@CASDDS.CASD\nkrb5_keytab = /etc/user.keytab\ndefault_shell = /bin/bash\nkrb5_store_password_if_offline = True\ncache_credentials = True\nkrb5_realm = CASDDS.CASD\nrealmd_tags = manages-system joined-with-adcli\nid_provider = ad\nfallback_homedir = /home/%u@%d\nad_domain = casdds.casd\nuse_fully_qualified_names = False\nldap_id_mapping = True\naccess_provider = ad\nldap_group_nesting_level = 2\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#42-configurer-pam","title":"4.2. Configurer PAM","text":"<p>Modifier les fichiers dans <code>/etc/pam.d/</code> pour inclure <code>pam_sss.so</code> </p> <pre><code>### /etc/pam.d/common-auth\nsudo: unable to resolve host debian118: Name or service not known\nauth      sufficient  pam_unix.so try_first_pass\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-account\nsudo: unable to resolve host debian118: Name or service not known\naccount   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <pre><code>### /etc/pam.d/common-password\nsudo: unable to resolve host debian118: Name or service not known\npassword  sufficient  pam_unix.so\npassword  sufficient  pam_sss.so\npassword  required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-session\nsudo: unable to resolve host debian118: Name or service not known\nsession   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#43-fichier-etckrb5conf","title":"4.3. Fichier <code>/etc/krb5.conf</code>","text":"<pre><code> [libdefaults]\n        default_realm = CASDDS.CASD\n\n        default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        permitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        proxiable = true\n        ticket_lifetime = 24h\n        dns_lookup_realm = true\n        dns_lookup_kdc = true\n        dns_canonicalize_hostname = false\n        rdns = false\n         allow_weak_crypto = true\n# The following libdefaults parameters are only for Heimdal Kerberos.\n        fcc-mit-ticketflags = true\n\n[realms]\n        CASDDS.CASD = {\n                kdc = 10.50.5.64\n                admin_server = 10.50.5.64\n        }\n\u2026..\n[domain_realm]\n\u2026.\n        .casdds.casd = CASDDS.CASD\n        casdds.casd = CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#etape-5-configuration-ssh-avec-gssapi","title":"\u00c9tape 5 : Configuration SSH avec GSSAPI","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#51-sur-debian","title":"5.1. Sur Debian","text":"<p>Modifier <code>/etc/ssh/sshd_config</code> :</p> <pre><code>UsePam yes\nGSSAPIAuthentication yes # l'authentification bas\u00e9e sur GSSAPI pour les connexions SSH\nGSSAPICleanupCredentials yes # la suppression automatique des identifiants temporaires obtenus via GSSAPI apr\u00e8s leur utilisation pour renforcer la s\u00e9curit\u00e9\nGSSAPIKeyExchange yes # s\u00e9curiser l'\u00e9change de cl\u00e9s, prot\u00e9geant ainsi le processus de n\u00e9gociation contre les interceptions\nGSSAPIStrictAcceptorCheck no # D\u00e9sactive la v\u00e9rification stricte de l'identit\u00e9 de l'acceptateur, facilitant les connexions dans des environnements o\u00f9 les noms de principal peuvent varier\n</code></pre> <p>Modifier <code>/etc/ssh/ssh_config</code>:</p> <pre><code>   Host *\n       \u2026\n       GSSAPIAuthentication yes\n       GSSAPIDelegateCredentials yes # d\u00e9l\u00e9guer les identifiants GSSAPI du client au serveur pour \n</code></pre> <p>Red\u00e9marrer les services :</p> <pre><code>sudo systemctl restart sshd sssd\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#52-sur-windows","title":"5.2. Sur Windows","text":"<p>Installer OpenSSH via PowerShell:</p> <pre><code>Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\nStart-Service sshd\nSet-Service -Name sshd -StartupType 'Automatic'\n</code></pre> <p>Activer GSSAPI dans <code>C:\\ProgramData\\ssh\\sshd_config</code> :</p> <pre><code>GSSAPIAuthentication yes\nGSSAPICleanupCredentials yes\n</code></pre> <p>Red\u00e9marrer le service:</p> <pre><code> Restart-Service sshd\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#etape-6-validation","title":"\u00c9tape 6 : Validation","text":""},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#61-test-kerberos","title":"6.1. Test Kerberos","text":"<p>Sur Debian :</p> <pre><code> kinit user@CASDDS.CASD  # Authentifier avec le mot de passe AD\nklist    \n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#62-connexion-ssh","title":"6.2. Connexion SSH","text":"<p>Depuis Windows :</p> <pre><code>ssh -K user@debian.casdds.casd  # -K active la d\u00e9l\u00e9gation Kerberos\n</code></pre>"},{"location":"adminsys/os_setup/security/02.Configure_ssh_pam_sssd_ad_fr/#appendix-notes-importantes","title":"Appendix Notes importantes :","text":"<ul> <li>Permissions SSSD : V\u00e9rifier que <code>/etc/sssd/sssd.conf</code> a les droits <code>600</code> :</li> </ul> <pre><code>sudo chmod 600 /etc/sssd/sssd.conf\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/","title":"Configure debian server to use AD/Krb for sshd authentication","text":"<p>In this tutorial, we show how to configure sshd, pam, sssd, to allow a <code>debian 11</code> server to use AD/Krb as  authentication server. We will follow the below steps:</p> <p>We suppose we have : - <code>AD/Krb</code> : The ip address is <code>10.50.5.64</code>, ad domain name <code>casdds.casd</code>, krb realm name <code>CASDDS.CASD</code>, hostname <code>auth</code>, fqdn is <code>auth.casdds.casd</code> - <code>debian 11</code>: ip address is <code>10.50.5.199</code>, hostname is <code>hadoop-client</code>, fqdn is <code>hadoop-client.casdds.casd</code></p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#step-1-prerequisite","title":"Step 1: Prerequisite","text":""},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#11-reset-hostname-of-hadoop-client","title":"1.1 Reset hostname of hadoop-client","text":"<p>The hostname is essential for the server to have a valid FQDN in the domain, so we need to make sure the hostname is set correctly. Follow the below steps: - set system hostname - update /etc/hosts</p> <pre><code># general form\nsudo hostnamectl set-hostname &lt;custom-hostname&gt;\n\n# for example \nsudo hostnamectl set-hostname hadoop-client\n\n# check the new hostname with below command\nhostname\n\n# expected output\nhadoop-client\n</code></pre> <p>you can also directly edit the hostname config file(not recommended) by using <code>sudo vim /etc/hostname</code></p> <p>Update <code>/etc/hosts</code>:</p> <pre><code>sudo vim /etc/hosts \n\n127.0.1.1 hadoop-client.casdds.casd hadoop-client\n10.50.5.199 hadoop-client.casdds.casd   hadoop-client\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#12-update-system-packages-in-hadoop-client","title":"1.2 Update system packages in hadoop-client","text":"<pre><code>sudo apt update \nsudo apt upgrade\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#13-change-dns-server-settings-in-hadoop-client","title":"1.3 Change dns server settings in hadoop-client","text":"<p>To join the server into an AD domain, you must use the AD as dns server.</p> <p>Edit the <code>/etc/resolv.conf</code> :</p> <pre><code>search casdds.casd\nnameserver 10.50.5.64\nnameserver 8.8.8.8\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#14-install-the-required-packages-in-hadoop-client","title":"1.4 Install the required packages in hadoop-client","text":"<pre><code>sudo apt install realmd sssd sssd-tools libnss-sss libpam-sss adcli samba-common-bin krb5-user oddjob oddjob-mkhomedir packagekit -y\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#step-2-join-the-debian-serverhadoop-client-to-the-ad-domain","title":"Step 2 : Join the debian server(hadoop-client) to the AD domain","text":""},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#21-check-if-the-domain-can-be-reached-or-not","title":"2.1. Check if the domain can be reached or not","text":"<pre><code>realm discover CASDDS.CASD\n</code></pre> <ul> <li>If the error message is realm command is unknown, open a new shell.</li> <li>If the error message is CASDDS.CASD is unknown, check the dns server ip is reachable, and dns server name setup is correct.</li> </ul>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#22-join-the-serverhadoop-client-to-the-ad-domain","title":"2.2. Join the server(hadoop-client) to the AD domain","text":"<p>To execute the below command, you must have an account with <code>domain administrator</code> privilege :</p> <pre><code>sudo realm join --user=Administrateur CASDDS.CASD\n</code></pre> <p>If there is no error message, it means your server has joined the domain.</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#23-configure-the-linux-serverhadoop-client-account-in-ad","title":"2.3 Configure the linux server(hadoop-client) account in AD","text":"<p>If the <code>hadoop-client</code> has success joined the AD domain, you should see the server appears in the <code>Computer</code> section in the AD manager GUI. Check the below figure</p> <p></p> <p>To check, you need to connect to the <code>Windows Server</code> -&gt; Open <code>AD manager</code> -&gt; In <code>Users and Computers</code> subfolder of  Active Directory. You should find a line of <code>HADOOP-CLIENT</code>. Right Click on it, and select <code>properties</code>, you should see the below pop-up window</p> <p></p> <p>Select the <code>Trust this computer for delegation to any service</code> option in <code>Delegation</code>. </p> <p>Click on the <code>Static IP address</code> option in <code>Dial-in</code>, then put the address ip of the <code>hadoop-client</code>.  </p> <p>You can add a new computer in AD manually, but we don't recommend that. Try to use the <code>realm join</code></p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#step-3-config-adkrb-dns-server-to-well-integrate-hadoop-client","title":"Step 3: Config AD/Krb, DNS server to well integrate hadoop-client","text":"<p>To make the debian server (hadoop-client) fqdn <code>recognizable</code> and <code>reachable</code> by the other servers in the domain, we need to configure the dns server </p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#31-check-the-dns-entries-in-windows-server","title":"3.1. Check the dns entries in windows server","text":"<p>Open the <code>dns manager</code> in the Windows server (<code>auth.casdds.casd</code>). Check the forward lookup and reverse lookup. You need to make sure the <code>hostname, fqdn and ip address</code> are correct. The two below figures are examples of the <code>hadoop-client</code> config.</p> <p></p> <p></p> <p>Normally, these entries are created automatically by the <code>realm join</code> command. If they are not created correcly, you  need to create them manually.</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#32-check-the-spn-service-principal-name-in-windows-server","title":"3.2. Check the SPN (Service Principal Name) in Windows server","text":"<p>Every registered computer in the domain should have a <code>valid SPN (Service Principal Name)</code>. You can check the name by  using the below command. You can open a <code>powershell prompt</code> in the <code>AD/krb</code> server.</p> <pre><code>setspn -L hadoop-client\n\n# expected output\nRegistered ServicePrincipalNames for CN=HADOOP-CLIENT,CN=Computers,DC=casdds,DC=casd:\n        RestrictedKrbHost/hadoop-client.casdds.casd\n        RestrictedKrbHost/HADOOP-CLIENT\n        host/hadoop-client.casdds.casd\n        host/HADOOP-CLIENT\n</code></pre> <p>If you don't see any outputs, you can create a <code>SPN (Service Principal Name)</code> manually. Below is the command to do so.</p> <pre><code># create a new spn and link it to the hadoop-client(AD account)\n# The -S option adds an SPN only if it does not already exist (avoids duplicates).\nsetspn -S host/hadoop-client.casdds.casd hadoop-client\n\n# generate a keytab for principal  host/hadoop-client.casdds.casd@CASDDS.CASD\nktpass -princ host/hadoop-client.casdds.casd@CASDDS.CASD -mapuser HADOOP-CLIENT$ -pass * -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 -out hadoop-client.keytab\n</code></pre> <p>Below lines are the explanation of the commands: - The <code>ktpass</code> command can generate a keytab file for Kerberos authentication.  - <code>-princ host/hadoop-client.casdds.casd@CASDDS.CASD</code> defines the Kerberos principal name. - <code>-mapuser HADOOP-CLIENT$</code> maps the Kerberos principal to the account (HADOOP-CLIENT) in Active Directory (AD). The <code>$</code> indicates it's a computer account (not a user). - <code>-crypto AES256-SHA1</code> specifies that only the <code>AES256-SHA1</code> crypto algo is supported. You can replace it with <code>ALL</code> to specify all available cryptographic algorithms should be supported for encryption. - <code>-ptype KRB5_NT_PRINCIPAL</code> specifies the principal type as KRB5_NT_PRINCIPAL, which is used for standard Kerberos authentication(for services, use KRB5_NT_SRV_HST). - <code>-pass *</code> prompts the user to enter the password manually. Typically, computer accounts in AD have auto-generated passwords. - <code>-out hadoop-client.keytab</code> saves the keytab file, which will be used by the linux server(hadoop-client) for authentication.</p> <p>The <code>hadoop-client.keytab</code> file is copied to the hadoop-client, so it can use this keytab for Kerberos authentication. <code>hadoop-client</code> can use the kerberos ticket to prove the identity of <code>hadoop-client</code>.</p> <p>After coping the <code>hadoop-client.keytab</code> file to <code>hadoop-client</code>, you can use the below command to check keytab contents:</p> <pre><code>klist -k /tmp/hadoop-client.keytab  \n\n# you should see outputs like\n\"host/hadoop-client.casdds.casd\"\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#33-rename-the-keytab-file-in-linuxhadoop-client","title":"3.3 Rename the keytab file in linux(hadoop-client)","text":"<p>In linux, many Kerberos-aware applications (e.g. kinit, Hadoop, etc.) expect the keytab file to be named <code>krb5.keytab</code>  and located in <code>/etc/</code> by default. We can use the below commands</p> <pre><code>sudo mv hadoop-client.keytab /etc/krb5.keytab\n\n# Ensures only root can read it (for security).\nsudo chmod 600 /etc/krb5.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#34-leave-and-rejoin-the-realm","title":"3.4 Leave and rejoin the realm","text":"<p>If there are errors that you can't resolve, you can always leave the realm and rejoin</p> <pre><code>sudo realm leave CASDDS.CASD\n\nsudo realm join --user=Administrateur CASDDS.CASD\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#35-create-a-service-account-in-ad-for-sssd-daemon","title":"3.5. Create a service account in AD for sssd daemon.","text":"<p>As we explained before, the linux server relies on <code>sssd</code> daemon to get the <code>user id and groups</code> from the AD server. This requires sssd to have an account that allows him to access AD.</p> <p>You need to create a service account <code>sssd</code> in <code>Active Directory manager</code>. Then use the below command to create a <code>principal</code> and the <code>keytab</code> file.</p> <pre><code>ktpass -princ sssd@CASDDS.CASD -mapuser sssd -crypto AES256-SHA1 -ptype KRB5_NT_PRINCIPAL -pass * -out sssd.keytab\n</code></pre> <p>Copie the keytab file to the debian server(hadoop-client):</p> <pre><code>scp sssd.keytab sssd@debian.casdds.casd:/tmp/\n</code></pre> <p>Put the keytab file in /etc</p> <pre><code>sudo cp /tmp/sssd.keytab /etc/\n# need to check the acl of the file, 644 is too open for me\nsudo chmod 644 /etc/sssd.keytab\nsudo chown root:root /etc/sssd.keytab\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#step-4-configuration-of-sssd-pam-and-kerberos","title":"Step 4 : Configuration of SSSD, PAM and Kerberos","text":"<p>We will follow the below order to configure each component: - kerberos client: configure krb client to connect to the target krb Realm - sshd/pam: configure sshd server to use pam as authentication backend - pam/sssd: configure pam to use sssd as backend - sssd/krb: configure sssd to use krb plugin</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#41-configure-kerberos-client-in-debianhadoop-client-server","title":"4.1 Configure kerberos client in debian(hadoop-client) server","text":"<pre><code># install the required package\nsudo apt install krb5-user\n\n# edit the config file `/etc/krb5.conf`  \nsudo vim /etc/krb5.conf\n</code></pre> <p>Put the below content in the file <code>/etc/krb5.conf</code> </p> <pre><code> [libdefaults]\n        default_realm = CASDDS.CASD\n\n        default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        permitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n        kdc_timesync = 1\n        ccache_type = 4\n        forwardable = true\n        # Fadoua said this must be removed from, otherwise the ticket will not be forwad to the target host \n        # proxiable = true\n        ticket_lifetime = 24h\n        dns_lookup_realm = true\n        dns_lookup_kdc = true\n        dns_canonicalize_hostname = false\n        rdns = false\n         allow_weak_crypto = true\n\n\n[realms]\n        CASDDS.CASD = {\n                kdc = 10.50.5.64\n                admin_server = 10.50.5.64\n        }\n\u2026..\n[domain_realm]\n\u2026.\n        .casdds.casd = CASDDS.CASD\n        casdds.casd = CASDDS.CASD\n</code></pre> <p>To check the krb client, use the below command</p> <pre><code># ask a ticket kerberos\nkinit host/hadoop-client.casdds.casd\n\n# the short version should work if the keytab is in place, if not you can specify the path of keytab\nkinit -kt /etc/krb5.keytab host/hadoop-client.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#42-configure-sshd-to-use-pam","title":"4.2. Configure sshd to use pam","text":"<p>We need to edit two files: - <code>/etc/ssh/sshd_config</code> (configuration for the ssh server) - <code>/etc/ssh/ssh_config</code> (configuration for the ssh client)</p> <p>In <code>/etc/ssh/sshd_config</code>, enable the below lines</p> <pre><code># disable other authentication methods\nChallengeResponseAuthentication no\nPasswordAuthentication no\n\n# use pam as authentication backend\nUsePAM yes\n\n# GSSAPI options for sshd server to accept GSSAPI, it's required for the server to accept krb ticket as \n# credentials\n# \nGSSAPIAuthentication yes\n# Cleans up the Kerberos credentials after the session.\nGSSAPICleanupCredentials yes\n# Ensures that the SSH client does not strictly check for a valid acceptor name in the Kerberos tickets.\nGSSAPIStrictAcceptorCheck no\n# Allows the exchange of Kerberos keys for stronger encryption.\nGSSAPIKeyExchange yes\n\n\nX11Forwarding yes\n\nPrintMotd no\n\n\n# Allow client to pass locale environment variables\nAcceptEnv LANG LC_*\n\n# override default of no subsystems\nSubsystem       sftp    /usr/lib/openssh/sftp-server\n</code></pre> <p>You need to restart the sshd service to enable the new config </p> <pre><code>sudo systemctl restart sshd\n</code></pre> <p>In the <code>/etc/ssh/ssh_config</code>, you need to add the below line </p> <pre><code>   Host *\n       GSSAPIAuthentication yes\n       GSSAPIDelegateCredentials yes\n       PasswordAuthentication no\n</code></pre> <p>For hadoop-client, the <code>ssh_config</code> is not required, because it defines the behaviour of the ssh client. It needs to be configured in the ssh client which wants to connect to the hadoop-client ssh server. </p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#43-configure-pam","title":"4.3 Configure pam","text":"<p>All the configuration files for pam are located in <code>/etc/pam.d/</code>. The below is the minimum config for the pam to use sssd daemon as authentication backend.</p> <pre><code>### /etc/pam.d/common-auth\nsudo: unable to resolve host debian118: Name or service not known\nauth      sufficient  pam_unix.so try_first_pass\nauth      sufficient  pam_sss.so use_first_pass\nauth      required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-account\nsudo: unable to resolve host debian118: Name or service not known\naccount   required    pam_unix.so\naccount   sufficient  pam_sss.so\naccount   required    pam_permit.so\n</code></pre> <pre><code>### /etc/pam.d/common-password\nsudo: unable to resolve host debian118: Name or service not known\npassword  sufficient  pam_unix.so\npassword  sufficient  pam_sss.so\npassword  required    pam_deny.so\n</code></pre> <pre><code>### /etc/pam.d/common-session\nsudo: unable to resolve host debian118: Name or service not known\nsession   required    pam_unix.so\nsession   optional    pam_sss.so\nsession   required    pam_mkhomedir.so skel=/etc/skel/ umask=0022\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#44-configure-sssd","title":"4.4 Configure sssd","text":"<p>Now we need to configure the sssd daemon. The main config file is in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[sssd]\nservices = nss, pam\ndomains = casdds.casd\nconfig_file_version = 2\n\n[nss]\nhomedir_substring = /home\n\n[pam]\n\n[domain/casdds.casd]\nldap_sasl_authid = sssd@CASDDS.CASD\nkrb5_keytab = /etc/sssd.keytab\ndefault_shell = /bin/bash\nkrb5_store_password_if_offline = True\ncache_credentials = True\nkrb5_realm = CASDDS.CASD\nrealmd_tags = manages-system joined-with-adcli\nid_provider = ad\nfallback_homedir = /home/%u@%d\nad_domain = casdds.casd\nuse_fully_qualified_names = False\nldap_id_mapping = True\naccess_provider = ad\nldap_group_nesting_level = 2\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#5configure-ssh-client-on-windows","title":"5.configure ssh client on Windows","text":"<p>In windows, there are many ssh clients: - MobaXterm: - tabby: https://tabby.sh/ - powershell+openssh - PuTTY</p> <p>Below is the instruction on how to install and configure openssh via PowerShell</p> <pre><code>Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\nStart-Service sshd\nSet-Service -Name sshd -StartupType 'Automatic'\n</code></pre> <p>In windows, all the configuration file for openssh is located in <code>C:\\ProgramData\\ssh</code> If you want to setup the config for ssh server, you can edit the file in <code>C:\\ProgramData\\ssh\\sshd_config</code>.</p> <p>To restart ssh service in windows</p> <pre><code># start sshd service\nStart-Service sshd\n\n# restart sshd service \nRestart-Service sshd\n</code></pre> <p>Configure ssh client </p> <pre><code># open a notepad\nnotepad $env:USERPROFILE\\.ssh\\config\n\n# add the below lines\n# * means for all hosts\nHost *\n    GSSAPIAuthentication yes\n    GSSAPIDelegateCredentials yes \n</code></pre> <p>You can also define the behaviors host by host, below is an example</p> <pre><code>Host hadoop-client\n    HostName hadoop-client.casdds.casd\n    User pengfei@casdds.casd\n    Port 22\n    GSSAPIAuthentication yes\n    GSSAPIDelegateCredentials yes \n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#step-6-test-the-solution","title":"Step 6 : Test the solution","text":"<p>In our scenario, the user follow the below steps: 1. first login to a Windows server, the first ticket kerberos is generated in the Windows server. 2. user ssh to hadoop-client with the ticket kerberos with option forward ticket 3. user try to access hdfs cluster with the forward kerberos ticket</p> <p>Suppose you have an account <code>user</code> in AD with the privilege to connect to <code>hadoop client</code>  \\</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#61-understand-the-ticket","title":"6.1. Understand the ticket","text":"<p>In linux, you can ask a ticket and check the ticket with the below command</p> <pre><code># ask a new ticket, you need to provide a password associated with the provided principal\nkinit user@CASDDS.CASD  \n\n# check the ticket contents\nklist -5fea   \n</code></pre> <p>The option: - 5: Show only Kerberos 5 tickets (modern Kerberos version). - f: Show ticket flags (like FORWARDABLE, RENEWABLE, etc.). - e: Display encryption type used for the ticket. - a Show addresses associated with the ticket (if address-restriction of the ticket is activated).</p> <p>You should see the below output as the ticket content</p> <pre><code>Ticket cache: FILE:/tmp/krb5cc_1000\nDefault principal: user@CASDDS.CASD\n\nValid starting       Expires              Service principal\n03/31/25 10:00:00  03/31/25 20:00:00  krbtgt/CASDDS.CASD@CASDDS.CASD\n        Flags: FRI\n        Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96\n        Addresses: 192.168.1.100\n</code></pre> <p>A kerberos ticket has the below properites:</p> <ul> <li>Ticket cache: Location of the ticket. </li> <li>Default principal: Your Kerberos identity (user@EXAMPLE.COM).</li> <li>Valid starting / Expires: Time range for which the ticket is valid.</li> <li>Service principal: The Kerberos service this ticket is for. (krbtgt/CASDDS.CASD@CASDDS.CASD is a tgt issued by CASDDS.CASD the kdc server)</li> <li>Flags (-f option): F = Forwardable (Can be forwarded to another machine). R = Renewable (Can be extended before expiration). I = Initial (Freshly obtained).</li> <li>Encryption type (-e option): aes256-cts-hmac-sha1-96, means AES-256 encryption with SHA-1 HMAC.</li> <li>Addresses (-a option): Shows the IP addresses associated with the ticket (if address-restricted).</li> </ul> <p>You can ask ticket with special options:</p> <pre><code># below command ask a Forwardable, Renewable for a 7 day validity\nkinit -f -r 7d\n</code></pre> <p>Based on the kdc configuration, it may or may not generate the ticket.</p>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#62-connexion-ssh","title":"6.2. Connexion SSH","text":"<p>From windows, if the server has joined the domain, windows will generate a kerberos ticket after user logon:</p> <pre><code># check the user ticket\nklist -5fea\n\n# for windows ssh client\n# -K active la d\u00e9l\u00e9gation Kerberos\nssh -K user@debian.casdds.casd  \n\n# for linux ssh client\nssh -o GSSAPIDelegateCredentials=yes user@debian.casdds.casd\n</code></pre>"},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#appendix","title":"Appendix :","text":""},{"location":"adminsys/os_setup/security/03.Configure_ssh_pam_sssd_ad_en/#acl-for-etcsssdsssdconf","title":"ACL for /etc/sssd/sssd.conf","text":"<p>The Permissions for <code>/etc/sssd/sssd.conf</code> must be <code>600</code> :</p> <pre><code>sudo chmod 600 /etc/sssd/sssd.conf\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/","title":"Configure sssd to use static uid, gid","text":"<p>POSIX (Portable Operating System Interface) is <code>UNIX/Linux standards for identity and access control</code>. A <code>POSIX</code>  account use specific attributes such as - uidNumber \u2013 Unique User ID (UID) - gidNumber \u2013 Primary Group ID (GID) - homeDirectory \u2013 User's home directory path  - loginShell \u2013 The default shell (e.g., /bin/bash)</p> <p>These attributes allow UNIX/Linux systems to recognize, authenticate users, and create user workspace.</p> <p>By default, AD does not use POSIX attributes for user and group.  Instead, AD relies on:</p> <ul> <li>Security Identifiers (SIDs): Every user and group has a <code>SID</code>, which is a unique identifier in Windows.</li> <li><code>sAMAccountName</code>: pliu (This is legacy login, )</li> <li>UserPrincipalName (UPN): pliu@casd.eu (authentication in modern windows server)</li> </ul>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#1-default-behavior-when-sssd-uses-ad-as-authentication-backend","title":"1. Default behavior when sssd uses AD as authentication backend","text":"<p>The default behavior in <code>SSSD</code> and <code>Winbind</code> is to use <code>auto id mapping</code>. SSSD will dynamically generate UIDs and GIDs from the <code>AD objects's ObjectSID</code>. This will lead to inconsistent UIDs and GIDs across machines.</p> <p>In certain scenarios, it will create conflicting ACL in user home. For example, if a user with AD account with name <code>test</code> login to a linux server, a user home will be created <code>/home/test</code>. If the user account is deleted, and a new account <code>test</code> is created, when the new <code>test</code> user login to the linux server, it will user /home/test as home dir too. But the new and old <code>test</code> will have different uid. So the old files in /home/test will have old uid as owner, the new  <code>test</code> user can't access it. </p> <p>To avoid inconsistent UIDs and GIDs, we recommend you to use static uidNumber and gidNumber.</p>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#2-configure-sssd-to-user-static-uidnumber-and-gidnumber","title":"2. Configure sssd to user static uidNumber and gidNumber.","text":"<p>To configure sssd to user static uidNumber and gidNumber, follow the below steps 1. Add posix attributes in AD 2. Configure sssd to read posix attributes</p>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#21-adds-posix-attributes-in-ad","title":"2.1 Adds posix attributes in AD","text":"<p>If SSSD wants to use static uidNumber and gidNumber, the AD server must have those attributes. Before Windows server 2016. The AD server can use <code>rfc2307</code> schema, which allows us to create posix compatible user  accounts and groups. This feature has been removed since <code>Windows server 2016</code>. But you can still add attributes such as <code>uidNumber</code>, <code>gidNumber</code> to a user account or group. </p> <p>Open <code>AD users and groups gui</code>-&gt; on the toolbar, click on <code>view</code>-&gt; Select <code>advance features</code> -&gt; now when you double-click on  a user account, you will see a tab called </p> <p></p> <p></p> <p>The official doc can be found here.</p>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#22-configure-sssd-to-read-posix-attributes","title":"2.2 Configure sssd to read posix attributes","text":"<p>Before changing your sssd configuration, make sure <code>AD Objects have gidNumber and uidNumber Attributes</code>.</p>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#221-for-ad-windows-server-2016","title":"2.2.1 For AD &lt; Windows server 2016.","text":"<p>Configure the AD server to use <code>rfc2307</code> schema, and create posix compatible accounts and groups. Then add the below  conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False\nldap_schema = rfc2307\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#221-for-ad-windows-server-2016_1","title":"2.2.1 For AD &gt;= Windows server 2016.","text":"<p>Add the below conf in <code>/etc/sssd/sssd.conf</code></p> <pre><code>[domain/YOURDOMAIN]\nid_provider = ad\naccess_provider = ad\nldap_id_mapping = False  # Important: Forces usage of uidNumber/gidNumber\nldap_user_uid_number = uidNumber\nldap_user_gid_number = gidNumber\nldap_group_gid_number = gidNumber\nenumerate = True  # Optional: Lists all users and groups\n</code></pre>"},{"location":"adminsys/os_setup/security/04.Static_uid_gid_integration_in_sssd/#23-restart-sssd-and-check-uid-gid","title":"2.3 Restart sssd and check uid, gid","text":"<pre><code># restart sssd\nsystemctl restart sssd\n# clear sssd cache\nsss_cache -E\n\n# check user id and groups\nid &lt;uid&gt;\n\n# you should see the output id value matches the value which you deined in AD\n\n# check group id\ngetent group &lt;groupname&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/","title":"Integrate kerberos into a hadoop cluster","text":"<p>In this tutorial, we will show how to integrate kerberos into a hadoop cluster. The goal is to use the kerberos tickets to authenticate users, hosts(e.g. namenode, datanode, resourceManager, etc.) and services(e.g. hdfs, yarn). </p> <p>We have different strategy for different kinds of users. For service account, we will generate <code>.keytab</code> files to generate kerberos tickets automatically. For user account, a password maybe required to generate the ticket.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#1-prerequisite","title":"1. Prerequisite","text":"<p>Before we start, we need to clarify the hadoop cluster context. Because the <code>AD/Ldap account</code> and <code>kerberos principal</code> naming conventions strongly depends on the cluster architecture.</p> <p>Suppose we have three servers, in each server we run different services: - spark-m01.casdds.casd: name-node(hdfs), resource-manager(yarn), history-server(spark) - spark-m02.casdds.casd: data-node(hdfs), node-manager(yarn) - spark-m03.casdds.casd: data-node(hdfs), node-manager(yarn)</p> <p>We suppose you already join these machines into the AD/krb realm. For more details, you can check this  doc </p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#11-prepare-service-account-and-their-keytab","title":"1.1 Prepare service account and their keytab","text":"<p>By convention, we recommend you to create <code>a dedicated AD/Ldap account and kerberos principal for each service</code>.  This ensures <code>secure authentication</code> and <code>proper ticket management</code>. It's also easier to monitor access and avoid  unexpected situations. Technically, an AD/Ldap account can be associated with one or more kerberos principals.</p> <p>Below is a list of all AD/Ldap accounts and kerberos principal you need to create:</p> Service Hadoop Role Host Kerberos Principal AD account name HDFS NameNode spark-m01.casdds.casd nn/spark-m01.casdds.casd@CASDDS.CASD hdfs-nn HDFS DataNode spark-m02.casdds.casd dn/spark-m02.casdds.casd@CASDDS.CASD hdfs-dn1 HDFS DataNode spark-m03.casdds.casd dn/spark-m03.casdds.casd@CASDDS.CASD hdfs-dn2 HDFS HTTP Service spark-m01.casdds.casd http/spark-m01.casdds.casd@CASDDS.CASD http-nn HDFS HTTP Service spark-m02.casdds.casd http/spark-m02.casdds.casd@CASDDS.CASD http-dn1 HDFS HTTP Service spark-m03.casdds.casd http/spark-m03.casdds.casd@CASDDS.CASD http-dn2 YARN ResourceManager spark-m01.casdds.casd rm/spark-m01.casdds.casd@CASDDS.CASD yarn-rn YARN NodeManager spark-m02.casdds.casd nm/spark-m02.casdds.casd@CASDDS.CASD yarn-nm1 YARN NodeManager spark-m03.casdds.casd nm/spark-m03.casdds.casd@CASDDS.CASD yarn-nm2 Spark History Server spark-m01.casdds.casd jhs/spark-m01.casdds.casd@CASDDS.CASD spark-jhs HOST None spark-m01.casdds.casd host/spark-m01.casdds.casd@CASDDS.CASD spark-m01 HOST None spark-m02.casdds.casd host/spark-m02.casdds.casd@CASDDS.CASD spark-m02 HOST None spark-m03.casdds.casd host/spark-m03.casdds.casd@CASDDS.CASD spark-m03 <p>The AD account name cannot contain special character such as <code>@</code> and <code>.</code>, so we can't use the principal name as  AD account name. </p> <p>You can create an AD account in windows with the below command</p> <pre><code># create AD account and kerberos principal\nNew-ADUser -Name \"hdfs-nn\" -SamAccountName \"hdfs-nn\" -UserPrincipalName \"nn/spark-m01.casdds.casd@CASDDS.CASD\" -Enabled $true -PasswordNeverExpires $true -CannotChangePassword $true -ChangePasswordAtLogon $false -PassThru | Set-ADAccountControl -PasswordNotRequired $true\n\n# create corresponding keytab\nktpass -princ nn/spark-m01.casdds.casd@CASDDS.CASD -mapuser hdfs-nn -crypto ALL -ptype KRB5_NT_PRINCIPAL -pass Password! -out hdfs-nn.keytab\n</code></pre> <p>After you generate the required keytab files for all principals, you need to copy them to the target server. For example, for server <code>spark-m01.casdds.casd@CASDDS.CASD</code>, you need to copy the keytab file for principals: - nn/spark-m01.casdds.casd@CASDDS.CASD - HTTP/spark-m01.casdds.casd@CASDDS.CASD - rm/spark-m01.casdds.casd@CASDDS.CASD - jhs/spark-m01.casdds.casd@CASDDS.CASD - host/spark-m01.casdds.casd@CASDDS.CASD</p> <p>The general rule is straightforward, you need to check the host fqdn name in the principals </p> <p>You can test the validity of the keytab file by asking a kerberos ticket. Below command is an example</p> <pre><code>kinit -kt /etc/hdfs-nn.keytab nn/spark-m01.casdds.casd@CASDDS.CASD\n</code></pre> <p>To show the details of a keytab file, you can use the below command: </p> <pre><code>klist -e -k -t /etc/yarn.keytab\n\n# expected output\nKeytab name: FILE: /etc/yarn.keytab\n KVNO Timestamp         Principal\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (AES-256 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (AES-128 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 yarn/spark-m02.casdds.casd (ArcFour with HMAC/md5)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (AES-256 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (AES-128 CTS mode with 96-bit SHA-1 HMAC)\n   4 07/18/11 21:08:09 host/spark-m02.casdds.casd (ArcFour with HMAC/md5)\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#12-merge-the-keytab-files","title":"1.2 Merge the keytab files","text":"<p>To avoid managing many keytab files, you can merge the multi keytab files into one by using <code>ktutil</code> tool.</p> <pre><code># start a ktuitl shell with sudo right\nsudo ktutil\n\n# load credentials from the keytab files\nrkt /tmp/yarnm02.keytab\nrkt /tmp/hostm02.keytab\n\n# output the loaded credential to a new keytable file\nwkt /tmp/merged.keytab\n\n# exit the ktuitl shell\nq\n</code></pre> <p>You can test the content of the merged keytab file with the below command</p> <pre><code>sudo klist -k /tmp/merged.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#13-check-if-the-hadoop-servers-are-in-the-ad-dns","title":"1.3. Check if the hadoop servers are in the AD DNS","text":"<p>Normally, when the linux servers have joined the AD/Krb realm, their AD/DNS configuration are done automatically.</p> <p>Just to make sure, you can open the DNS manager on the <code>Domain controller</code> where AD/Krb is located. Below figure is an example of the <code>DNS manager GUI</code></p> <p></p> <p>You need to check the <code>value of FQDN and ip</code> for each server in <code>forward and reverse lookup zones</code>.</p> <p>Below figure is an example for the Forward loopup zone definition of a server</p> <p></p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#14-check-kerberos-client","title":"1.4. Check kerberos client","text":"<p>Normally, you should have a valid krb5 client and config on each hadoop node.</p> <p>Below is an example of the krb5 client conf(<code>/etc/krb5.conf</code>).</p> <pre><code>[libdefaults]\ndefault_realm = CASDDS.CASD\ndefault_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\ndefault_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\npermitted_enctypes   = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\nkdc_timesync = 1\nccache_type = 4\nforwardable = true\nticket_lifetime = 24h\ndns_lookup_realm = true\ndns_lookup_kdc = true\n\n#allow_weak_crypto = true\n\n[realms]\nCASDDS.CASD = {\n    kdc = 10.50.5.64\n    admin_server = 10.50.5.64\n}\n\n[domain_realm]\n.CASDDS.CASD = CASDDS.CASD\nCASDDS.CASD = CASDDS.CASD\n</code></pre> <p>You can enable allow_weak_crypto = true, if the AD/Krb can't use advance crypto algorithm</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#2-enable-ssl-in-the-hadoop-cluster","title":"2. Enable SSL in the hadoop cluster","text":"<p>To secure communication between services in the hadoop cluster, we can enable SSL.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#21-generate-certificate","title":"2.1 Generate certificate","text":"<p>Generate a key pair and store it in a java key store</p> <pre><code>sudo keytool -genkeypair \\\n  -alias hadoop \\\n  -keyalg RSA \\\n  -keysize 2048 \\\n  -validity 365 \\\n  -keystore /opt/hadoop/keystore.jks \\\n  -storepass changeit\n</code></pre> <p>Check the keystore content </p> <pre><code>sudo keytool -list -keystore /opt/hadoop/keystore.jks -storepass changeit\n</code></pre> <p>Export the certificate</p> <pre><code>sudo keytool -export -alias hadoop -keystore /opt/hadoop/keystore.jks -file /opt/hadoop/hadoop-cert.pem -storepass changeit\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#22-configuration-of-ssl-in-hadoop-cluster","title":"2.2 Configuration of SSL in hadoop cluster","text":"<p>The ssl configuration file in hadoop cluster is <code>$HADOOP_HOME/etc/hadoop/ssl-server.xml</code> and  <code>$HADOOP_HOME/etc/hadoop/ssl-client.xml</code>. In our case, we only need to modify <code>ssl-server.xml</code>.</p> <p>Below is an example of the <code>ssl-server.xml</code></p> <pre><code>sudo vim ssl-server.xml\n</code></pre> <p>Add the below lines</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/keystore.jks&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n    &lt;value&gt;jks&lt;/value&gt;\n    &lt;description&gt;(Optionnel) Format du keystore (par d\u00e9faut \u00ab jks \u00bb).&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.exclude.cipher.list&lt;/name&gt;\n    &lt;value&gt;\n      TLS_ECDHE_RSA_WITH_RC4_128_SHA,\n      SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,\n      SSL_RSA_WITH_DES_CBC_SHA,\n      SSL_DHE_RSA_WITH_DES_CBC_SHA,\n      SSL_RSA_EXPORT_WITH_RC4_40_MD5,\n      SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,\n      SSL_RSA_WITH_RC4_128_MD5\n    &lt;/value&gt;\n    &lt;description&gt;(Optionnel) Liste des suites de chiffrement faibles \u00e0 exclure.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>This needs to be done all nodes of the hadoop cluster</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#2-integrate-kerberos-in-to-hadoop-cluster","title":"2. Integrate kerberos in to Hadoop cluster","text":"<p>Here, we suppose you already have a working hadoop cluster, the below steps only shows how to integrate kerberos into Hadoop. If you want to learn how to deploy a hadoop cluster, you need to follow this  doc</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#21-edit-hadoop-envsh","title":"2.1 Edit hadoop-env.sh","text":"<p>The <code>hadoop-env.sh</code> file specifies all environment variables related to the hadoop cluster</p> <p>Below is a list of variables you need to check </p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext\"\n# use krb client config \nexport HADOOP_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf $HADOOP_OPTS\"\nexport HDFS_NAMENODE_USER=hadoop\nexport HDFS_DATANODE_USER=hadoop\nexport HDFS_SECONDARYNAMENODE_USER=hadoop\nexport JSVC_HOME=$(dirname $(which jsvc))\nexport HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport HADOOP_SECURITY_LOGGER=INFO,RFAS,console\nexport JAVA_TOOL_OPTIONS=\"$JAVA_TOOL_OPTIONS --add-opens=java.base/sun.net.dns=ALL-UNNAMED\"\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#22-updates-the-security-configuration-of-jdk","title":"2.2 Updates the security configuration of JDK","text":"<p>For kerberos interoperate well with JDK, we need to update the default security conf of the JDK. </p> <pre><code>sudo vim $JAVA_HOME/conf/security/java.security\n\n# in our case, we use openjdk in debian 11. We can use the absolute path\nsudo vim /usr/lib/jvm/java-11-openjdk-amd64/conf/security/java.security\n\n# you need to add the below lines\ncrypto.policy=unlimited\nsun.security.krb5.disableReferrals=true\n</code></pre> <p>By default, the <code>RC4</code> encryption algo is disabled, because it's weak. But some Windows server still uses it. You can find the line <code>jdk.jar.disabledAlgorithms</code> and <code>jdk.tls.disabledAlgorith</code>, then remove the <code>RC4</code> from the  disabled algo list.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#23-update-the-hadoop-service-configuration","title":"2.3 Update the hadoop service configuration","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#231-for-name-nodes","title":"2.3.1 For Name nodes","text":"<p>For <code>Name nodes</code>, you need to edit the below config files: - core-site.xml - hdfs-site.xml - yarn-site.xml</p> <pre><code>sudo vim core-site.xml \n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/etc/hadoop/ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;\n    &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/httpm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.security.AuthenticationFilterInitializer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@casdds\\.casd)s/@casdds\\.casd//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers le nom d\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim hdfs-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n    &lt;description&gt;Augmentation de la file d\u2019attente pour g\u00e9rer davantage de connexions clients.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour s\u00e9curiser l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:9870&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim yarn-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#232-for-datanode","title":"2.3.2 For DataNode","text":"<p>For data nodes, you need to edit the below configuration files: - core-site.xml - hdfs-site.xml - yarn-site.xml</p> <pre><code>sudo vim core-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@CASDDS\\.CASD)s/@CASDDS\\.CASD//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers l\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim hdfs-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;ip-10-50-5-203.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode sur le DataNode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfsm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n    &lt;value&gt;750&lt;/value&gt;\n    &lt;description&gt;Permissions requises sur les r\u00e9pertoires de donn\u00e9es.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>sudo vim yarn-site.xml\n\n# add the below lines\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m02.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m02.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarnm02.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#refresh-user-to-group-mappings","title":"Refresh User to group mappings","text":"<pre><code>hdfs dfsadmin -refreshUserToGroupsMappings\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#reference","title":"Reference","text":"<ul> <li>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</li> <li>http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.5/security-reference/content/kerberos_nonambari_adding_security_information_to_configuration_files.html</li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster/#repo-test","title":"Repo test","text":"<p>https://github.com/CASD-EU/admin_sys/tree/test/dev/roles</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/","title":"Integrate kerberos in hadoop cluster fr","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#integration-de-kerberos-dans-un-cluster-hadoop","title":"Int\u00e9gration de Kerberos dans un cluster Hadoop","text":"<p>Cette documentation d\u00e9crit les \u00e9tapes pour s\u00e9curiser l\u2019authentification des composants d\u2019un cluster Hadoop (NameNode, DataNode, ResourceManager, etc.) \u00e0 l\u2019aide de Kerberos et SSL.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#1-contexte-et-architecture","title":"1. Contexte et architecture","text":"<ul> <li> <p>Cluster : 3 n\u0153uds (spark-m01, spark-m02, spark-m03) dans le domaine CASDDS.CASD</p> </li> <li> <p>spark-m01 : NameNode, ResourceManager, HistoryServer</p> </li> <li> <p>spark-m02 &amp; spark-m03 : DataNode, NodeManager</p> </li> <li> <p>But :</p> </li> <li> <p>Authentification forte via Kerberos</p> </li> <li>Chiffrement des communications via SSL/TLS</li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#2-prerequis","title":"2. Pr\u00e9requis","text":"<ol> <li> <p>AD/Kerberos</p> </li> <li> <p>Les serveurs Linux doivent \u00eatre joints au domaine AD/Kerberos <code>CASDDS.CASD</code></p> </li> <li> <p>Un contr\u00f4leur de domaine (KDC + DNS) configur\u00e9 pour forward/reverse lookup</p> </li> <li> <p>Logiciels</p> </li> <li> <p>Java\u00a011 (OpenJDK)</p> </li> <li>Hadoop 3.x</li> <li>Utilitaires Kerberos (<code>kinit</code>, <code>klist</code>, <code>ktpass</code>, <code>ktutil</code>)</li> <li>Keytool (Java)</li> </ol>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#3-configuration-kerberos","title":"3. Configuration Kerberos","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#31-creation-des-comptes-service-et-keytabs","title":"3.1. Cr\u00e9ation des comptes service et keytabs","text":"<p>Pour chaque service, cr\u00e9ez un compte AD d\u00e9di\u00e9 et g\u00e9n\u00e9rez un fichier <code>.keytab</code> :</p> Service R\u00f4le FQDN Principal Kerberos AD User HDFS NameNode NameNode spark-m01.casdds.casd <code>hdfs/spark-m01.casdds.casd@CASDDS.CASD</code> <code>hdfs-m01</code> HDFS DataNode DataNode spark-m02.casdds.casd <code>hdfs/spark-m02.casdds.casd@CASDDS.CASD</code> <code>hdfs-m02</code> HDFS DataNode DataNode spark-m03.casdds.casd <code>hdfs/spark-m03.casdds.casd@CASDDS.CASD</code> <code>hdfs-m03</code> HTTP HTTP Service spark-m01.casdds.casd <code>HTTP/spark-m01.casdds.casd@CASDDS.CASD</code> <code>http-m01</code> YARN RM ResourceManager spark-m01.casdds.casd <code>yarn/spark-m01.casdds.casd@CASDDS.CASD</code> <code>yarn-m01</code> YARN NM NodeManager spark-m0X.casdds.casd <code>yarn/spark-m0X.casdds.casd@CASDDS.CASD</code> <code>yarn-m0x</code> HOST Host principal spark-m0X.casdds.casd <code>host/spark-m0X.casdds.casd@CASDDS.CASD</code> <code>host-m0X</code> <p>Commande Windows (AD) :</p> <pre><code>New-ADUser -Name \"hdfs-m01\" -SamAccountName \"hdfs-m01\" \\\n  -UserPrincipalName \"hdfs/spark-m01.casdds.casd@CASDDS.CASD\" \\\n  -Enabled $true -PasswordNeverExpires $true -CannotChangePassword $true\n\nktpass -princ hdfs/spark-m01.casdds.casd@CASDDS.CASD \\\n  -mapuser hdfs-m01 -crypto ALL -ptype KRB5_NT_PRINCIPAL \\\n  -pass \"Password!\" -out hdfs-m01.keytab\n\nscp hdfs-m01.keytab user@spark-m0x.casdds.casd:/tmp/\n</code></pre> <p>Copiez chaque keytab sous <code>/etc/</code> sur le serveur correspondant, avec permissions <code>root:hadoop</code>, <code>chmod 640</code>.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#32-verification-des-keytabs","title":"3.2. V\u00e9rification des keytabs","text":"<pre><code># Authentication sans mot de passe\nkinit -kt /etc/hdfs-m01.keytab hdfs/spark-m01.casdds.casd@CASDDS.CASD\n# Afficher tickets\nklist\n# Lister contenu d'un keytab\nklist -e -k -t /etc/hdfs-m01.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#33-fusion-de-plusieurs-keytabs","title":"3.3. Fusion de plusieurs keytabs","text":"<pre><code>sudo ktutil\nrkt /tmp/yarn-m02.keytab\nrkt /tmp/host-m02.keytab\nwkt /etc/merged.keytab\nq\nsudo klist -k /etc/merged.keytab\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#4-configuration-du-client-kerberos-etckrb5conf","title":"4. Configuration du client Kerberos (/etc/krb5.conf)","text":"<pre><code>[libdefaults]\n  default_realm = CASDDS.CASD\n  default_tkt_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  default_tgs_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  permitted_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\n  kdc_timesync = 1\n  ticket_lifetime = 24h\n  forwardable = true\n  dns_lookup_realm = true\n  dns_lookup_kdc = true\n  rdns = false\n  #allow_weak_crypto = true\n\n[realms]\n  CASDDS.CASD = {\n    kdc = @ip\n    admin_server = @ip\n  }\n\n[domain_realm]\n  .casdds.casd = CASDDS.CASD\n  casdds.casd = CASDDS.CASD\n</code></pre> <p>D\u00e9commentez <code>allow_weak_crypto = true</code> si n\u00e9cessaire pour RC4.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#5-securisation-ssltls","title":"5. S\u00e9curisation SSL/TLS","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#51-generation-du-keystore-java","title":"5.1. G\u00e9n\u00e9ration du keystore Java","text":"<pre><code>sudo keytool -genkeypair \\\n  -alias hadoop -keyalg RSA -keysize 2048 -validity 365 \\\n  -keystore /opt/hadoop/keystore.jks -storepass changeit\nsudo keytool -list -keystore /opt/hadoop/keystore.jks -storepass changeit\nsudo keytool -export -alias hadoop \\\n  -file /opt/hadoop/hadoop-cert.pem -keystore /opt/hadoop/keystore.jks \\\n  -storepass changeit\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#52-configuration-ssl-serverxml","title":"5.2. Configuration <code>ssl-server.xml</code>","text":"<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/keystore.jks&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n    &lt;value&gt;changeit&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n        &lt;value&gt;jks&lt;/value&gt;\n        &lt;description&gt;Optional. The keystore file format, default value is \"jks\".&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.exclude.cipher.list&lt;/name&gt;\n        &lt;value&gt;TLS_ECDHE_RSA_WITH_RC4_128_SHA,SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,\n        SSL_RSA_WITH_DES_CBC_SHA,SSL_DHE_RSA_WITH_DES_CBC_SHA,\n        SSL_RSA_EXPORT_WITH_RC4_40_MD5,SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,\n        SSL_RSA_WITH_RC4_128_MD5&lt;/value&gt;\n        &lt;description&gt;Optional. The weak security cipher suites that you want excludedfrom SSL communication.&lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>R\u00e9p\u00e9tez l\u2019op\u00e9ration sur tous les n\u0153uds.</p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#6-configuration-hadoop","title":"6. Configuration Hadoop","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#61-hadoop-envsh","title":"6.1. <code>hadoop-env.sh</code>","text":"<pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext\"\nexport HADOOP_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf $HADOOP_OPTS\"\nexport HDFS_NAMENODE_USER=hadoop\nexport HDFS_DATANODE_USER=hadoop\nexport HDFS_SECONDARYNAMENODE_USER=hadoop\nexport JSVC_HOME=$(dirname $(which jsvc))\nexport HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport HADOOP_SECURITY_LOGGER=INFO,RFAS,console\nexport JAVA_TOOL_OPTIONS=\"$JAVA_TOOL_OPTIONS --add-opens=java.base/sun.net.dns=ALL-UNNAMED\"\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#62-politiques-de-securite-java","title":"6.2. Politiques de s\u00e9curit\u00e9 Java","text":"<pre><code># $JAVA_HOME/conf/security/java.security\ncrypto.policy = unlimited\nsun.security.krb5.disableReferrals = true\n# Retirer RC4 de jdk.jar.disabledAlgorithms / jdk.tls.disabledAlgorithms si besoin\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#63-configuration-des-services","title":"6.3. Configuration des services","text":""},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#631-namenode-core-sitexml-hdfs-sitexml-yarn-sitexml","title":"6.3.1. NameNode (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code>)","text":"<pre><code>&lt;!-- core-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;/opt/hadoop/etc/hadoop/ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;\n    &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/http-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.security.AuthenticationFilterInitializer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@casdds\\.casd)s/@casdds\\.casd//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers le nom d\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- hdfs-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:50470&lt;/value&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n    &lt;description&gt;Augmentation de la file d\u2019attente pour g\u00e9rer davantage de connexions clients.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour s\u00e9curiser l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd:9870&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- yarn-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#632-datanode-core-sitexml-hdfs-sitexml-yarn-sitexml","title":"6.3.2. DataNode (<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code>)","text":"<pre><code>&lt;!-- core-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://spark-m01.casdds.casd:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n    &lt;value&gt;\n      RULE:[2:$1@$0](.*@CASDDS\\.CASD)s/@CASDDS\\.CASD//\n      RULE:[1:$1]\n      DEFAULT\n    &lt;/value&gt;\n    &lt;description&gt;Mapping du principal Kerberos vers l\u2019utilisateur local.&lt;/description&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- hdfs-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;\n    &lt;value&gt;ssl-server.xml&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.http.policy&lt;/name&gt;\n    &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.port&lt;/name&gt;\n    &lt;value&gt;50470&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n    &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.secondary.https.port&lt;/name&gt;\n    &lt;value&gt;50490&lt;/value&gt;\n    &lt;description&gt;Port HTTPS pour le secondary-namenode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.https.address&lt;/name&gt;\n    &lt;value&gt;ip-x.x.x.x.casdds.casd:50470&lt;/value&gt;  &lt;!-- @ip namdenode --&gt;\n    &lt;description&gt;Adresse HTTPS d\u2019\u00e9coute du Namenode sur le DataNode.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///opt/hadoop/hadoop_tmp/hdfs/data&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/hdfs-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation de la v\u00e9rification des permissions sur HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;Nom du groupe des super-utilisateurs.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.max.response.size&lt;/name&gt;\n    &lt;value&gt;5242880&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Activation des tokens d\u2019acc\u00e8s pour l\u2019acc\u00e8s aux datanodes.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n    &lt;value&gt;750&lt;/value&gt;\n    &lt;description&gt;Permissions requises sur les r\u00e9pertoires de donn\u00e9es.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.access.time.precision&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n    &lt;description&gt;D\u00e9sactivation de la mise \u00e0 jour des temps d\u2019acc\u00e8s pour les fichiers HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.cluster.administrators&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n    &lt;description&gt;ACL pour l\u2019acc\u00e8s aux servlets par d\u00e9faut de HDFS.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;ipc.server.read.threadpool.size&lt;/name&gt;\n    &lt;value&gt;5&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n&lt;!-- yarn-site.xml --&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m0x.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;spark-m01.casdds.casd&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n    &lt;value&gt;2&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;512&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m01.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m01.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt;\n    &lt;value&gt;yarn/spark-m0x.casdds.casd@CASDDS.CASD&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/yarn-m0x.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#7-validation-et-tests","title":"7. Validation et tests","text":"<ol> <li>Tester kinit sur chaque n\u0153ud/service</li> <li>D\u00e9marrer Hadoop en mode s\u00e9curis\u00e9 </li> <li>V\u00e9rifier que les services \u00e9coutent en HTTPS et que <code>klist</code> montre les tickets actifs</li> <li>Rafra\u00eechir les mappings utilisateurs-groupes :</li> </ol> <p><code>bash    hdfs dfsadmin -refreshUserToGroupsMappings</code></p>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#8-references","title":"8. R\u00e9f\u00e9rences","text":"<ul> <li> <p>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</p> </li> <li> <p>http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.5/security-reference/content/kerberos_nonambari_adding_security_information_to_configuration_files.html</p> </li> </ul>"},{"location":"hadoop/Integrate_kerberos_in_hadoop_cluster_fr/#9-repo-ansible","title":"9. Repo Ansible","text":"<ul> <li>https://github.com/CASD-EU/admin_sys/tree/test/dev/roles</li> </ul>"}]}